[
  {
    "objectID": "tutorials/tutorial-w09.html",
    "href": "tutorials/tutorial-w09.html",
    "title": "DAL tutorial - Week 9",
    "section": "",
    "text": "Go to https://bookdown.org/yih_huynh/Guide-to-R-Book/trouble.html and read about common errors and how to fix them.\n\n\n\n\n\n\nTerminology\n\n\n\nThe linked page talks about “loading libraries” and “loading packages”, but you know very well by now that we attach packages and we don’t “load libraries”.\nFor a technical explanation about loading vs attaching and packages vs libraries see: https://r-pkgs.org/dependencies-mindset-background.html#sec-dependencies-namespace and following sections."
  },
  {
    "objectID": "tutorials/tutorial-w09.html#troubleshooting",
    "href": "tutorials/tutorial-w09.html#troubleshooting",
    "title": "DAL tutorial - Week 9",
    "section": "",
    "text": "Go to https://bookdown.org/yih_huynh/Guide-to-R-Book/trouble.html and read about common errors and how to fix them.\n\n\n\n\n\n\nTerminology\n\n\n\nThe linked page talks about “loading libraries” and “loading packages”, but you know very well by now that we attach packages and we don’t “load libraries”.\nFor a technical explanation about loading vs attaching and packages vs libraries see: https://r-pkgs.org/dependencies-mindset-background.html#sec-dependencies-namespace and following sections."
  },
  {
    "objectID": "tutorials/tutorial-w09.html#fix-me",
    "href": "tutorials/tutorial-w09.html#fix-me",
    "title": "DAL tutorial - Week 9",
    "section": "2 Fix me!",
    "text": "2 Fix me!\nThe following code chunks have code that throws errors or generate mistakes (without throwing errors). Try to run them in R, study the error and fix the code!\nExample 1:\n\na &lt;- \"2\"\na + 1\n\nExample 2:\nNote: The goal of the following code chunk is to produce a tibble/data frame.\n\niris |&gt;\n  group_by(species) |&gt;\n  summary(\n    mean = mean(Petal.Lengths)\n  )\n\nExample 3:\n\nlirbary(tidyvrese)\n\nstarwars\n  mutate(\n    sex = fatcor(sex, level = c(\"female\", \"hermaphrodite\", \"male\", \"none\"))\n  ) |&gt; \n  count(Sex)\n\nDo all of the levels of the variable sex show up in your tibble/data frame of counts?\nExample 4:\n\niris |&gt; \n  ggplot(x = Species, y = Petal.Length) |&gt; \n  geom_jiter()\n\nExample 5:\n\nbat &lt;- tibble(\n  id = c(A, B, C),\n  response = rbinom(10, 1, 0.5)\n)\n\ncat(bat.response)\n\nExample 6:\n\nstarwars %&gt;% \n  filter(sex != is.na(sex)) %&gt;% \n  ggplot(aes(x = mass, y = height, colour = sex, shape = sex) +\n  geom_point() +\n  scale_x_log10()\n  labs(\n    x = \"Mass\"\n    y = \"Height\"\n    colour = \"Sex\"\n    shape = \"Sex\n  )\n\nExample 7:\n\ntucker2019 &lt;- read_csv(\"data/tucker2019/mald_1_1.rds\")\n\ntucker2019 %&gt;% \n  ggplot(aes(x = isword, y = RT, colour = ACC, fill = ACC)) +\n  geom_jitter(\n    alpha = 0.3,\n    position_jitterdodge(),\n  ) +\n  scale_colour_manual(values = c(\"#dc050c\", \"#196b50\", ))"
  },
  {
    "objectID": "tutorials/tutorial-w09.html#where-to-ask-for-help-online",
    "href": "tutorials/tutorial-w09.html#where-to-ask-for-help-online",
    "title": "DAL tutorial - Week 9",
    "section": "3 Where to ask for help online",
    "text": "3 Where to ask for help online\nThere are a few places you can ask for help online:\n\nStackOverflow is always a good place to start. You should first check if your question or a similar one has been answered already (in most cases, it has!) and if not you can post your question. Make sure you include a Minimal Working Example.\nAnother place is the Posit Community. It’s similar to StackOverflow, but it is specific to R and RStudio related things.\nYou can also ask ChatGPT for help. It is usually good for programming/coding stuff, but not so much for statistics.\nIf you have issues with specific R packages, you could open an issue on GitHub on the respective package repository to let the developer know about the issue.\nIn most cases, even just searching for your question using any web search engine will take you to the right places.\n\n\n\n\n\n\n\nDisclosing help in assessments\n\n\n\nNote that if you ask for help online as part of your assessment and you receive an answer (either by a human or by AI), you are obliged to disclose it in your assessment.\nAsking for help won’t impact your mark, but make sure that proper attribution is given in the assessment (for example, if somebody shared some code with you, you should acknowledge the author of the code)."
  },
  {
    "objectID": "tutorials/tutorial-w07.html",
    "href": "tutorials/tutorial-w07.html",
    "title": "DAL tutorial - Week 7",
    "section": "",
    "text": "For this tutorial, you should go through the following chapters of the R4DS textbook:\n\nR4DS Ch 5 on tidying data.\nR4DS Ch 19 on joining data.\n\nYou can read them in any order you like (even jump from one to the other). Make sure you try the code out yourself and feel free to try the exercises as well (you can find the solutions here.\nThese chapters cover functions and methods that you will need to complete Exercise 2 of Summative Assessment 1."
  },
  {
    "objectID": "tutorials/tutorial-w07.html#pivoting-and-joining-data",
    "href": "tutorials/tutorial-w07.html#pivoting-and-joining-data",
    "title": "DAL tutorial - Week 7",
    "section": "",
    "text": "For this tutorial, you should go through the following chapters of the R4DS textbook:\n\nR4DS Ch 5 on tidying data.\nR4DS Ch 19 on joining data.\n\nYou can read them in any order you like (even jump from one to the other). Make sure you try the code out yourself and feel free to try the exercises as well (you can find the solutions here.\nThese chapters cover functions and methods that you will need to complete Exercise 2 of Summative Assessment 1."
  },
  {
    "objectID": "tutorials/tutorial-w07.html#reading-multiple-files-at-once",
    "href": "tutorials/tutorial-w07.html#reading-multiple-files-at-once",
    "title": "DAL tutorial - Week 7",
    "section": "2 Reading multiple files at once",
    "text": "2 Reading multiple files at once\nAnother important skill to learn is how to read multiple files at once and save the output into a single tibble/data frame.\nThis can be achieved with the list.files() function.\nFor example, let’s read individual files with tongue contours data from ultrasound tongue imaging (UTI). These files are in data/coretta2018/ultrasound/.\nWe can list all files like so:\n\nlist.files(\"data/coretta2018/ultrasound\")\n\n [1] \"it01-tongue-cart.tsv\"  \"it01-vowel-series.tsv\" \"it02-tongue-cart.tsv\" \n [4] \"it02-vowel-series.tsv\" \"it03-tongue-cart.tsv\"  \"it03-vowel-series.tsv\"\n [7] \"it04-tongue-cart.tsv\"  \"it04-vowel-series.tsv\" \"it05-tongue-cart.tsv\" \n[10] \"it05-vowel-series.tsv\" \"it07-tongue-cart.tsv\"  \"it07-vowel-series.tsv\"\n[13] \"it09-tongue-cart.tsv\"  \"it09-vowel-series.tsv\" \"it11-tongue-cart.tsv\" \n[16] \"it11-vowel-series.tsv\" \"it12-tongue-cart.tsv\"  \"it12-vowel-series.tsv\"\n[19] \"it13-tongue-cart.tsv\"  \"it13-vowel-series.tsv\" \"it14-tongue-cart.tsv\" \n[22] \"it14-vowel-series.tsv\" \"pl02-tongue-cart.tsv\"  \"pl02-vowel-series.tsv\"\n[25] \"pl03-tongue-cart.tsv\"  \"pl03-vowel-series.tsv\" \"pl04-tongue-cart.tsv\" \n[28] \"pl04-vowel-series.tsv\" \"pl05-tongue-cart.tsv\"  \"pl05-vowel-series.tsv\"\n[31] \"pl06-tongue-cart.tsv\"  \"pl06-vowel-series.tsv\" \"pl07-tongue-cart.tsv\" \n[34] \"pl07-vowel-series.tsv\"\n\n\nBy default, the list.files() function returns just the name of the file, but we need the full path for the files to be read in R.\n\nlist.files(\"data/coretta2018/ultrasound\", full.names = TRUE)\n\n [1] \"data/coretta2018/ultrasound/it01-tongue-cart.tsv\" \n [2] \"data/coretta2018/ultrasound/it01-vowel-series.tsv\"\n [3] \"data/coretta2018/ultrasound/it02-tongue-cart.tsv\" \n [4] \"data/coretta2018/ultrasound/it02-vowel-series.tsv\"\n [5] \"data/coretta2018/ultrasound/it03-tongue-cart.tsv\" \n [6] \"data/coretta2018/ultrasound/it03-vowel-series.tsv\"\n [7] \"data/coretta2018/ultrasound/it04-tongue-cart.tsv\" \n [8] \"data/coretta2018/ultrasound/it04-vowel-series.tsv\"\n [9] \"data/coretta2018/ultrasound/it05-tongue-cart.tsv\" \n[10] \"data/coretta2018/ultrasound/it05-vowel-series.tsv\"\n[11] \"data/coretta2018/ultrasound/it07-tongue-cart.tsv\" \n[12] \"data/coretta2018/ultrasound/it07-vowel-series.tsv\"\n[13] \"data/coretta2018/ultrasound/it09-tongue-cart.tsv\" \n[14] \"data/coretta2018/ultrasound/it09-vowel-series.tsv\"\n[15] \"data/coretta2018/ultrasound/it11-tongue-cart.tsv\" \n[16] \"data/coretta2018/ultrasound/it11-vowel-series.tsv\"\n[17] \"data/coretta2018/ultrasound/it12-tongue-cart.tsv\" \n[18] \"data/coretta2018/ultrasound/it12-vowel-series.tsv\"\n[19] \"data/coretta2018/ultrasound/it13-tongue-cart.tsv\" \n[20] \"data/coretta2018/ultrasound/it13-vowel-series.tsv\"\n[21] \"data/coretta2018/ultrasound/it14-tongue-cart.tsv\" \n[22] \"data/coretta2018/ultrasound/it14-vowel-series.tsv\"\n[23] \"data/coretta2018/ultrasound/pl02-tongue-cart.tsv\" \n[24] \"data/coretta2018/ultrasound/pl02-vowel-series.tsv\"\n[25] \"data/coretta2018/ultrasound/pl03-tongue-cart.tsv\" \n[26] \"data/coretta2018/ultrasound/pl03-vowel-series.tsv\"\n[27] \"data/coretta2018/ultrasound/pl04-tongue-cart.tsv\" \n[28] \"data/coretta2018/ultrasound/pl04-vowel-series.tsv\"\n[29] \"data/coretta2018/ultrasound/pl05-tongue-cart.tsv\" \n[30] \"data/coretta2018/ultrasound/pl05-vowel-series.tsv\"\n[31] \"data/coretta2018/ultrasound/pl06-tongue-cart.tsv\" \n[32] \"data/coretta2018/ultrasound/pl06-vowel-series.tsv\"\n[33] \"data/coretta2018/ultrasound/pl07-tongue-cart.tsv\" \n[34] \"data/coretta2018/ultrasound/pl07-vowel-series.tsv\"\n\n\nYou see now the full path is return, relative to the Quarto Project directory.\nIn our case, we really just want to read the *-tongue-cart.tsv files, so we can specify a regular expression to list only those files that contain -tongue-cart.tsv.\n\nlist.files(\"data/coretta2018/ultrasound\", full.names = TRUE, pattern = \"*-tongue-cart.tsv\")\n\n [1] \"data/coretta2018/ultrasound/it01-tongue-cart.tsv\"\n [2] \"data/coretta2018/ultrasound/it02-tongue-cart.tsv\"\n [3] \"data/coretta2018/ultrasound/it03-tongue-cart.tsv\"\n [4] \"data/coretta2018/ultrasound/it04-tongue-cart.tsv\"\n [5] \"data/coretta2018/ultrasound/it05-tongue-cart.tsv\"\n [6] \"data/coretta2018/ultrasound/it07-tongue-cart.tsv\"\n [7] \"data/coretta2018/ultrasound/it09-tongue-cart.tsv\"\n [8] \"data/coretta2018/ultrasound/it11-tongue-cart.tsv\"\n [9] \"data/coretta2018/ultrasound/it12-tongue-cart.tsv\"\n[10] \"data/coretta2018/ultrasound/it13-tongue-cart.tsv\"\n[11] \"data/coretta2018/ultrasound/it14-tongue-cart.tsv\"\n[12] \"data/coretta2018/ultrasound/pl02-tongue-cart.tsv\"\n[13] \"data/coretta2018/ultrasound/pl03-tongue-cart.tsv\"\n[14] \"data/coretta2018/ultrasound/pl04-tongue-cart.tsv\"\n[15] \"data/coretta2018/ultrasound/pl05-tongue-cart.tsv\"\n[16] \"data/coretta2018/ultrasound/pl06-tongue-cart.tsv\"\n[17] \"data/coretta2018/ultrasound/pl07-tongue-cart.tsv\"\n\n\nThere’s another catch. These files don’t have column headings! We need to supply them ourselves as a character vector to the col_names argument of read_tsv(). Alternatively you can set that to FALSE and automatic column names will be created for you.\nFinally, we might want to create a new column on the fly which has the file path. This is helpful when the files you are reading don’t have a column that allows you to distinguish data from different files (in these files the first column do this for us).\nYou can create a new column with the path by specifying a name for this new column as the value of the id argument. With id = \"file\" a new column called file will be created with the path of the file.\n\nfiles &lt;- list.files(\n  \"data/coretta2018/ultrasound\",\n  full.names = TRUE,\n  pattern = \"*-tongue-cart.tsv\"\n)\n\n# Column names of the first 14 columns. The rest of the columns are X and Y\n# coordinates of tongue contours of 42 points along the contour:\n# X1,Y1,X2,Y2,X3,Y3,...,X42,Y42.\n#\n# Note that R automatically names unnamed columns with X followed by\n# the column number, so the 84 coordinate columns will be all named Xn.\ncolumns &lt;- c(\n  \"speaker\",\n  \"seconds\",\n  \"rec_date\",\n  \"prompt\",\n  \"label\",\n  \"TT_displacement_sm\",\n  \"TT_velocity\",\n  \"TT_velocity_abs\",\n  \"TD_displacement_sm\",\n  \"TD_velocity\",\n  \"TD_velocity_abs\",\n  \"TR_displacement_sm\",\n  \"TR_velocity\",\n  \"TR_velocity_abs\"\n)\n\ntongue &lt;- read_tsv(files, id = \"file\", col_names = columns, na = \"*\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 7598 Columns: 99\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (4): speaker, rec_date, prompt, label\ndbl (94): seconds, TT_displacement_sm, TT_velocity, TT_velocity_abs, TD_disp...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntongue\n\n\n  \n\n\n\nIf you are wondering what na = \"*\" does, it just tells R that cells with * in them should be treated as NAs.\nFab! Now why don’t you practice pivoting and change the tongue tibble from a wide format to a long format where each row has the X,Y coordinates of a single point out of the 42 points?\nI will give you a head start by renaming the columns with the X,Y coordinates for you. Make sure you inspect the code and understand what it does.\n\nnew_col_names &lt;- paste0(rep(c(\"X\", \"Y\"), 42), \"_\", rep(1:42, each = 2))\n\n# The square brakets [] are a way of extracting specific items from a vector.\n# This method is called \"indexing\" and specifically this is the \"braket indexing\"\n# method. See the Box below for a more detailed explanation.\nexisting_col_names &lt;- colnames(tongue)[1:15]\n\ncolnames(tongue) &lt;- c(existing_col_names, new_col_names)\n\nNow transform the data so that you end up with three new columns that replace all the X_n, Y_n columns: point with the point number (1 to 42), X with the x coordinate and Y with the y coordinate. You will need to use the separate() function to separate the X_n, Y_n names into X and n and Y and n.\n\n\n\n\n\n\nHint\n\n\n\n\n\nFollow this workflow:\n\nPivot all the Xn,Yn columns from wide to longer.\nSeparate the column that has X_n’s and X_y’s into two columns: one that has X or Y and one with the point number 1 to 42. You can use the separate() function to do so. Check ?separate.\nNow pivot from long to wider so that you get an X column and a Y column\n\nIt should look like this (only relevant columns shown):\n\n\n\n  \n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntongue |&gt; \n  pivot_longer(X_1:Y_42, names_to = \"coord\", values_to = \"value\") |&gt; \n  drop_na(value) |&gt; \n  separate(coord, c(\"axis\", \"point\")) |&gt; \n  pivot_wider(names_from = axis, values_from = value) |&gt; \n  select(prompt, point, X, Y)"
  },
  {
    "objectID": "tutorials/tutorial-w05.html",
    "href": "tutorials/tutorial-w05.html",
    "title": "DAL tutorial - Week 5",
    "section": "",
    "text": "Data transformation is a fundamental aspect of data analysis.\nAfter the data you need to use is imported into R, you will have to filter rows, create new columns, or join data frames, among many other transformation operations.\nIn this tutorial we will learn how to filter() the data and mutate() or create new columns. In Week 6 (after Flexible Learning week) you will learn how to obtain summary measures and how to count occurrences using the summarise(), group_by() and count() functions."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#data-transformation",
    "href": "tutorials/tutorial-w05.html#data-transformation",
    "title": "DAL tutorial - Week 5",
    "section": "",
    "text": "Data transformation is a fundamental aspect of data analysis.\nAfter the data you need to use is imported into R, you will have to filter rows, create new columns, or join data frames, among many other transformation operations.\nIn this tutorial we will learn how to filter() the data and mutate() or create new columns. In Week 6 (after Flexible Learning week) you will learn how to obtain summary measures and how to count occurrences using the summarise(), group_by() and count() functions."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#filter",
    "href": "tutorials/tutorial-w05.html#filter",
    "title": "DAL tutorial - Week 5",
    "section": "2 Filter",
    "text": "2 Filter\nFiltering data based on specific criteria couldn’t be easier with filter(), from the dplyr package (one of the tidyverse core packages),\nLet’s work with the coretta2022/glot_status data frame. It’s an .rds file, so you need to use the readRDS() function. Go ahead and read the data into glot_status.\nThe glot_status data frame contains the endangerment status for 7,845 languages from Glottolog. There are thousands of languages in the world, but most of them are losing speakers, and some are already no longer spoken. The endangerment status of a language in the data is on a scale from not endangered (languages with large populations of speakers) through threatened, shifting and nearly extinct, to extinct (languages that have no living speakers left).\n\nglot_status\n\n\n  \n\n\n\nBefore we can move on onto filtering data, we first need to learn about logical operators.\n\n2.1 Logical operators\nThere are four main logical operators:\n\nx == y: x equals y.\nx != y: x is not equal to y.\nx &gt; y: x is greater than y.\nx &lt; y: x is smaller than y.\n\nLogical operators return TRUE or FALSE depending on whether the statement they convey is true or false. Remember, TRUE and FALSE are logical values.\nTry these out in the Console:\n\n# This will return FALSE\n1 == 2\n\n[1] FALSE\n\n# FALSE\n\"apples\" == \"oranges\"\n\n[1] FALSE\n\n# TRUE\n10 &gt; 5\n\n[1] TRUE\n\n# FALSE\n10 &gt; 15\n\n[1] FALSE\n\n# TRUE\n3 &lt; 4\n\n[1] TRUE\n\n\n\n\n\n\n\n\nLogical operators\n\n\n\nLogical operators are symbols that compare two objects and return either TRUE or FALSE.\nThe most common logical operators are ==, !=, &gt;, and &lt;.\n\n\n\n\n\n\n\n\nQuiz 2\n\n\n\n\nWhich of the following does not contain a logical operator?\n\n 3 &gt; 1 \"a\" = \"a\" \"b\" != \"b\" 19 &lt; 2\n\nWhich of the following returns c(FALSE, TRUE)?\n\n 3 &gt; c(1, 5) c(\"a\", \"b\") != c(\"a\") \"apple\" != \"apple\"\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n2a.\nCheck for errors in the logical operators.\n2b.\nRun them in the console to see the output.\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n2a.\nThe logical operator == has TWO equal signs. A single equal sign = is an alternative way of writing the assignment operator &lt;-, so that a = 1 and a &lt;- 1 are equivalent.\n2b.\nLogical operators are “vectorised” (you will learn more about this below), i.e they are applied sequentially to all elements in pairs. If the number of elements on one side does not match than of the other side of the operator, the elements on the side that has the smaller number of elements will be recycled.\n\n\n\n\n\nNow let’s see how these work with filter()!\n\n\n2.2 The filter() function\nFiltering in R with the tidyverse is straightforward. You can use the filter() function.\nfilter() takes one or more statements with logical operators.\nLet’s try this out. The following code filters the status column so that only the extinct status is included in the new data frame extinct.\nYou’ll notice we are using the pipe |&gt; to transfer the data into the filter() function; the output of the filter function is assigned &lt;- to extinct. The flow might seem a bit counter-intuitive but you will get used to think like this when writing R code soon enough!\n\nextinct &lt;- glot_status |&gt;\n  filter(status == \"extinct\")\n\nextinct\n\n\n  \n\n\n\nNeat! What if we want to include all statuses except extinct? Easy, we use the non-equal operator !=.\n\nnot_extinct &lt;- glot_status |&gt;\n  filter(status != \"extinct\")\n\nnot_extinct\n\n\n  \n\n\n\nAnd if we want only non-extinct languages from South America? We can include multiple statements separated by a comma!\n\nsouth_america &lt;- glot_status |&gt;\n  filter(status != \"extinct\", Macroarea == \"South America\")\n\nsouth_america\n\n\n  \n\n\n\nCombining statements like this will give you only those rows where both conditions apply. You can add as many statements as you need.\nNow try to filter the data so that you include only not_endangered languages from all macro-areas except Eurasia. This time, don’t save the output to a new data frame. What happens? Where is the output shown?\n\nglot_status |&gt;\n  filter(...)\n\nThis is all great, but what if we want to include more than one status or macro-area?\nTo do that we need another operator: %in%.\n\n\n2.3 The %in% operator\n\n\n\n\n\n\n%in%\n\n\n\nThe %in% operator is a special logical operator that returns TRUE if the value to the left of the operator is one of the values in the vector to its right, and FALSE if not.\n\n\nTry these in the Console:\n\n# TRUE\n5 %in% c(1, 2, 5, 7)\n\n[1] TRUE\n\n# FALSE\n\"apples\" %in% c(\"oranges\", \"bananas\")\n\n[1] FALSE\n\n\nBut %in% is even more powerful because the value on the left does not have to be a single value, but it can also be a vector! We say %in% is vectorised because it can work with vectors (most functions and operators in R are vectorised).\n\n# TRUE, TRUE\nc(1, 5) %in% c(4, 1, 7, 5, 8)\n\n[1] TRUE TRUE\n\nstocked &lt;- c(\"durian\", \"bananas\", \"grapes\")\nneeded &lt;- c(\"durian\", \"apples\")\n\n# TRUE, FALSE\nneeded %in% stocked\n\n[1]  TRUE FALSE\n\n\nTry to understand what is going on in the code above before moving on.\n\n\n2.4 Now filter the data\nNow we can filter glot_status to include only the macro-areas of the Global South and only languages that are either “threatened”, “shifting”, “moribund” or “nearly_extinct”. I have started the code for you, you just need to write the line for filtering status.\n\nglobal_south &lt;- glot_status |&gt;\n  filter(\n    Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"),\n    ...\n  )\n\nThis should not look too alien! The first statement, Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\") looks at the Macroarea column and, for each row, it returns TRUE if the current row value is in c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"), and FALSE if not."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#bar-charts",
    "href": "tutorials/tutorial-w05.html#bar-charts",
    "title": "DAL tutorial - Week 5",
    "section": "3 Bar charts",
    "text": "3 Bar charts\n\n\n\n\n\n\nBar charts\n\n\n\nBar charts are useful when you are counting things. For example:\n\nNumber of verbs vs nouns vs adjectives in a corpus.\nNumber of languages by geographic area.\nNumber of correct vs incorrect responses.\n\nThe bar chart geometry is geom_bar().\n\n\nWe will first create a plot with counts of the number of languages in global_south by their endangerment status and then a plot where we also split the counts by macro-area.\n\n3.1 Number of languages of the Global South by status\nTo create a bar chart, you can use the geom_bar() geometry.\n\n\n\n\n\n\nBar chart axes\n\n\n\nIn a simple bar chart, you only need to specify one axis, the x-axis, in the aesthetics aes().\nThis is because the counts that are placed on the y-axis are calculated by the geom_bar() function under the hood.\nThis quirk is something that confuses many new learners, so make sure you internalise this.\n\n\nGo ahead and complete the following code to create a bar chart.\n\nglobal_south |&gt;\n  ggplot(aes(x = status)) +\n  ...\n\nNote how we’re using |&gt; to pipe the glot_status data frame into the ggplot() function. This works because ggplot()’s first argument is the data, and piping is a different way of providing the first argument to a function.\nAs mentioned above, the counting for the y-axis is done automatically. R looks in the status column and counts how many times each value in the column occurs in the data frame.\nIf you did things correctly, you should get the following plot.\n\n\n\n\n\n\n\n\nFigure 1: Number of languages by endangerment status.\n\n\n\n\n\nThe x-axis is now status and the y-axis corresponds to the number of languages by status (count). As mentioned above, count is calculated under the hood for you (you will learn how to count levels with count() later in the course).\nYou could write a description of the plot that goes like this:\n\nThe number of languages in the Global South by endangered status is shown as a bar chart in Figure 1. Among the languages that are endangered, the majority are threatened or shifting.\n\nWhat if we want to show the number of languages by endangerment status within each of the macro-areas that make up the Global South? Easy! You can make a stacked bar chart, but we will get to that after we first learn about mutate()."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#mutate",
    "href": "tutorials/tutorial-w05.html#mutate",
    "title": "DAL tutorial - Week 5",
    "section": "4 Mutate",
    "text": "4 Mutate\nTo change existing columns or create new columns, we can use the mutate() function from the dplyr package.\nTo learn how to use mutate(), we will re-create the status column (let’s call it Status this time) from the Code_ID column in glot_status.\nThe Code_ID column contains the status of each language in the form aes-STATUS where STATUS is one of not_endangered, threatened, shifting, moribund, nearly_extinct and extinct.\n\n\n[1] \"aes-shifting\"       \"aes-extinct\"        \"aes-moribund\"      \n[4] \"aes-nearly_extinct\" \"aes-threatened\"     \"aes-not_endangered\"\n\n\nWe want to create a new column called Status which has only the STATUS label (without the aes- part). To remove aes- from the Code_ID column we can use the str_remove() function from the stringr package. Check the documentation of ?str_remove to learn which arguments it uses.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = str_remove(Code_ID, \"aes-\")\n  )\n\nIf you check glot_status now you will find that a new column, Status, has been added. This column is a character column (chr).\nLet’s reproduce the bar chart from above but with all the data from glot_status, using now the Status column.\n\nglot_status |&gt;\n  ggplot(aes(x = Status)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 2: Number of languages by endangerment status (repeated).\n\n\n\n\n\nBut something is not quite right… The order of the levels of Status does not match the order that makes sense (from least to most endangered)! Why?\nThis is because status (the pre-existing column) is a factor column, rather than a simple character column. What is a factor vector/column?\n\n\n\n\n\n\nFactor vector\n\n\n\nA factor vector (or column) is a vector that contains a list of values (called levels) from a closed set.\nThe levels of a factor are ordered alphabetically by default.\n\n\nA vector/column can be mutated into a factor column with the as.factor() function. In the following code, we change the existing column Status, in other words we overwrite it (this happens automatically, because the Status column already exists, so it is replaced).\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = as.factor(Status)\n  )\n\n# read below for an explanation of the dollar disgn $ syntax\nlevels(glot_status$Status)\n\n[1] \"extinct\"        \"moribund\"       \"nearly_extinct\" \"not_endangered\"\n[5] \"shifting\"       \"threatened\"    \n\n\nThe levels() functions returns the levels of a factor column in the order they are stored in the factor: by default the order is alphabetical. But wait, what is that $ in glot_status$Status?\nThe dollar sign $ a base R way of extracting a single column (in this case Status) from a data frame (glot_status).\n\n\n\n\n\n\nThe dollar sign `$`\n\n\n\nYou can use the dollar sign $ to extract a single column from a data frame as a vector.\n\n\nWhat if we want the levels of Status to be ordered in a more logical manner: not_endangered, threatened, shifting, moribund, nearly_extinct and extinct? Easy! We can use the factor() function instead of as.factor() and specify the levels and their order.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = factor(Status, levels = c(\"not_endangered\", \"threatened\", \"shifting\", \"moribund\", \"nearly_extinct\", \"extinct\"))\n  )\n\nlevels(glot_status$Status)\n\n[1] \"not_endangered\" \"threatened\"     \"shifting\"       \"moribund\"      \n[5] \"nearly_extinct\" \"extinct\"       \n\n\nYou see that now the order of the levels returned by levels() is the one we specified.\nTransforming character columns to vector columns is helpful to specify a particular order of the levels which can then be used when plotting.\n\nglot_status |&gt;\n  ggplot(aes(x = Status)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 3: Number of languages by endangerment status (repeated)."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#stacked-bar-charts",
    "href": "tutorials/tutorial-w05.html#stacked-bar-charts",
    "title": "DAL tutorial - Week 5",
    "section": "5 Stacked bar charts",
    "text": "5 Stacked bar charts\nA special type of bar charts are the so-called stacked bar charts.\n\n\n\n\n\n\nStacked bar chart\n\n\n\nA stacked bar chart is a bar chart in which each contains a “stack” of shorter bars, each indicating the counts of some sub-groups.\nThis type of plot is useful to show how counts of something vary depending on some other grouping (in other words, when you want to count the occurrences of a categorical variable based on another categorical variable). For example:\n\nNumber of languages by endangerment status, grouped by geographic area.\nNumber of infants by head-turning preference, grouped by first language.\nNumber of past vs non-past verbs, grouped by verb class.\n\n\n\nTo create a stacked bar chart, you just need to add a new aesthetic mapping to aes(): fill. The fill aesthetic lets you fill bars or areas with different colours depending on the values of a specified column.\nLet’s make a plot on language endangerment by macro-area.\nComplete the following code by specifying that fill should be based on status.\n\nglobal_south |&gt;\n  ggplot(aes(x = Macroarea, ...)) +\n  geom_bar()\n\nYou should get the following.\n\n\n\n\n\n\n\n\nFigure 4: Number of languages by macro-area and endangerment status.\n\n\n\n\n\nA write-up example:\n\nFigure 4 shows the number of languages by geographic macro-area, subdivided by endangerment status. Africa, Eurasia and Papunesia have substantially more languages than the other areas.\n\n\n\n\n\n\n\nQuiz 4\n\n\n\nWhat is wrong in the following code?\ngestures |&gt;\n  ggplot(aes(x = status), fill = Macroarea) +\n  geom_bar()"
  },
  {
    "objectID": "tutorials/tutorial-w05.html#filled-stacked-bar-charts",
    "href": "tutorials/tutorial-w05.html#filled-stacked-bar-charts",
    "title": "DAL tutorial - Week 5",
    "section": "6 Filled stacked bar charts",
    "text": "6 Filled stacked bar charts\nIn the plot above it is difficult to assess whether different macro-areas have different proportions of endangerment. This is because the overall number of languages per area differs between areas.\nA solution to this is to plot proportions instead of raw counts.\nYou could calculate the proportions yourself, but there is a quicker way: using the position argument in geom_bar().\nYou can plot proportions instead of counts by setting position = \"fill\" inside geom_bar(), like so:\n\nglobal_south |&gt;\nggplot(aes(x = Macroarea, fill = status)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nFigure 5: Proportion of languages by macro-area and endangerment status.\n\n\n\n\n\nThe plot now shows proportions of languages by endangerment status for each area separately.\nNote that the y-axis label is still “count” but should be “proportion”. Use labs() to change the axes labels and the legend name.\n\nglobal_south |&gt;\nggplot(aes(x = Macroarea, fill = status)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    ...\n  )\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIf to change the name of the colour legend, you use the colour argument in labs(), guess which argument you should use for fill?\n\n\n\nYou should get this.\n\n\n\n\n\n\n\n\nFigure 6: Proportion of languages by macro-area and endangerment status.\n\n\n\n\n\nWith this plot it is easier to see that different areas have different proportions of endangerment. In writing:\n\nFigure 6 shows proportions of languages by endangerment status for each macro-area. Australia, South and North America have a substantially higher proportion of extinct languages than the other areas. These areas also have a higher proportion of near extinct languages. On the other hand, Africa has the greatest proportion of non-endangered languages followed by Papunesia and Eurasia, while North and South America are among the areas with the lower proportion, together with Australia which has the lowest."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#faceting-and-panels",
    "href": "tutorials/tutorial-w05.html#faceting-and-panels",
    "title": "DAL tutorial - Week 5",
    "section": "7 Faceting and panels",
    "text": "7 Faceting and panels\nSometimes we might want to separate the data into separate panels within the same plot.\nWe can achieve that easily using faceting.\n\n7.1 Polite again\nLet’s reproduce this plot from Week 4, but this time let’s spice things up.\nThis is the plot you made in Week 4. Try and reproduce it by writing the code yourself (you also have to read in the data!).\n\n\n\n\n\n\n\n\nFigure 7: Scatter plot of mean f0 and H1-H2 difference.\n\n\n\n\n\n\n\n7.2 Does being a music student matter?\nThat looks great, but we want to know if being a music student has an effect on the relationship of f0mn and H1H2.\nIn the plot above, the aesthetics mappings are the following:\n\nf0mn on the x-axis.\nH1H2 on the y-axis.\ngender as colour.\n\nHow can we separate data further depending on whether the participant is a music student or not (musicstudent)?\nWe can create panels using facet_grid(). This function takes lists of variables to specify panels in rows and/or columns.\n\n7.2.1 Faceting\nFaceting a plot allows to split the plot into multiple panels, arranged in rows and columns, based on one or more variables.\nTo facet a plot, use the facet_grid() function.\nThe syntax is a bit strange. You can specify rows of panels with the rows argument and columns of panels with cols argument, but you have to include column names inside vars(), like this:\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  facet_grid(cols = vars(musicstudent)) +\n  labs(\n    x = \"Mean f0 (Hz)\",\n    y = \"H1-H2 difference (dB)\",\n    colour = \"Gender\"\n  )\n\n\n\n\n\n\n\nFigure 8: Scatter plot of mean f0 and H1-H2 difference in non-music students (left) vs music students (right).\n\n\n\n\n\nYou could write a description of this plot like this:\n\nFigure 8 shows mean f0 and H1-H2 difference as a scatter plot. The two panels indicate whether the participant was a student of music. Within each panel, the participant’s gender is represented by colour (red for female and blue for male). Male participants tend to have higher H1-H2 differences and lower mean f0 than females. From the plot it can also be seen that there is greater variability in H1-H2 difference in female music students compared to female non-music participants. Within each group of gender by music student there does not seem to be any specific relation between mean f0 and H1-H2 difference.\n\nThe polite data also has a column attitude with values inf for informal and pol for polite. Subjects were asked to speak either as if they were talking to a friend (inf attitude) or to someone with a higher status (pol attitude).\nRecreate the last plot, this time faceting also by attitude. Use the rows column to create two separate rows for each value of attitude.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  facet_grid(cols = vars(musicstudent), rows = ...)\n\nNow write a description of the plot."
  },
  {
    "objectID": "tutorials/tutorial-w03.html",
    "href": "tutorials/tutorial-w03.html",
    "title": "DAL tutorial - Week 3",
    "section": "",
    "text": "Important\n\n\n\nWhen working through these tutorials, always make sure you are in the course’s RStudio Quarto Project you created.\nYou know you are in an RStudio Project because you can see the name of the Project in the top-right corner of RStudio, next to the light blue cube icon.\nIf you see Project (none) in the top-right corner, that means your are not in an RStudio Project.\nTo make sure you are in the RStudio project, to open the project go to the project folder in File Explorer or Finder and double click on the .Rproj file."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#some-computer-basics",
    "href": "tutorials/tutorial-w03.html#some-computer-basics",
    "title": "DAL tutorial - Week 3",
    "section": "1 Some computer basics",
    "text": "1 Some computer basics\nIn the tutorial last week you’ve been playing around with R, RStudio and R scripts.\nBut what if you want to import data in R?\nEasy! You can use the read_*() functions (* is just a place holder for specific types of files to read, like read_csv() or read_excel()) to read your files into R. But before we dive in, let’s first talk about some computer basics. (You can skip this section if it’s too basic for you.)\n\n\n\n\n\n\nEnable all extensions\n\n\n\nBefore moving on, we recommend you enable the option to show all file extensions in the File Explorer/Finder.\nFollow the instructions here:\n\nWindows: https://support.microsoft.com/en-us/windows/common-file-name-extensions-in-windows-da4a4430-8e76-89c5-59f7-1cdbbc75cb01\nmacOS (For all files): https://support.apple.com/en-gb/guide/mac-help/mchlp2304/mac#:~:text=In%20the%20Finder%20on%20your,“Show%20all%20filename%20extensions”.\n\n\n\n\n1.1 Files, folder and file extensions\nFiles saved on your computer live in a specific place. For example, if you download a file from a browser (like Google Chrome, Safari or Firefox), the file is normally saved in the Download folder.\nBut where does the Download folder live? Usually, in your user folder! The user folder normally is the name of your account or a name you picked when you created your computer account. In my case, my user folder is simply called ste.\n\n\n\n\n\n\nUser folder\n\n\n\nThe user folder is the folder with the name of your account.\n\n\n\n\n\n\n\n\nHow to find your user folder name\n\n\n\n\n\nOn macOS\n\nGo to Finder &gt; Preferences/Settings.\nGo to Sidebar.\nThe name next to the house icon is the name of your home folder.\n\nOn Windows\n\nRight-click an empty area on the navigation panel in File Explorer.\nFrom the context menu, select the ‘Show all folders’ and your user profile will be added as a location in the navigation bar.\n\n\n\n\nSo, let’s assume I download a file, let’s say big_data.csv, in the Download folder of my user folder.\nNow we can represent the location of the big_data.csv file like so:\nste/\n└── Downloads/\n    └── big_data.csv\nTo mark that ste and Downloads are folders, we add a final forward slash /. That simply means “hey! I am a folder!”. big_data.csv is a file, so it doesn’t have a final /.\nInstead, the file name big_data.csv has a file extension. The file extension is .csv. A file extension marks the type of file: in this the big_data file is a .csv file, a comma separated value file (we will see an example of what that looks like later).\nDifferent file type have different file extensions:\n\nExcel files: .xlsx.\nPlain text files: .txt.\nImages: .png, .jpg, .gif.\nAudio: .mp3, .wav.\nVideo: .mp4, .mov, .avi.\nEtc…\n\n\n\n\n\n\n\nFile extension\n\n\n\nA file extension is a sequence of letters that indicates the type of a file and it’s separated with a . from the file name.\n\n\n\n\n1.2 File paths\nNow, we can use an alternative, more succinct way, to represent the location of the big_data.csv:\nste/Downloads/big_data.csv\nThis is called a file path! It’s the path through folders that lead you to the file. Folders are separated by / and the file is marked with the extension .csv.\n\n\n\n\n\n\nFile path\n\n\n\nA file path indicates the location of a file on a computer as a path through folders that lead you to the file.\n\n\nNow the million pound question: where does ste/ live on my computer???\nUser folders are located in different places depending on the operating system you are using:\n\nOn macOS: the user folder is in /Users/.\n\nYou will notice that there is a forward slash also before the name of the folder. That is because the /Users/ folder is a top folder, i.e. there are no folders further up in the hierarchy of folders.\nThis means that the full path for the big_data.csv file on a computer running macOS would be: /Users/ste/Downloads/big_data.csv.\n\nOn Windows: the user folder is in C:/Users/\n\nYou will notice that C is followed by a colon :. That is because C is a drive, which contains files and folders. C: is not contained by any other folder, i.e. there are no other folders above C: in the hierarchy of folders.\nThis means that the full path for the big_data.csv file on a Windows computer would be: C:/Users/ste/Downloads/big_data.csv.\n\n\nWhen a file path starts from a top-most folder, we call that path the absolute file path.\n\n\n\n\n\n\nAbsolute path\n\n\n\nAn absolute path is a file path that starts with a top-most folder.\n\n\nThere is another type of file paths, called relative paths. A relative path is a partial file path, relative to a specific folder. You will learn how to use relative paths below, when we will go through importing files in R using R scripts below.\nImporting files in R is very easy with the tidyverse packages. You just need to know the file type (very often the file extension helps) and the location of the file (i.e. the file path).\nThe next sections will teach you how to import data in R!\n\n\n\n\n\n\nQuiz 1\n\n\n\nWhich of the following is an absolute path?\n\n Downloads/courses/dal/data/ /Users/smruti/Downloads/data/files/ sascha/Documents/files_pdf/paper.pdf"
  },
  {
    "objectID": "tutorials/tutorial-w03.html#data-types",
    "href": "tutorials/tutorial-w03.html#data-types",
    "title": "DAL tutorial - Week 3",
    "section": "2 Data types",
    "text": "2 Data types\n\n2.1 Tabular data\n\n\n\n\n\n\nTabular data\n\n\n\nTabular data is data that has a form of a table: i.e. values structured in columns and rows.\n\n\nMost of the data we will be using in this course will be tabular and the files will be in the .csv format.\nThe comma separated values format (.csv) is the best format to save data in because it is basically a plain text file, it’s quick to parse, and can be opened and edited with any software (plus, it’s not a proprietary format like .docx or .xlsx—these formats are specific to particular software).\nThis is what a .csv file looks like when you open it in a text editor (showing only the first few lines).\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\nThe file contains tabular data (data that is structured as columns and rows, like a spreadsheet).\nTo separate the values of each column, a .csv file uses a comma , (hence the name “comma separated values”) to separate the values in every row.\nThe first line of the file indicates the names of the columns of the table:\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\nThere are 11 columns. The rest of the rows is the data, i.e. the values of each column separated by commas.\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\nThis might look a bit confusing, but you will see later that, after importing this type of file, you can view it as a nice spreadsheet (as you would in Excel).\nAnother common type of tabular data file is spreadsheets, like spreadsheets created by Microsoft Excel or Apple Numbers. These are all proprietary formats that require you to have the software that were created with if you want to modify them.\nPortability and openness are important aspects of conducting ethical research, so that using open and non-proprietary file types makes your research more accessible and doesn’t privilege those who have access to specific software (remember, R is free!).\nThere are also variations of the comma separated values type, like tab separated values files (.tsv, which uses tab characters instead of commas) and fixed-width files (usually .txt, where columns are separated by as many white spaces as needed so that the columns align).\n\n\n2.2 Non-tabular data\nOf course, R can import also data that is not tabular, like map data and complex hierarchical data.\nWe will dip our toes into map data at the end of course, but virtually all of the data we will use will be tabular, just because that’s the format you need to do data visualisation and analyses.\n\n\n2.3 .rds files\nR has a special way of saving data: .rds files.\n.rds files allow you to save an R object to a file on your computer, so that you can read that file in when you need it.\nA common use for .rds files is to save tabular data that you have processed so that it can be readily used in many different scripts or even by other people.\nIn the following sections you will learn how to import (aka read) three types of data: .csv, Excel and .rds files."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#download-the-data-files",
    "href": "tutorials/tutorial-w03.html#download-the-data-files",
    "title": "DAL tutorial - Week 3",
    "section": "3 Download the data files",
    "text": "3 Download the data files\nThroughout the course we will be using data files that come from linguistic research. You should download now the data files according to the following instructions\nPlease, follow these instructions carefully.\n\nDownload the zip archive with all the data by right-clicking on the following link and download the file: data.zip.\nUnzip the zip file to extract the contents. (If you don’t know how to do this, ask one of the tutors to help you!)\nCreate a folder called data/ (the slash is there just to remind you that it’s a folder, but you don’t have to include it in the name) in the Quarto project you are using for the course.\n\nTo create a folder, go to the Files tab of the bottom-right panel in RStudio.\nMake sure you are viewing the project’s main folder.\nClick on the New Folder button, enter “data” in the text box and click OK\n\nMove the contents of the data.zip archive into the data/ folder.\n\nOpen a Finder or File Explorer window.\nNavigate to the folder where you have extracted the zip file (it will very likely be the Downloads/ folder).\nCopy the contents of the zip file.\nIn Finder or File Explorer, navigate to the Quarto project folder, then the data/ folder, and paste the contents in there. (You can also drag and drop if you prefer.)\n\n\nThe rest of the tutorial will assume that you have created a folder called data/ in the Quarto project folder and that the files you downloaded are in that folder. The data folder should like something like this:\ndata/\n└── cameron2020/\n    └── gestures.csv\n└── coretta2018/\n    └── formants.csv\n    └── token-measures.csv\n└── ...\nI recommend that you start being very organised with your files in other projects from now on, whether it’s for this course or your dissertation or else. I also suggest to avoid overly nested structures (for example, avoid having one folder for each week for this course. Rather, save all data files in the data/ folder).\n\n\n\n\n\n\nOrganising your files\n\n\n\n\n\nThe Open Science Framework has the following recommendations that apply very well to any type of research project.\n\nUse one folder per project. This will also be your RStudio project folder.\nSeparate raw data from derived data.\nSeparate code from data.\nMake raw data read-only.\n\nTo learn more about this, check the OSF page Organising files.\nIn brief, what these recommendations mean is that you want a folder for your research project/course/else, and inside the folder two folders: one for data and one for code.\nThe data/ folder could further contain raw/ for raw data (data that should not be lost or changed, for example collected data or annotations) and derived/ for data that derives from the raw data, for example through automated data processing.\nI usually also have a separate folder called figs/ or img/ where I save plots. Of course which folders you will have it’s ultimately up to you and needs will vary depending on the project and field!"
  },
  {
    "objectID": "tutorials/tutorial-w03.html#import-.csv-files",
    "href": "tutorials/tutorial-w03.html#import-.csv-files",
    "title": "DAL tutorial - Week 3",
    "section": "4 Import .csv files",
    "text": "4 Import .csv files\nLet’s start with data from this paper: Song et al. 2020. Second language users exhibit shallow morphological processing. DOI: 10.1017/S0272263120000170.\nThe study consisted of a lexical decision task in which participants were first shown a prime, followed by a target word for which they had to indicate whether it was a real word or a nonce word.\nThe prime word belonged to one of three possible groups (Relation_type in the data) each of which refers to the morphological relation of the prime and the target word:\n\nUnrelated: for example, prolong (assuming unkindness as target, [[un-kind]-ness]).\nConstituent: unkind.\nNonConstituent: kindness.\n\n\n4.1 The tidyverse packages\nImporting .csv files is very easy. You can use the read_csv() function from a collection of R packages known as the tidyverse.\nTo import data in R we will use the read_csv() function from the readr package, one of the tidyverse packages.\nInstalling the tidyverse packages is easy: you just need to install the tidyverse package and that will take care of installing the most important packages in the collection (called the “core” tidyverse packages).\nGo ahead and install the tidyverse from the Packages tab.1\n\n\n4.2 read_csv()\n\n\n\n\n\n\nDid you open the RStudio Quarto project?\n\n\n\nBefore moving on, make sure that you have opened the RStudio Quarto project correctly (see warning at the top of the tutorial)\n\n\nThe read_csv() function from the readr package only requires you to specify the file path as a string (remember, strings are quoted between \" \", for example \"year_data.txt\"). On my computer, the file path of song2020/shallow.csv is /Users/ste/dal/data/song2020/shallow.csv, but on your computer the file path will be different, of course.\nAlso, note that it is not enough to use the read_csv() function. You also must assign the output of the read_csv() function (i.e. the data we are reading) to a variable, using the assignment arrow &lt;-, just like we were assigning values to variables in the previous weeks.\nAnd since the read_csv() is a function from the tidyverse, you first need to attach the tidyverse packages with library(tidyverse) (remember, you need to attach packages only once per session). This will attach the core tidyverse packages, including readr. Of course, you can also attach the individual packages directly: library(readr). If you use library(tidyverse) there is no need to attach individual tidyverse packages.\nBefore reading the data, create a new R script named tutorial_w03.R and save it in the code/ folder of your Quarto project.\nGenerally, you start the script with calls to library() to load all the packages you need for the script.\nNow we only need one package, tidyverse, but in most cases you will need more than one! The best practice is to attach all of packages first, in the top of your script. Please, get in the habit of doing this from now, so that you can keep your scripts tidy and pretty!\n\n\n\n\n\n\nWarning\n\n\n\nPlease, don’t include install.packages() in your R scripts!\nRemember, you only have to install a package once, and you can just type it in the Console.\nBut DO include library() in your scripts.\n\n\nAt the top of tutorial_w03.R, write the following lines of code. Then run the code.\n\nlibrary(tidyverse)\n\nshallow &lt;- read_csv(\"./data/song2020/shallow.csv\")\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you look at the Environment tab, you will see song2020 under Data.\n\n\n\n\n\n\nData frames and tibbles\n\n\n\nIn R, a data table is called a data frame.\nTibbles are special data frame created with the read functions from the tidyverse. If you are curious about the difference, check this page.\nIn this course’s tutorials, “data frame” and “tibble” will be used interchangeably (since we are using the read functions from the tidyverse, all resulting data frames will be tibbles).\n\n\nBut wait, what is that \"./data/song2020/shallow.csv\"? That’s a relative path. We briefly mentioned relative paths above, but let’s understand the details now. You will be able to view the data soon.\n\n\n4.3 Relative paths\n\n\n\n\n\n\nRelative path\n\n\n\nA relative path is a file path that is relative to a folder. The folder the path starts at is represented by ./.\n\n\nWhen you are using R scripts in Quarto projects, the ./ folder paths are relative to is the project folder! This is true whichever the name of the folder/project and whichever it’s location on your computer.\nFor example, if your project it’s called awesome_proj and it’s in Downloads/stuff/, then if you write ./data/results.csv you really mean Downloads/stuff/awesome_proj/data/results.csv!\nHow does R know the path is relative to the project folder?\nThat is because when working with Quarto projects, all relative paths are relative to the project folder (i.e. the folder with the .Rproj file)!\nThe folder which relative paths are relative to is called the working directory (directory is just another way of saying folder).\n\n\n\n\n\n\nWorking directory\n\n\n\nThe working directory is the folder which relative paths are relative to.\nWhen using Quarto projects, the working directory is the project folder.\n\n\nThe code read_csv(\"./data/song2020/shallow.csv\") above will work because you are using a Quarto project and inside the project folder there is a folder called data/ and in it there’s the song2020/shallow.csv file.\nSo from now on I encourage you to use Quarto projects, R scripts and relative paths always!\nThe benefit of doing so is that, if you move your project or rename it, or if you share the project with somebody, all the paths will just work because they are relative!\n\n\n\n\n\n\nGet the working directory\n\n\n\nYou can get the current working directory with the getwd() command.\nRun it now in the Console! Is the returned path the project folder path?\nIf not, it might be that you are not working from a Quarto project. Check the top-right corner of RStudio: is the project name in there or do you see Project (none)?\nIf it’s the latter, you are not in a Quarto project, but you are running R from somewhere else (meaning, the working directory is somewhere else). If so, close RStudio and open the project.\n\n\n\n\n4.4 View the data\nNow we can finally view the data.\nThe easiest way is to click on the name of the data listed in the Environment tab, in the top-right panel of RStudio.\nYou will see a nicely formatted table, as you would in a programme like Excel.\nData tables in R (i.e. tabular, spread-sheet like data) are called data frames or tibbles.2\nThe shallow data frame contains 11 columns (called variables in the Environment tab). The 11 columns are the following:\n\nGroup: L1 vs L2 speakers of English.\nID: Subject unique ID.\nList: Word list (A to F).\nTarget: Target word in the lexical decision trial.\nACC: Lexical decision response accuracy (0 incorrect response, 1 correct response).\nRT: Reaction times of response in milliseconds.\nlogRT: Logged reaction times.\nCritical_Filler: Whether the trial was a filler or critical.\nWord_Nonword: Whether the Target was a real Word or a Nonword.\nRelation_type: The type of relation between prime and target word (Unrelated, NonCostituent, Constituent, Phonological).\nBranching: Constituent syntactic branching, Left and Right (shout out to Charlie Puth).\n\n\n\n\n\n\n\nQuiz 3\n\n\n\nHow many rows does shallow have?\n\n 11 650 6500"
  },
  {
    "objectID": "tutorials/tutorial-w03.html#import-excel-sheets",
    "href": "tutorials/tutorial-w03.html#import-excel-sheets",
    "title": "DAL tutorial - Week 3",
    "section": "5 Import Excel sheets",
    "text": "5 Import Excel sheets\nTo read an Excel file we need first to attach the readxl package. It should already be installed, because it comes with the tidyverse. If not, then install it.\n\nlibrary(readxl)\n\nThen we can use the read_excel() function. Let’s read the file.\n\nrelatives &lt;- read_excel(\"./data/los2023/relatives.xlsx\")\n\nNow you can view the tibble los2023.\nNote that if the Excel file has more than one sheet, you can specify the sheet number when reading the file (the default is sheet = 1).\n\nrelatives_2 &lt;- read_excel(\"./data/los2023/relatives.xlsx\", sheet = 2)\n\nThe second sheet in los2023/relatives.xlx contains the description of the columns in the first sheet."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#import-.rds-files",
    "href": "tutorials/tutorial-w03.html#import-.rds-files",
    "title": "DAL tutorial - Week 3",
    "section": "6 Import .rds files",
    "text": "6 Import .rds files\nAnother useful type of data files is a file type specifically designed for r: .rds files.\nUsually, each .rds file contains one R object, like one tibble.\nYou can read .rds files with the readRDS() function.\n\nglot_status &lt;- readRDS(\"./data/coretta2022/glot_status.rds\")\n\nAs always, you need to assign the output of the function to a variable, here glot_status.\n\n\n\n\n\n\n.rds files\n\n\n\n.rds files are a type of R file which can store any R object and save it on disk.\nR objects can be saved to an .rds file with the saveRDS() function and they can be read with the readRDS() function.\n\n\nView the glot_status tibble now.\nIt is also very easy to save a tibble to an .rds file with the saveRDS() function.\nFor example:\n\nsaveRDS(shallow, \"./data/song2020/shallow.rds\")\n\nThe first argument is the name of the tibble object and the second argument is the file path to save the object to."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#practice",
    "href": "tutorials/tutorial-w03.html#practice",
    "title": "DAL tutorial - Week 3",
    "section": "7 Practice",
    "text": "7 Practice\n\n\n\n\n\n\nPractice 1\n\n\n\n\n\nRead the following files in R, making sure you use the right read_*() function.\n\nkoppensteiner2016/takete_maluma.txt (a tab separated file)\npankratz2021/si.csv\nGo to https://datashare.ed.ac.uk/handle/10283/4006 and download the file conflict_data_.xlsx. Read both sheets (“conflict_data2” and “demographics”). Any issues? (I suggest looking at the spread sheet in Excel if it helps)."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#summary",
    "href": "tutorials/tutorial-w03.html#summary",
    "title": "DAL tutorial - Week 3",
    "section": "8 Summary",
    "text": "8 Summary\n\n\n\n\n\n\n\nYou have learnt about directories, file extensions and file paths.\nYou can import tabular data in R with the read_*() functions from the tidyverse package readr.\nYou can view data in RStudio as spreadsheets."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#footnotes",
    "href": "tutorials/tutorial-w03.html#footnotes",
    "title": "DAL tutorial - Week 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLab PCs should already have the tidyverse packages installed.↩︎\nA tibble is a special data frame. We will learn more about tibbles in the following weeks.↩︎"
  },
  {
    "objectID": "tutorials/tutorial-w01.html",
    "href": "tutorials/tutorial-w01.html",
    "title": "DAL tutorial - Week 1",
    "section": "",
    "text": "R can be used to analyse all sorts of data, from tabular data (also known as “spreadsheets”), textual data, geographic data and even images.\n\nThis course will focus on the analysis of tabular data, since all of the techniques relevant to this type of data also apply to the other types.\n\nThe R community is a very inclusive community and it’s easy to find help. There are several groups that promote R in minority/minoritised groups, like R-Ladies, Africa R, and Rainbow R just to mention a few.\nMoreover, R is open source and free!"
  },
  {
    "objectID": "tutorials/tutorial-w01.html#why-r",
    "href": "tutorials/tutorial-w01.html#why-r",
    "title": "DAL tutorial - Week 1",
    "section": "",
    "text": "R can be used to analyse all sorts of data, from tabular data (also known as “spreadsheets”), textual data, geographic data and even images.\n\nThis course will focus on the analysis of tabular data, since all of the techniques relevant to this type of data also apply to the other types.\n\nThe R community is a very inclusive community and it’s easy to find help. There are several groups that promote R in minority/minoritised groups, like R-Ladies, Africa R, and Rainbow R just to mention a few.\nMoreover, R is open source and free!"
  },
  {
    "objectID": "tutorials/tutorial-w01.html#the-r-console",
    "href": "tutorials/tutorial-w01.html#the-r-console",
    "title": "DAL tutorial - Week 1",
    "section": "2 The R console",
    "text": "2 The R console\n\n\n\n\n\n\nR\n\n\n\n\nR is a programming language.\nWe use programming languages to interact with computers.\nYou run commands written in a console and the task related to the command is executed.\n\n\n\nWe will begin our R journey with some basics concepts from computer science. The box above introduces you to three important concepts:\n\nProgramming languages.\nExecuting commands.\nConsole.\n\nR comes with its own console. Open now the R Console.\nIt should look like the following (there will be some aesthetic differences since you are using Windows).\n\nThe Console is an interactive interface that allows you to input commands and execute them.\nYou know you can enter a command because the prompt (&gt;) is displayed, and next to it you can see the text cursor (|) flashing.\nTry writing the following command (you will learn more about R commands below):\ncat(\"Hello!\")\nTo execute the command (aka run the command), press ENTER/RETURN on your keyboard.\nThe command cat(\"Hello!\") returns (aka outputs) in the console the text given between double quotes: Hello.\nCongratulations, you have run your first R command! This command involved a function (more on functions below): the cat() function (no feline involvement…).\nSo there are different types of R commands that you can use. In the following sections you will learn about the basic types of R commands and what they can be used for.\nYou will learn more and more commands throughout the course. You don’t have to memorise them all at once: focus on understanding what they can be useful for and if you don’t remember the details, you can always check them!"
  },
  {
    "objectID": "tutorials/tutorial-w01.html#r-basics",
    "href": "tutorials/tutorial-w01.html#r-basics",
    "title": "DAL tutorial - Week 1",
    "section": "3 R basics",
    "text": "3 R basics\nIn this part of the tutorial you will learn the very basics of R.\nIf you have prior experience with programming, you should find all this familiar. If not, not to worry! Make sure you understand the concept highlighted in the green boxes and practice the related skills.\nFor this tutorial, you will just run code directly in the R Console, i.e. you will type code in the Console and press ENTER/RETURN (ENTER from now on) to run it.\nIn future tutorials, you will learn how to save your code in a script file, so that you can keep track of what you have run and make your work reproducible.\n\n3.1 R as a calculator\nWrite this line of code 1 + 2 in the Console, then press ENTER to run it.\nFantastic! You should see that the answer of the addition has been printed in the Console, like this:\n[1] 3\n(Never mind the [1] part for now).\n\n\n\n\n\n\nArithmentic operations\n\n\n\nYou can run arithmetic operations using maths operators: the most common are +, -, *, / for addition, subtraction, multiplication and division.\n\n\nNow, try some more operations (write one line and press ENTER, then write the following line and so on…). Feel free to add your own operations to the mix!\n\n67 - 13\n2 * 4\n268 / 43\n\nYou can also chain multiple operations.\n\n6 + 4 - 1 + 2\n4 * 2 + 3 * 2\n\n\n\n\n\n\n\nQuiz 2\n\n\n\nAre the following statements true of false?\n\n3 * 2 / 4 returns the same result as 3 * (2 / 4) TRUEFALSE\n10 * 2 + 5 * 0.2 returns the same result as (10 * 2 + 5) * 0.2 TRUEFALSE\n\n\n\n\n\n\n\n\n\nExtra: Arithmetics\n\n\n\n\n\nIf you need a maths refresher, I recommend checking the following pages:\n\nhttps://www.mathsisfun.com/definitions/order-of-operations.html\nhttps://www.mathsisfun.com/algebra/introduction.html\n\n\n\n\n\n\n3.2 Variables\n\nForget-me-not.\n\nMost times, we want to store a certain value so that we can use it again later.\nWe can achieve this by creating variables.\n\n\n\n\n\n\nVariable\n\n\n\nA variable holds one or more values and it’s stored in the computer memory for later use.\n\n\nYou can create a variable by using the assignment operator &lt;-.\nLet’s assign the value 156 to the variable my_num.\n\nmy_num &lt;- 156\n\n\nNow, you can just call the variable back when you need it! Write the following in the Console and press ENTER.\n\nmy_num\n\n[1] 156\n\n\nYou should see the value of my_num being printed in the console.\nA variable like my_num is called a numeric vector: i.e. a vector that contains a number (hence numeric).\n\n\n\n\n\n\nVector\n\n\n\nA vector is an R object that contains one or more values of the same type.\n\n\nA numeric vector is a type of vector. However, it’s fine in most cases to use the word variable to mean vector (just note that a variable can also be something else than a vector; you will learn about other R objects from next week).\nLet’s now try some operations using variables.\n\nincome &lt;- 1200\nexpenses &lt;- 500\nincome - expenses\n\n[1] 700\n\n\nSee? You can use math operators with variables too!\nAnd you can also go all the way with variables.\n\nsavings &lt;- income - expenses\n\nNow check the value of savings…\n\nsavings\n\n[1] 700\n\n\nVectors can hold more than one item or value.\nJust use the combine c() function to create a vector containing multiple values.\nThe following are all numeric vectors.\n\na &lt;- 6\n# Vector with 2 values\nb &lt;- c(6, 8)\n# Vector with 3 values\nc &lt;- c(6, 8, 42)\n\n\nYou can check the type of vector (called class in R) with the class() function: for example, class(a) returns \"numeric\".\n\nclass(a)\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\nNumeric vector\n\n\n\nA numeric vector is a vector that holds one or more numeric values.\n\n\nNote that the following are the same:\n\na &lt;- 6\na\n\n[1] 6\n\nd &lt;- c(6)\nd\n\n[1] 6\n\n\nAnother important aspect of variables is that they are… variable! Meaning that once you assign a value to one variable, you can overwrite the value by assigning a new one to the same variable.\n\nmy_num &lt;- 88\nmy_num &lt;- 63\nmy_num\n\n[1] 63\n\n\nWhat if you want to know which variables you have created so far? Easy: use the ls() function. Just write ls() in the console and press ENTER: a list of existing variables will be returned.\n\n\n\n\n\n\nQuiz 3\n\n\n\nTrue or false?\n\nA vector can be created with the c() function. TRUEFALSE\nNot all variables are vectors. TRUEFALSE\nA numeric vector can only hold numeric values. TRUEFALSE\n\n\n\n\n\n3.3 Functions\n\nR cannot function without… functions.\n\nWe have encountered a few functions: cat(), c(), class() and ls().\n\n\n\n\n\n\nFunction\n\n\n\nA function usually runs an operation on one or more specified arguments.\n\n\nA function in R has the form function() where:\n\nfunction is the name of the function, like cat.\n() are round parentheses, inside of which you write arguments, separated by commas.\n\nLet’s see an example with the function sum() (can you guess what it does?):\n\nsum(3, 5)\n\n[1] 8\n\n\nThe sum() function sums the numbers listed as arguments. Above, the arguments are 3 and 5.\nAnd of course arguments can be vectors!\n\nmy_nums &lt;- c(3, 5, 7)\n\nsum(my_nums)\n\n[1] 15\n\nmean(my_nums)\n\n[1] 5\n\n\nSome functions work without specifying an argument, like ls().\nYou can also nest functions one inside the other: the output of the “lowest” function is used as the argument of the function above. Try and untangle the following.\n\ny &lt;- 10\nu &lt;- 6\ni &lt;- 7\no &lt;- 2\n\ncat(mean(c(sum(y, u), sum(i, o))))\n\n12.5\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuiz 4\n\n\n\nTrue or false?\n\nYou can use functions within functions. TRUEFALSE\nAll function arguments must be specified. TRUEFALSE\nAll functions need at least one argument. TRUEFALSE\n\n\n\n\n\n\n\n\n\nExtra: R vs Python\n\n\n\n\n\nIf you are familiar with Python, you will soon realise that R and Python, although they share many concepts and types of objects, they can differ substantially. This is because R is a functional programming language (based on functions) while Python is an Object Oriented programming language (based on methods applied on objects).\nGenerally speaking, functions look like print(x) while methods look like x.print()\n\n\n\n\n\n3.4 String and logical vectors\n\nNot just numbers.\n\nWe have seen that variables can hold numeric vectors. But vectors are not restricted to being numeric. They can also store strings.\nA string is basically a set of characters (a word, a sentence, a full text).\nIn R, strings have to be quoted using double quotes \" \".\nChange the following strings to your name and surname. Remember to keep the double quotes\n\nname &lt;- \"Stefano\"\nsurname &lt;- \"Coretta\"\n\nname\n\n[1] \"Stefano\"\n\n\nStrings can be used as arguments in functions, like numbers can.\n\ncat(\"My name is\", name, surname)\n\nMy name is Stefano Coretta\n\n\nRemember that you can reuse the same variable name to override the variable value.\n\nname &lt;- \"Raj\"\n\ncat(\"My name is\", name, surname)\n\nMy name is Raj Coretta\n\n\nYou can combine multiple strings into a character vector, using c().\n\n\n\n\n\n\nCharacter vector\n\n\n\nA character vector is a vector that holds one or more strings.\n\n\n\nfruit &lt;- c(\"apple\", \"oranges\", \"bananas\")\nfruit\n\n[1] \"apple\"   \"oranges\" \"bananas\"\n\n\nUse the class() function to check the vector class.\n\nclass(fruit)\n\n[1] \"character\"\n\n\nAnother type of vector is one that contains either TRUE or FALSE. Vectors of this type are called logical vectors and their class is logical.\n\n\n\n\n\n\nLogical vector\n\n\n\nA logical vector is a vector that holds one or more TRUE or FALSE values.\n\n\n\ngroceries &lt;- c(\"apple\", \"flour\", \"margarine\", \"sugar\")\nin_pantry &lt;- c(TRUE, TRUE, FALSE, TRUE)\nclass(in_pantry)\n\n[1] \"logical\"\n\ndata.frame(groceries, in_pantry)\n\n\n  \n\n\n\nTRUE and FALSE values must be written in all capitals and without double quotes (they are not strings!).\n(We will talk about data frames, another type of object in R, in the following weeks.)\n\n\n\n\n\n\nQuiz 5\n\n\n\n\nWhich of the following is not a character vector.\n\n c(1, 2, \"43\") \"s\" c(apple) (assuming apple &lt;- 45) c(letters)\n\nWhich of the following is not a logical vector.\n\n c(T, T, F) TRUE \"FALSE\" c(FALSE)\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use the class() function to check the type (“class”) of a vector.\n\nclass(FALSE)\n\n[1] \"logical\"\n\nclass(c(1, 45))\n\n[1] \"numeric\"\n\nclass(c(\"a\", \"b\"))\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n5a\n\nc(1, 2, \"43\") is a character vector because the last number \"43\" is a string (it’s between double quotes!). A vector cannot have a mix of types of elements: they have to be all numbers or all strings or else, but not some numbers and some strings. Numbers are special in that if you include a number in a character vector without quoting it, it is automatically converted into a string. Try the following:\n\n\nchar &lt;- c(\"a\", \"b\", \"c\")\nchar &lt;- c(char, 1)\nchar\nclass(char)\n\n\nc(letters) is a character vector because letters contains the letters of the alphabet as strings (this vector comes with base R).\nc(apple) is not a character vector because the variable apple holds a number, 45!\n\n5b\n\n\"FALSE\" is not a logical vector because FALSE has been quoted (anything that is quoted is a string!).\n\n\n\n\n\n\n\n\n\n\n\n\nExtra: For-loops and if-else statements\n\n\n\n\n\nThis course does not cover programming in R in the strict sense, but if you are curious here’s a short primer on for-loops and if-else statements in R.\nFor-loops\n\nfruits &lt;- c(\"apples\", \"mangos\", \"durians\")\n\nfor (fruit in fruits) {\n  cat(\"I like\", fruit, \"\\n\")\n}\n\nI like apples \nI like mangos \nI like durians \n\n\nIf-else\n\nfor (fruit in fruits) {\n  if (grepl(\"n\", fruit)) {\n    cat(fruit, \"has an 'n'\", \"\\n\")\n  } else {\n    cat(fruit, \"does not have an 'n'\", \"\\n\")\n  }\n}\n\napples does not have an 'n' \nmangos has an 'n' \ndurians has an 'n'"
  },
  {
    "objectID": "tutorials/tutorial-w01.html#summary",
    "href": "tutorials/tutorial-w01.html#summary",
    "title": "DAL tutorial - Week 1",
    "section": "4 Summary",
    "text": "4 Summary\nYou made it! You completed this week’s tutorial.\nHere’s a summary of what you learnt.\n\n\n\n\n\n\n\nR is a programming language while RStudio is an IDE.\nYou can perform mathematical operations with +, -, *, /.\nYou can store values in variables.\nA typical object to be stored in a variable is a vector: there are different type of vectors, like numeric, character and logical.\nFunctions are used to perform an operation on its arguments: sum() sums it’s arguments, mean() calculates the mean and cat() prints the arguments.\n\n\n\n\n\n\n\n\n\n\nExtra: Programming in R\n\n\n\n\n\nIf you are interested in learning about programming in R, I recommend you go through Chapters 26-28 of the R4DS book and the Advanced R book.\nNote that these topics are not covered in the course, nor will be assessed."
  },
  {
    "objectID": "slides/lecture-w08.html#mosaic-plot",
    "href": "slides/lecture-w08.html#mosaic-plot",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Mosaic plot",
    "text": "Mosaic plot\n\nglot_status |&gt; \n  filter(status != \"extinct\") |&gt; \n  droplevels() |&gt; \n  ggplot() +\n  # from the ggmosaic package\n  geom_mosaic(\n    aes(product(Macroarea), fill = status),\n    alpha = 1) +\n  scale_fill_brewer(palette = \"BuPu\") +\n  theme_dark() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "slides/lecture-w08.html#mosaic-plot-output",
    "href": "slides/lecture-w08.html#mosaic-plot-output",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Mosaic plot",
    "text": "Mosaic plot"
  },
  {
    "objectID": "slides/lecture-w08.html#line-plot",
    "href": "slides/lecture-w08.html#line-plot",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Line plot",
    "text": "Line plot\n\nformants %&gt;%\n  filter(c2 %in% c(\"t\", \"k\")) |&gt;\n  filter(vowel %in% c(\"a\", \"o\")) |&gt; \n  ggplot(aes(time, value, group = id, colour = formant)) +\n  geom_line(alpha = 0.8) +\n  facet_grid(cols = vars(c2), rows = vars(vowel)) +\n  scale_color_brewer(type = \"qual\") +\n  labs(\n    x = \"Relative time\", y = \"Formant value (Hz)\",\n    title = \"Formant trajectories of Italian /a, o/ followed by /k/ or /t/\"\n  )"
  },
  {
    "objectID": "slides/lecture-w08.html#line-plot-output",
    "href": "slides/lecture-w08.html#line-plot-output",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Line plot",
    "text": "Line plot"
  },
  {
    "objectID": "slides/lecture-w08.html#line-plot-with-points",
    "href": "slides/lecture-w08.html#line-plot-with-points",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Line plot with points",
    "text": "Line plot with points\n\nformants %&gt;%\n  filter(c2 %in% c(\"t\", \"k\")) |&gt;\n  filter(vowel %in% c(\"a\", \"o\")) |&gt; \n  ggplot(aes(time, value, group = id,\n             colour = formant)) +\n  geom_line(alpha = 0.25) +\n  geom_point(alpha = 0.5) +\n  facet_grid(cols = vars(c2), rows = vars(vowel)) +\n  scale_color_brewer(type = \"qual\") +\n  labs(\n    x = \"Relative time\", y = \"Formant value (Hz)\",\n    title = \"Formant trajectories of Italian /a, o/ followed by /k/ or /t/\"\n  )"
  },
  {
    "objectID": "slides/lecture-w08.html#line-plot-with-points-output",
    "href": "slides/lecture-w08.html#line-plot-with-points-output",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Line plot with points",
    "text": "Line plot with points"
  },
  {
    "objectID": "slides/lecture-w08.html#connected-dots-plot",
    "href": "slides/lecture-w08.html#connected-dots-plot",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Connected dots plot",
    "text": "Connected dots plot\n\ngestures |&gt; \n  filter(gesture != \"ho_gv\", count &lt; 30) |&gt; \n  group_by(dyad, background, months, gesture) |&gt; \n  summarise(\n    count_tot = sum(count),\n    .groups = \"drop\"\n  ) |&gt; \n  ggplot(aes(gesture, count_tot, colour = background),\n         alpha = 0.2) +\n  geom_line(aes(group = dyad),\n            alpha = 0.5) +\n  geom_point() +\n  facet_grid(background ~ months) +\n  labs(\n    title = \"Infant gesture counts at 10, 11 and 12 mo\",\n    x = \"Gesture type\", y = \"Gesture count\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/lecture-w08.html#connected-dots-plot-output",
    "href": "slides/lecture-w08.html#connected-dots-plot-output",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Connected dots plot",
    "text": "Connected dots plot"
  },
  {
    "objectID": "slides/lecture-w08.html#wals-data",
    "href": "slides/lecture-w08.html#wals-data",
    "title": "Data Analysis for LEL - Week 8",
    "section": "WALS data",
    "text": "WALS data\n\nlibrary(ritwals)\nwals_data &lt;- intersect_features(c(\"45A\", \"70A\")) |&gt; \n  filter(feature_ID %in% c(\"45A\", \"70A\")) |&gt; \n  select(feature_ID, value, language) |&gt; \n  filter(str_detect(value, \"No\", negate = TRUE)) |&gt; \n  pivot_wider(names_from = feature_ID, values_from = value) |&gt; \n  count(`45A`, `70A`) |&gt; \n  mutate(\n    `45A` = factor(`45A`, levels = c(\"Binary politeness distinction\", \"Multiple politeness distinctions\", \"Pronouns avoided for politeness\")),\n    `70A` = factor(`70A`, levels = c(\"Second singular and second plural\", \"Second singular\", \"Second plural\", \"Second person number-neutral\"))\n  ) |&gt; \n  drop_na()\nwals_data\n\n# A tibble: 8 × 3\n  `45A`                            `70A`                                 n\n  &lt;fct&gt;                            &lt;fct&gt;                             &lt;int&gt;\n1 Binary politeness distinction    Second person number-neutral         10\n2 Binary politeness distinction    Second plural                         1\n3 Binary politeness distinction    Second singular                       5\n4 Binary politeness distinction    Second singular and second plural    15\n5 Multiple politeness distinctions Second person number-neutral          1\n6 Multiple politeness distinctions Second singular                       2\n7 Multiple politeness distinctions Second singular and second plural     7\n8 Pronouns avoided for politeness  Second person number-neutral          2"
  },
  {
    "objectID": "slides/lecture-w08.html#alluvial-plot",
    "href": "slides/lecture-w08.html#alluvial-plot",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Alluvial plot",
    "text": "Alluvial plot\n\nwals_data |&gt; \n  ggplot(aes(y = n, axis1 = `45A`, axis2 = `70A`)) +\n  geom_alluvium(aes(fill = `45A`)) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\",\n            aes(label = after_stat(stratum))) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Languages with a politeness distinction and a morphological imperative\") +\n  theme_light() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/lecture-w08.html#alluvial-plot-output",
    "href": "slides/lecture-w08.html#alluvial-plot-output",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Alluvial plot",
    "text": "Alluvial plot"
  },
  {
    "objectID": "slides/lecture-w08.html#violin-plot",
    "href": "slides/lecture-w08.html#violin-plot",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Violin plot",
    "text": "Violin plot\n\npolite |&gt; \n  drop_na(f0mn) |&gt; \n  group_by(subject) |&gt; \n  mutate(f0mn_z = (f0mn - mean(f0mn)) / sd(f0mn)) |&gt; \n  ggplot(aes(attitude, f0mn_z)) +\n  geom_violin() +\n  geom_jitter(aes(colour = attitude),\n              width = 0.05,\n              alpha = 0.5) +\n  stat_summary(fun = \"median\", geom = \"point\",\n               shape = 17, size = 5) +\n  facet_grid(cols = vars(musicstudent), labeller = label_both) +\n  scale_color_manual(values = c(\"orange\", \"purple\")) +\n  labs(\n    title = \"Mean f0 of Korean speakers by attitude\",\n    y = \"Mean f0 (z-scores)\",\n    caption = \"Korean speakers were students living in Germany. Mean f0 was z-scored within each speaker.\"\n  ) +\n  ThemePark::theme_starwars() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/lecture-w08.html#violin-plot-output",
    "href": "slides/lecture-w08.html#violin-plot-output",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Violin plot",
    "text": "Violin plot"
  },
  {
    "objectID": "slides/lecture-w08.html#dont-use-box-plots",
    "href": "slides/lecture-w08.html#dont-use-box-plots",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Don’t use box-plots",
    "text": "Don’t use box-plots\nThe Datasaurus example.\nAlso see post by Nick Desbarats: I’ve stopped using box plots, should you?."
  },
  {
    "objectID": "slides/lecture-w08.html#dont-use-error-bars",
    "href": "slides/lecture-w08.html#dont-use-error-bars",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Don’t use error bars",
    "text": "Don’t use error bars\nSee The issue with error bars by Data To Viz."
  },
  {
    "objectID": "slides/lecture-w08.html#dont-plot-means-of-proportionpercentage-means",
    "href": "slides/lecture-w08.html#dont-plot-means-of-proportionpercentage-means",
    "title": "Data Analysis for LEL - Week 8",
    "section": "Don’t plot means of proportion/percentage means",
    "text": "Don’t plot means of proportion/percentage means\n\nset.seed(992)\nparticipants &lt;- 10\nobservations &lt;- round(runif(participants, 1, 15))\nprobabilities &lt;- runif(participants)\nbinary_outcomes &lt;- rbinom(participants, size = observations, prob = probabilities)\n\nmean(binary_outcomes / observations)\n\n[1] 0.714881\n\nsum(binary_outcomes) / sum(observations)\n\n[1] 0.7866667\n\n\n\nThe mean of means is the same as the overall mean only if the N for each participant is the same.\nEven in this case, a mean by itself is MEANingless and plotting standard errors with error bars can be misleading (see previous slide).\nSo plot the proportion of each participant and the overall proportion!"
  },
  {
    "objectID": "slides/lecture-w08.html#to-summarise",
    "href": "slides/lecture-w08.html#to-summarise",
    "title": "Data Analysis for LEL - Week 8",
    "section": "To summarise",
    "text": "To summarise\n\n\nShow raw data (e.g. individual observations, participants, items…).\nDo not use box-plots, even if you are asked to!\n\nUse density plots, violin plots, strip charts.\n\nDo not use “error bars”, even if you are asked to!\n\nIf the person asking insists, ask “which measure should I use for the error bars?”.\nThe only sensible answer is “X% Confidence Intervals” (where X is usually 95%).\n\nDo not use means of proportions!\n\nCalculate the overall proportion and/or show the proportion of each participant as raw data."
  },
  {
    "objectID": "slides/lecture-w06.html#section",
    "href": "slides/lecture-w06.html#section",
    "title": "Data Analysis for LEL - Week 6",
    "section": "",
    "text": "Link: https://forms.office.com/e/rc0CAJc8YV"
  },
  {
    "objectID": "slides/lecture-w06.html#summary-measures",
    "href": "slides/lecture-w06.html#summary-measures",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Summary measures",
    "text": "Summary measures"
  },
  {
    "objectID": "slides/lecture-w06.html#summary-measures-1",
    "href": "slides/lecture-w06.html#summary-measures-1",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Summary measures",
    "text": "Summary measures\n\nWe can summarise variables using summary measures.\n\n\n\nThere are two types of summary measures.\nMeasures of central tendency\n\nMeasures of central tendency indicate the typical or central value of a sample.\n\nMeasures of dispersion\n\nMeasures of dispersion indicate the spread or dispersion of the sample values around the central tendency value.\n\n\n\n\n\nAlways report a measure of central tendency together with its measure of dispersion!"
  },
  {
    "objectID": "slides/lecture-w06.html#measures-of-central-tendency",
    "href": "slides/lecture-w06.html#measures-of-central-tendency",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\n\nMean\n\\[\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + ... + x_n}{n}\\]\n\n\n\nMedian\n\\[\\text{if } n \\text{ is odd, } x_\\frac{n+1}{2}\\]\n\\[\\text{if } n \\text{ is even,  } \\frac{x_\\frac{n}{2} + x_\\frac{n}{2}}{2}\\]\n\n\n\n\nMode\nThe most common value."
  },
  {
    "objectID": "slides/lecture-w06.html#measures-of-dispersion",
    "href": "slides/lecture-w06.html#measures-of-dispersion",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\n\nMinimum and maximum values\n\n\n\nRange\n\\[ max(x) - min(x)\\]\nThe difference between the largest and smallest value.\n\n\n\n\nStandard deviation\n\\[\\text{SD} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}} = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + ... + (x_n - \\bar{x})^2}{n-1}}\\]"
  },
  {
    "objectID": "slides/lecture-w06.html#mean",
    "href": "slides/lecture-w06.html#mean",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Mean",
    "text": "Mean\nUse the mean with numeric continuous variables, if:\n\nThe variable can take on any positive and negative number, including 0.\n\n\n\n[1] -0.354\n\n\n\nThe variable can take on any positive number only.\n\n\n\n[1] 1.122\n\n\n\n\nDon’t take the mean of proportions and percentages!\nBetter to calculate the proportion/percentage across the entire data, rather than take the mean of individual proportions/percentages: see this blog post. If you really really have to, use the median."
  },
  {
    "objectID": "slides/lecture-w06.html#median",
    "href": "slides/lecture-w06.html#median",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Median",
    "text": "Median\nUse the median with numeric (continuous and discrete) variables.\n\n\n[1] 0.09\n\n\n[1] -2.10 -1.12  0.09  0.41  0.95\n\n\n[1] 1.09\n\n\n[1] 0.12 0.32 1.09 1.50 2.58"
  },
  {
    "objectID": "slides/lecture-w06.html#median-1",
    "href": "slides/lecture-w06.html#median-1",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Median",
    "text": "Median\n\n\n[1] 6.5\n\n\n[1]  3  4  6  7  9 15"
  },
  {
    "objectID": "slides/lecture-w06.html#median-2",
    "href": "slides/lecture-w06.html#median-2",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Median",
    "text": "Median\n\n\n[1] 6.5\n\n\n[1] 7.333333\n\n\n[1] 6.5\n\n\n[1] 11.5"
  },
  {
    "objectID": "slides/lecture-w06.html#median-3",
    "href": "slides/lecture-w06.html#median-3",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Median",
    "text": "Median\n\n\nThe mean is very sensitive to outliers.\nThe median is not."
  },
  {
    "objectID": "slides/lecture-w06.html#mode",
    "href": "slides/lecture-w06.html#mode",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Mode",
    "text": "Mode\nUse the mode with categorical (discrete) variables.\n\n\n\n  blue  green    red yellow \n     2      1      3      2 \n\n\nThe mode is the most frequent value: red.\n\n\nLikert scales are ordinal (categorical) variables, so the mean and median are not appropriate!\nYou should use the mode (You can use the median with Likert scales if you really really need to…)"
  },
  {
    "objectID": "slides/lecture-w06.html#minimum-and-maximum",
    "href": "slides/lecture-w06.html#minimum-and-maximum",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Minimum and maximum",
    "text": "Minimum and maximum\nReport minimum and maximum values for any numeric variable.\n\n\n[1] -2.1\n\n\n[1] 0.95\n\n\n[1] -2.10  0.95"
  },
  {
    "objectID": "slides/lecture-w06.html#range",
    "href": "slides/lecture-w06.html#range",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Range",
    "text": "Range\nUse the range with any numeric variable.\n\n\n[1] 3.05\n\n\n[1] 2.46\n\n\n[1] 12"
  },
  {
    "objectID": "slides/lecture-w06.html#standard-deviation",
    "href": "slides/lecture-w06.html#standard-deviation",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Standard deviation",
    "text": "Standard deviation\nUse the standard deviation with numeric continuous variables, if:\n\nThe variable can take on any positive and negative number, including 0.\n\n\n\n[1] 1.23658\n\n\n\nThe variable can take on any positive number only.\n\n\n\n[1] 0.9895555\n\n\n\n\nStandard deviations are relative and depend on the measurement unit/scale!\n\n–\n\nDon’t use the standard deviation with proportions and percentages!"
  },
  {
    "objectID": "slides/lecture-w06.html#summary-measures-overview",
    "href": "slides/lecture-w06.html#summary-measures-overview",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Summary measures overview",
    "text": "Summary measures overview"
  },
  {
    "objectID": "slides/lecture-w06.html#summary",
    "href": "slides/lecture-w06.html#summary",
    "title": "Data Analysis for LEL - Week 6",
    "section": "Summary",
    "text": "Summary\n\n\nThe sample \\(y\\) is generated by a (random) variable \\(Y\\).\nA (statistical) variable is any characteristics, number, or quantity that can be measured or counted.\nVariables can be numeric or categorical.\n\nNumeric variables can be continuous or discrete.\nCategorical variables are only discrete.\n\nWe operationalise a measure/observation as a numeric or a categorical variable.\nWe summarise variables using summary measures:\n\nMeasures of central tendency indicate the typical or central value of a sample.\nMeasures of dispersion indicate the spread or dispersion of the sample values around the central tendency value."
  },
  {
    "objectID": "slides/lecture-w04.html#good-data-visualisation",
    "href": "slides/lecture-w04.html#good-data-visualisation",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Good data visualisation",
    "text": "Good data visualisation\n\nAlberto Cairo has identified four common features of good data visualisation (Spiegelhalter 2019:64–66):\n\nIt contains reliable information.\nThe design has been chosen so that relevant patterns become noticeable.\nIt is presented in an attractive manner, but appearance should not get in the way of honesty, clarity and depth.\nWhen appropriate, it is organized in a way that enables some exploration."
  },
  {
    "objectID": "slides/lecture-w04.html#endangerment-status",
    "href": "slides/lecture-w04.html#endangerment-status",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Endangerment status",
    "text": "Endangerment status\n\nglot_status\n\n# A tibble: 8,345 × 18\n   ID        Language_ID Parameter_ID Value Code_ID Comment Source codeReference\n   &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;lgl&gt;        \n 1 kolp1236… kolp1236    aes          3     aes-sh… Kol (1… hh:he… NA           \n 2 tana1288… tana1288    aes          3     aes-sh… Tanahm… hh:he… NA           \n 3 touo1238… touo1238    aes          3     aes-sh… Touo (… hh:he… NA           \n 4 bert1248… bert1248    aes          3     aes-sh… Fadash… hh:he… NA           \n 5 sius1254… sius1254    aes          6     aes-ex… Siusla… hh:he… NA           \n 6 cent2045… cent2045    aes          6     aes-ex… Jalaa … &lt;NA&gt;   NA           \n 7 else1239… else1239    aes          3     aes-sh… Elseng… hh:he… NA           \n 8 taia1239… taia1239    aes          4     aes-mo… Taiap … hh:he… NA           \n 9 pyuu1245… pyuu1245    aes          3     aes-sh… Pyu (4… hh:he… NA           \n10 mato1253… mato1253    aes          6     aes-ex… Arára … hh:he… NA           \n# ℹ 8,335 more rows\n# ℹ 10 more variables: status &lt;fct&gt;, Name &lt;chr&gt;, Macroarea &lt;chr&gt;,\n#   Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, Glottocode &lt;chr&gt;, ISO639P3code &lt;chr&gt;,\n#   Countries &lt;chr&gt;, Family_ID &lt;chr&gt;, Language_ID.y &lt;chr&gt;"
  },
  {
    "objectID": "slides/lecture-w04.html#information-is-not-reliable",
    "href": "slides/lecture-w04.html#information-is-not-reliable",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Information is (not) reliable",
    "text": "Information is (not) reliable"
  },
  {
    "objectID": "slides/lecture-w04.html#information-is-reliable",
    "href": "slides/lecture-w04.html#information-is-reliable",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Information is reliable",
    "text": "Information is reliable"
  },
  {
    "objectID": "slides/lecture-w04.html#patterns-are-not-noticeable",
    "href": "slides/lecture-w04.html#patterns-are-not-noticeable",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Patterns are (not) noticeable",
    "text": "Patterns are (not) noticeable"
  },
  {
    "objectID": "slides/lecture-w04.html#patterns-are-noticeable",
    "href": "slides/lecture-w04.html#patterns-are-noticeable",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Patterns are noticeable",
    "text": "Patterns are noticeable"
  },
  {
    "objectID": "slides/lecture-w04.html#aesthetics-should-not-get-in-the-way",
    "href": "slides/lecture-w04.html#aesthetics-should-not-get-in-the-way",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Aesthetics (should not) get in the way",
    "text": "Aesthetics (should not) get in the way\n\nImage source. See more examples on Ugly Charts."
  },
  {
    "objectID": "slides/lecture-w04.html#does-not-enable-exploration",
    "href": "slides/lecture-w04.html#does-not-enable-exploration",
    "title": "Data Analysis for LEL - Week 4",
    "section": "(Does not) enable exploration",
    "text": "(Does not) enable exploration"
  },
  {
    "objectID": "slides/lecture-w04.html#enables-exploration",
    "href": "slides/lecture-w04.html#enables-exploration",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Enables exploration",
    "text": "Enables exploration"
  },
  {
    "objectID": "slides/lecture-w04.html#practical-tips",
    "href": "slides/lecture-w04.html#practical-tips",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Practical tips",
    "text": "Practical tips\n\n\nShow raw data (e.g. individual observations, participants, items…).\nSeparate data in different panels as needed.\nUse simple but informative labels for axes, panels, etc…\nUse colour as a visual aid, not just for aesthetics.\nReuse labels, colours, shapes throughout different plots to indicate the same thing."
  },
  {
    "objectID": "slides/lecture-w04.html#activity",
    "href": "slides/lecture-w04.html#activity",
    "title": "Data Analysis for LEL - Week 4",
    "section": "Activity",
    "text": "Activity\nForm small groups and discuss:\n\nGo to Ugly Charts, pick a couple charts to discuss.\nDiscuss the plots in a couple recent papers you have read."
  },
  {
    "objectID": "slides/lecture-w02.html#section",
    "href": "slides/lecture-w02.html#section",
    "title": "Data Analysis for LEL - Week 2",
    "section": "",
    "text": "Rate the following from 1 (strongly disagree) to 5 (strongly agree) about your experience with Week 1 of the QML course"
  },
  {
    "objectID": "slides/lecture-w02.html#section-1",
    "href": "slides/lecture-w02.html#section-1",
    "title": "Data Analysis for LEL - Week 2",
    "section": "",
    "text": "List things from Week 1 that you feel still unsure about"
  },
  {
    "objectID": "slides/lecture-w02.html#research-process",
    "href": "slides/lecture-w02.html#research-process",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Research process",
    "text": "Research process"
  },
  {
    "objectID": "slides/lecture-w02.html#section-2",
    "href": "slides/lecture-w02.html#section-2",
    "title": "Data Analysis for LEL - Week 2",
    "section": "",
    "text": "Which of the following are research hypotheses?"
  },
  {
    "objectID": "slides/lecture-w02.html#research-rationale",
    "href": "slides/lecture-w02.html#research-rationale",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Research rationale",
    "text": "Research rationale"
  },
  {
    "objectID": "slides/lecture-w02.html#research-questions-and-hypotheses",
    "href": "slides/lecture-w02.html#research-questions-and-hypotheses",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Research questions and hypotheses",
    "text": "Research questions and hypotheses\n\nResearch question\n\nResearch questions are testable questions whose answers directly address the research problem.\n\n\n\n\nResearch hypotheses\n\nResearch hypotheses are testable statements (not questions) about the research problem.\nThe hypotheses must be falsifiable (there can be in principle an outcome that shows them to be false).\nHypotheses can never be true nor confirmed. We can only corroborate hypothesis, and it’s a long term process."
  },
  {
    "objectID": "slides/lecture-w02.html#research-cycle",
    "href": "slides/lecture-w02.html#research-cycle",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Research cycle",
    "text": "Research cycle"
  },
  {
    "objectID": "slides/lecture-w02.html#section-3",
    "href": "slides/lecture-w02.html#section-3",
    "title": "Data Analysis for LEL - Week 2",
    "section": "",
    "text": "Which part of a research study do you believe should be beyond the control of the researcher?"
  },
  {
    "objectID": "slides/lecture-w02.html#section-4",
    "href": "slides/lecture-w02.html#section-4",
    "title": "Data Analysis for LEL - Week 2",
    "section": "",
    "text": "Which part of a research study do you believe is most important for advancing a researcher’s career?"
  },
  {
    "objectID": "slides/lecture-w02.html#research-cycle-the-dangers",
    "href": "slides/lecture-w02.html#research-cycle-the-dangers",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Research cycle: the dangers",
    "text": "Research cycle: the dangers"
  },
  {
    "objectID": "slides/lecture-w02.html#replication-in-linguistics",
    "href": "slides/lecture-w02.html#replication-in-linguistics",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Replication in linguistics",
    "text": "Replication in linguistics\n\n[O]nly 1 in 1,250 experimental linguistic articles contains an independent direct replication.\n\n\nThe observed smaller number of citations of replication studies compared to corresponding initial studies is also in line with the lack of perceived value of replication studies reported in other fields.\n\n—Korback and Roettger 2023"
  },
  {
    "objectID": "slides/lecture-w02.html#statistical-power-in-linguistics",
    "href": "slides/lecture-w02.html#statistical-power-in-linguistics",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Statistical power in linguistics",
    "text": "Statistical power in linguistics\n\nAn opportunistic review of seven papers from the November 2017 issue of this journal containing statistical analyses of acoustic speech production data found the number of participants to range from 11 to 39.\n\n—Kirby and Sonderegger 2018\n\nIn group differences research, the median sample sizes (n = 18 and n = 14 for case and control groups, respectively) were insufficient for detecting large, medium, or small effect sizes.\n\n—Gaeta and Brydges 2020\n\n[O]f the 1004 studies reviewed by Lehtonen et al. (2018) 878 had sample sizes smaller than 50 participants per group (i.e., 87%) and 987 had sample sizes smaller than 100 (98%).\n\n—Brysbaert 2020"
  },
  {
    "objectID": "slides/lecture-w02.html#case-study-participant-number-in-prosody-research",
    "href": "slides/lecture-w02.html#case-study-participant-number-in-prosody-research",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Case study: participant number in prosody research",
    "text": "Case study: participant number in prosody research\nThe dataset contains data from 113 studies, published between 1955 and 2017 (the bulk of studies is within the range 1990-2017 though). The median number of speakers per study is 5. (An estimate of number of speakers per study in phonetics, Coretta 2019)."
  },
  {
    "objectID": "slides/lecture-w02.html#case-study-participant-number-in-prosody-research-1",
    "href": "slides/lecture-w02.html#case-study-participant-number-in-prosody-research-1",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Case study: participant number in prosody research",
    "text": "Case study: participant number in prosody research"
  },
  {
    "objectID": "slides/lecture-w02.html#reproducibility-in-linguistics",
    "href": "slides/lecture-w02.html#reproducibility-in-linguistics",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Reproducibility in linguistics",
    "text": "Reproducibility in linguistics\n\n35% of the articles were published open access and the rates of sharing materials, data, and protocols were below 10%. None of the articles reported preregistrations, 1% reported replications, and 10% had conflict of interest statements. These rates have not increased noticeably between 2008/2009 and 2018/2019.\n\n—Bochynska et al. 2023"
  },
  {
    "objectID": "slides/lecture-w02.html#activity",
    "href": "slides/lecture-w02.html#activity",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Activity",
    "text": "Activity\n\nSearch for a paper you are interested in or that you have recently read.\nFill in the following spreadsheet with info on the paper you picked.\n\nSpreadsheet."
  },
  {
    "objectID": "slides/lecture-w02.html#open-scholarship-practices",
    "href": "slides/lecture-w02.html#open-scholarship-practices",
    "title": "Data Analysis for LEL - Week 2",
    "section": "Open Scholarship practices",
    "text": "Open Scholarship practices\n\n\nPre-registration and Registered Reports.\nShare the Research Compendium (data, materials, code, etc).\nEnsure your study is reproducible.\nThink about sample size.\n\n\n\n\n\nBe open, go Open!"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set-up instructions",
    "section": "",
    "text": "Important\n\n\n\nThe PPLS Computer Lab will have all the necessary software, but if you wish to use your own laptop you can follow these instructions."
  },
  {
    "objectID": "setup.html#r-and-rstudio",
    "href": "setup.html#r-and-rstudio",
    "title": "Set-up instructions",
    "section": "1 R and RStudio",
    "text": "1 R and RStudio\nYou must install both R and RStudio (they are two separate pieces of software).\n\nMake sure your operating system is up-to-date.\nThe latest version of R (https://cloud.r-project.org).\nThe latest version of RStudio (https://www.rstudio.com/products/rstudio/download/#download)."
  },
  {
    "objectID": "setup.html#quarto",
    "href": "setup.html#quarto",
    "title": "Set-up instructions",
    "section": "2 Quarto",
    "text": "2 Quarto\nFollow the installation instructions for Quarto on the Get Started page."
  },
  {
    "objectID": "data/tucker2019/mald_1_1.html",
    "href": "data/tucker2019/mald_1_1.html",
    "title": "Massive Auditory Lexical Decision 1.1",
    "section": "",
    "text": "Subject\n\nSubject unique identifier.\n\nItem\n\nWord.\n\nIsWord\n\nWhether it is a real word or a nonce word.\n\nPhonLev\n\nMean phoneme-level Levenshtein distance.\n\nRT\n\nReaction times (ms).\n\nACC\n\nAccuracy of lexical decision.\n\nRT_log\n\nLogged reaction times."
  },
  {
    "objectID": "data/tucker2019/mald_1_1.html#description",
    "href": "data/tucker2019/mald_1_1.html#description",
    "title": "Massive Auditory Lexical Decision 1.1",
    "section": "",
    "text": "Subject\n\nSubject unique identifier.\n\nItem\n\nWord.\n\nIsWord\n\nWhether it is a real word or a nonce word.\n\nPhonLev\n\nMean phoneme-level Levenshtein distance.\n\nRT\n\nReaction times (ms).\n\nACC\n\nAccuracy of lexical decision.\n\nRT_log\n\nLogged reaction times."
  },
  {
    "objectID": "data/ota2009/key-rock.html",
    "href": "data/ota2009/key-rock.html",
    "title": "The KEY to the ROCK: Near-homophony in nonnative visual word recognition",
    "section": "",
    "text": "The data contains only trials from the Japanese participants.\n\nSubject\n\nParticipant ID.\n\nProcedure\n\nWhether the trial is a practice (PracticeProc) of a test trial (TrialProc).\n\nVersion\n\nTrial version.\n\nContrast\n\nType of contrast (F filler, H homophone, LR /l~r/, P phonological, PB /p~b/).\n\nItem\n\nItem number.\n\nCondition\n\nTrial condition (whether the pair contains Control, Related, or Unrelated words),\n\nWordL\n\nWord shown on the left-side of the screen.\n\nWordR\n\nWord shown on the right-side of the screen.\n\nWords.ACC\n\nWhether the participant correctly identified the pair being related or unrelated.\n\nWords.RT\n\nReaction time of response in milliseconds."
  },
  {
    "objectID": "data/ota2009/key-rock.html#description",
    "href": "data/ota2009/key-rock.html#description",
    "title": "The KEY to the ROCK: Near-homophony in nonnative visual word recognition",
    "section": "",
    "text": "The data contains only trials from the Japanese participants.\n\nSubject\n\nParticipant ID.\n\nProcedure\n\nWhether the trial is a practice (PracticeProc) of a test trial (TrialProc).\n\nVersion\n\nTrial version.\n\nContrast\n\nType of contrast (F filler, H homophone, LR /l~r/, P phonological, PB /p~b/).\n\nItem\n\nItem number.\n\nCondition\n\nTrial condition (whether the pair contains Control, Related, or Unrelated words),\n\nWordL\n\nWord shown on the left-side of the screen.\n\nWordR\n\nWord shown on the right-side of the screen.\n\nWords.ACC\n\nWhether the participant correctly identified the pair being related or unrelated.\n\nWords.RT\n\nReaction time of response in milliseconds."
  },
  {
    "objectID": "data/koppensteiner2016/takete_maluma.html",
    "href": "data/koppensteiner2016/takete_maluma.html",
    "title": "Shaking Takete and Flowing Maluma. Non-Sense Words Are Associated with Motion Patterns",
    "section": "",
    "text": "Tak_Mal_Stim\n\nThe nature of the stimulus (Takete vs Maluma),\n\nAnswer\n\nAccuracy of response (CORRECT vs INCORRECT). Whether the participant has correctly identified the stimulus nature.\n\nCorr_1_Wrong_0\n\nSame as Answer but coded with 0 and 1.\n\nRater\n\nID of the participant.\n\nFemale_0\n\nParticipant’s gender (female = 0, male = 1)."
  },
  {
    "objectID": "data/koppensteiner2016/takete_maluma.html#description",
    "href": "data/koppensteiner2016/takete_maluma.html#description",
    "title": "Shaking Takete and Flowing Maluma. Non-Sense Words Are Associated with Motion Patterns",
    "section": "",
    "text": "Tak_Mal_Stim\n\nThe nature of the stimulus (Takete vs Maluma),\n\nAnswer\n\nAccuracy of response (CORRECT vs INCORRECT). Whether the participant has correctly identified the stimulus nature.\n\nCorr_1_Wrong_0\n\nSame as Answer but coded with 0 and 1.\n\nRater\n\nID of the participant.\n\nFemale_0\n\nParticipant’s gender (female = 0, male = 1)."
  },
  {
    "objectID": "data/coretta2018/formants.html",
    "href": "data/coretta2018/formants.html",
    "title": "Formant trajectories in Italian and Polish",
    "section": "",
    "text": "speaker\n\nspeaker’s ID\n\nfile\n\naudio chunk file name\n\nword\n\nword stimulus\n\ntime\n\ntime point within vowel (9 points per vowel)\n\nf1\n\nF1 (Hz)\n\nf2\n\nF2 (Hz)\n\nf3\n\nF3 (Hz)\n\nf4\n\nfundamental frequency (F0) (Hz)\n\nlanguage\n\nspeaker’s native language (Italian, Polish)\n\ngender\n\nspeaker’s gender (f, m)\n\nglottocode\n\nlanguage Glottocode\n\nitem\n\nword ID number\n\nipa\n\nIPA transcription of the word\n\nc1\n\nfirst consonant (C1)\n\nc1_phonation\n\nvoicing of C1 (voiceless, voiced)\n\nvowel\n\nV1 and V2 (a, o, u)\n\nanteropost\n\nbackness of the vowel (back, central)\n\nheight\n\nheight of the vowel (high, mid, low)\n\nc2\n\nsecond consonant (C2)\n\nc2_phonation\n\nvoicing of C2 (voiceless or voiced)\n\nc2_place\n\nplace of C2 (coronal, `velar``)"
  },
  {
    "objectID": "data/coretta2018/formants.html#description",
    "href": "data/coretta2018/formants.html#description",
    "title": "Formant trajectories in Italian and Polish",
    "section": "",
    "text": "speaker\n\nspeaker’s ID\n\nfile\n\naudio chunk file name\n\nword\n\nword stimulus\n\ntime\n\ntime point within vowel (9 points per vowel)\n\nf1\n\nF1 (Hz)\n\nf2\n\nF2 (Hz)\n\nf3\n\nF3 (Hz)\n\nf4\n\nfundamental frequency (F0) (Hz)\n\nlanguage\n\nspeaker’s native language (Italian, Polish)\n\ngender\n\nspeaker’s gender (f, m)\n\nglottocode\n\nlanguage Glottocode\n\nitem\n\nword ID number\n\nipa\n\nIPA transcription of the word\n\nc1\n\nfirst consonant (C1)\n\nc1_phonation\n\nvoicing of C1 (voiceless, voiced)\n\nvowel\n\nV1 and V2 (a, o, u)\n\nanteropost\n\nbackness of the vowel (back, central)\n\nheight\n\nheight of the vowel (high, mid, low)\n\nc2\n\nsecond consonant (C2)\n\nc2_phonation\n\nvoicing of C2 (voiceless or voiced)\n\nc2_place\n\nplace of C2 (coronal, `velar``)"
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "DAL",
    "section": "",
    "text": "Week\nLecture\nLab\n\n\n\n\n\n1\nQuantitative methods and uncertainty\nR basics\n\n\n\n2\nResearch process\nRStudio + R scripts\n\n\n\n3\nFile management\nRead data\n\n\n\n4\nData viz\nQuarto + Plotting basics\n\n\n\n5\nStatistical variables\nData transformation I\nF1\n\n\nFL\nNo classes\n\n\n\n\n6\nData summaries\nData transformation II\n\n\n\n7\nTidy data\nData tidying and joining\nF2\n\n\n8\nPlot design\nAdvanced plotting\n\n\n\n9\nWhere to next?\nTroubleshooting\n\n\n\n25 Mar\n\n\nS2 proposal\n\n\n28 Mar\n\n\nS1\n\n\n25 Apr\n\n\nS2"
  },
  {
    "objectID": "content.html#schedule-overview",
    "href": "content.html#schedule-overview",
    "title": "DAL",
    "section": "",
    "text": "Week\nLecture\nLab\n\n\n\n\n\n1\nQuantitative methods and uncertainty\nR basics\n\n\n\n2\nResearch process\nRStudio + R scripts\n\n\n\n3\nFile management\nRead data\n\n\n\n4\nData viz\nQuarto + Plotting basics\n\n\n\n5\nStatistical variables\nData transformation I\nF1\n\n\nFL\nNo classes\n\n\n\n\n6\nData summaries\nData transformation II\n\n\n\n7\nTidy data\nData tidying and joining\nF2\n\n\n8\nPlot design\nAdvanced plotting\n\n\n\n9\nWhere to next?\nTroubleshooting\n\n\n\n25 Mar\n\n\nS2 proposal\n\n\n28 Mar\n\n\nS1\n\n\n25 Apr\n\n\nS2"
  },
  {
    "objectID": "content.html#functions-and-data",
    "href": "content.html#functions-and-data",
    "title": "DAL",
    "section": "2 Functions and data",
    "text": "2 Functions and data\n\n\n\n\nFunctions\nData\n\n\n\n\n1\nc(), sum(), mean(), cat(), data.frame()\n\n\n\n2\nlength(), library(), .libPath()\n\n\n\n3\nread_csv(), read_excel(), readRDS()\nshallow, relatives, glot_status\n\n\n4\nggplot(), aes(), geom_point(), labs(), colour and alpha, scale_colour_brewer(), scale_colour_viridis_d(), |&gt;\npolite\n\n\n5\nfilter(), mutate(), geom_bar(), as.factor(), factor(), facet_grid()\nglot_status, polite\n\n\n6\ngroup_by(), summarise(), count(), geom_density(), geom_rug(), mean(), sd(), median(), min(), max(), range(), log()\nshallow, gestures\n\n\n7\npivot_*(), join_*(), list.files(), geom_violin(), geom_jitter(), facet_wrap()\n\n\n\n8\ngeom_hist(), stat_ellipse(), geom_tile(), likert, geom_sf\n\n\n\n9\n\n\n\n\nF1\n\ntoken-measures\n\n\nF2\n\n\n\n\nS1\n\n\n\n\nS2"
  },
  {
    "objectID": "content.html#weekly-schedule",
    "href": "content.html#weekly-schedule",
    "title": "DAL",
    "section": "3 Weekly schedule",
    "text": "3 Weekly schedule\n\n3.1 Week 1\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat is quantitative data analysis?\nIs statistics a science, an art, or both?\nHow do uncertainty and variability affect data analysis?\n\nSkills\n\nThink critically about statistics, uncertainty and variability.\nUse R to perform simple calculations.\nMaster the basics of the programming language R.\n\n\n\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\nCourse website\n\nCarefully read the homepage.\nFamiliarise yourself with this Course content page (note that the materials will be updated throughout the course).\n\nIntake form\n\nYou must complete the intake form before coming to the Tuesday lecture.\nThe link to the form can be found on the Learn website.\n\nOPTIONAL: Install R, RStudio and Quarto\n\nThe labs will take place in the PPLS computer lab which will have all the necessary software (hopefully…).\nIf instead you wish to use your own laptop, you are welcome to do so provided you take care of installing everything and fix issues that might arise. We won’t be able to help you with the installation process.\nPlease, follow the instructions in the Setup page.\n\n\n\n\n\n\n\n\n\n\nReadings\n\n\n\n\n\nALL READINGS ARE OPTIONAL. The textbook can be used to revise what done in class and you can pick and choose any of the other readings based on your interests.\nTextbook\nThis course will be loosely based on the following online textbook. Note that the order of the contents of the course will not closely follow that of this book, but specific chapter will be noted on this page in each week\n\nR for Data Science (R4DS).\nThe Turing Way (TTW).\n\nOther\n\nEllis and Levy 2008. Framework of Problem-Based Research: A Guide for Novice Researchers on the Development of a Research-Worthy Problem.\nMethods as theory.\nVasishth and Gelman 2021. How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis.\nMolnar 2022. Modeling Mindsets: The many cultures of learning from data.\nDarwin Holmes 2020. Researcher Positionality - A Consideration of Its Influence and Place in Qualitative Research - A New Researcher Guide\nJafar 2018. What is positionality and should it be expressed in quantitative studies?\n\n\n\n\n\n\n3.2 Week 2\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhich are the steps of the research process/cycle?\nHow can questionable research practices affect each step in the research cycle?\nHow can we prevent questionable research practices?\n\nSkills\n\nUsing RStudio and Quarto Projects\nWrite and run code in R scripts.\nInstall and attach R packages.\n\n\n\n\n\n\n\n\n\n\nReadings\n\n\n\n\n\nTextbook\n\nR4DS Ch 6.\n\nFrom the lecture\n\nKorback and Roettger 2023. Assessing the replication landscape in experimental linguistics.\nKirby and Sonderegger 2018. Mixed-effects design analysis for experimental phonetics.\nGaeta and Brydges 2020. An Examination of Effect Sizes and Statistical Power in Speech, Language, and Hearing Research.\nBrysbaert 2020. Power considerations in bilingualism research: Time to step up our game.\nAn estimate of number of speakers per study in phonetics (Coretta 2019).\nBochynska et al. 2023. Reproducible research practices and transparency across linguistics.\n\nOther\n\nStudy design.\nFalsifiability.\n\n\n\n\n\n\n3.3 Week 3\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat does file management involve?\nWhat is a research compendium?\nWhat are licenses and how does one choose how to license work?\n\nSkills\n\nOrganising project files and picking licenses.\nRead data into R.\nSubmitting a Data Management Plan.\n\n\n\n\n\n\n\n\n\n\nReadings\n\n\n\n\n\nTextbook\n\nR4DS Ch 6.\nTTW, Research Compendia and File naming conventions.\n\nFrom the lecture\n\nUoE Research Data Management policy.\nUoE DMP online.\nCreative Commons licenses.\n\nOther\n\n\n\n\n\n\n\n\n3.4 Week 4\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat are the principles of good data visualisation?\nWhich are the pitfalls to avoid?\nHow can showing the raw data make the plot more reliable?\n\nSkills\n\nCreate dynamic reports with Quarto.\nLearn about plotting systems.\nBasics of plotting with ggplot2.\n\n\n\n\n\n\n\n\n\n\nReadings\n\n\n\n\n\nTextbook\n\nR4DS Ch 1.\nGet started with Quarto.\n\nFrom the lecture\n\nSpiegerhalter 2019. The art of statistics.\n\nOther\n\nFundamentals of data visualisation.\n\nCatalogues\n\nDirectory of visualisations\nData viz catalogue\nData Viz project\nTop 50\nData Viz\n\nColour\n\nColorBrewer2.\nMetBrewer.\nUse colour wisely.\n\nRecommendations\n\nSame stats different data.\nBehind bars.\nI stopped using box plots.\nIssues with error bars.\n\n\n\n\n\n\n3.5 Week 5\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat is a statistical variable?\nWhich types of statistical variable are there?\nWhat is “operationalisation”?\n\nSkills\n\nFilter rows and mutate columns.\nCreate bar charts.\nSeparate data in plots using faceting.\n\n\n\n\n\n\n3.6 Week 6\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat are summary measures?\nWhich summary measures are appropriate for which type of variable?\nHow should summary measures be reported?\n\nSkills\n\nObtain summary measures in R.\nUse grouped data.\nCreate density plots.\n\n\n\n\n\n\n\n\n\n\nReadings\n\n\n\n\n\nTextbook\n\nR4DS Ch 3.\n\n\n\n\n\n\n3.7 Week 7\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat types of data do exist?\nWhat does data coding entail?\nWhat is tidy data and what does it mean for data to be non-tidy?\n\nSkills\n\nCoding data.\nTidy up non-tidy tibbles.\nJoin separate tibbles based on key columns.\n\n\n\n\n\n\n\n\n\n\nReadings\n\n\n\n\n\nTextbook\n\nR4DS Ch 5.\nR4DS Ch 19.\n\n\n\n\n\n\n3.8 Week 8\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat other type of plots can be helpful?\nWhat are common pitfalls of designing plots?\nHow can we avoid those pitfalls?\n\nSkills\n\nThink about pros and cons of different types of plots.\nCreate specialised plots.\nNormalise data and compute proportions.\n\n\n\n\n\n\n\n\n\n\nReadings\n\n\n\n\n\nSee week 8 slides."
  },
  {
    "objectID": "assessments.html",
    "href": "assessments.html",
    "title": "DAL",
    "section": "",
    "text": "This course will be assessed on the basis of 2 formative assessments and 2 summative assessments.\nFormative assessments are mock assessments that give you the chance for an interim check in with what you have learnt and what you might need to revise. They will not be individually marked, but a model answer will be shared after the deadline. There will be two formative assessments.\nYou will have to submit two summative assessments, each covering all the course content. They will weight 50% and 50% of the final mark.\nSee below for details (DETAILS TO BE ADDED)."
  },
  {
    "objectID": "assessments.html#assessment-overview",
    "href": "assessments.html#assessment-overview",
    "title": "DAL",
    "section": "",
    "text": "This course will be assessed on the basis of 2 formative assessments and 2 summative assessments.\nFormative assessments are mock assessments that give you the chance for an interim check in with what you have learnt and what you might need to revise. They will not be individually marked, but a model answer will be shared after the deadline. There will be two formative assessments.\nYou will have to submit two summative assessments, each covering all the course content. They will weight 50% and 50% of the final mark.\nSee below for details (DETAILS TO BE ADDED)."
  },
  {
    "objectID": "assessments.html#feedback-and-marking",
    "href": "assessments.html#feedback-and-marking",
    "title": "DAL",
    "section": "2 Feedback and marking",
    "text": "2 Feedback and marking\nThe formative assessments will not be individually marked and feedback will consist of a model answer, shared after the formative deadline. You will receive individual feedback for the summative assessments, as comments on the submitted file in Turnitin.\nThe comments will be categorised according the the Feedback Categorisation Rubric used for this course, which you can see here. The rubric is based on the learning outcomes of the course. For each criterion in the rubric there are three possible outcomes: insufficient, developing, proficient. This should help you identify areas of strength and those that can be improved.\nNote that the Feedback Categorisation Form is not used to calculate a numeric mark. Marking will follow the PPLS MSc Common Marking Scheme, which you can find here (login required).\nMore specifically, we will use a “step marking” procedure, where, within each mark band, you can get a 2, 5 or 8 (for example, 62, 65 or 68). We will not use other numbers within the scale. These are the criteria for getting 2, 5, or 8:\n\n2: The criteria for the mark band have been achieved, but there might be minor issues.\n5: The criteria for the mark band have been achieved fully.\n8: The criteria for the mark band have been achieved fully and in a particularly remarkable way, but not in such a way to grant a higher mark band.\n\nFinally, please note that marking does not follow a point-based system, i.e. different exercises are not assigned a specific amount of points you can obtain and we will instead adopt an “impression-based” marking system, based on the descriptors of the PPLS MSC Common Marking Scheme."
  },
  {
    "objectID": "assessments.html#formative-assessments",
    "href": "assessments.html#formative-assessments",
    "title": "DAL",
    "section": "3 Formative assessments",
    "text": "3 Formative assessments\n\n\n\n\n\n\nFormative 1: Week 5 (Thu 15 February at noon)\n\n\n\nReading and visualising data\nThis formative assessments covers Weeks 1-4.\nYou can find the instructions and data for the first formative here: https://github.com/uoelel/dal-f1/.\n\n\n\n\n\n\n\n\nFormative 2: Week 7 (Thu 7 March at noon)\n\n\n\nInterim feedback on Summative 1\nYou can submit your work so far on Summative 1 (see below). General feedback will be released short after the F2 deadline."
  },
  {
    "objectID": "assessments.html#summative-assessments",
    "href": "assessments.html#summative-assessments",
    "title": "DAL",
    "section": "4 Summative assessments",
    "text": "4 Summative assessments\n\n\n\n\n\n\nProposal for Summative 2: Monday 25 March at noon\n\n\n\nFor the second summative assessment you will have to find a data set from published research of your choice and write a short data analysis report on the chosen data.\nYou must submit to Turnitin for approval a brief description of the data set including a link to the relevant publication, by Monday 25th March.\n\n\n\n\n\n\n\n\nSummative 1: Week 10 (Thu 28 March at noon)\n\n\n\nDue on Thursday 28 March at noon\nThe first summative contains 2 guided exercises that cover things done in Weeks 1 to 7.\nYou can find the instructions and data for the first summative here: https://github.com/uoelel/dal-s1.\n\n\n\n\n\n\n\n\nSummative 2: Thu 25 April at noon\n\n\n\nDue on Thursday 25 April at noon\nFor the second summative assessment you will have to find a data set from published research of your choice and write a short data analysis report on the chosen data.\nYou must submit to Turnitin for approval a brief description of the data set including a link to the relevant publication, by Monday 25th March.\nThe summative assessment report is due on 25 April and should include:\n\nA brief explanation of the study the data comes from (remember to include proper attribution by citing relevant publications).\nA general description of the data frame (number and types of columns, number of observations, summary measures).\nPlots that illustrate patterns in the data (at least 5). Each plot should be accompanied by a caption and a textual description."
  },
  {
    "objectID": "data/coretta2018/token-measures.html",
    "href": "data/coretta2018/token-measures.html",
    "title": "Acoustics and articulatory durational measures of Italian and Polish",
    "section": "",
    "text": "index\n\nobservation number within speaker\n\nspeaker\n\nspeaker’s ID\n\nfile\n\naudio chunk file name\n\nrec_date\n\ndate and time of recording\n\nipu\n\nSPPAS IPU index\n\nprompt\n\nsentence stimulys\n\nword\n\nword stimulus\n\ntime\n\ntime of the sentence onset within the concatenated audio file (s)\n\nsentence_ons\n\nonset time of the sentence (s)\n\nsentence_off\n\noffset time of the sentence (s)\n\nword_ons\n\nonset time of the target word (s)\n\nword_off\n\noffset time of the target word (= C1 onset) (s)\n\nv1_ons\n\nonset time of V1 (= C1 offset) (s)\n\nc2_ons\n\nonset time of C2 (= V1 offset) (s)\n\nv2_ons\n\nonset time of V2 (= C2 offset) (s)\n\nc1_rel\n\ntime of C1 release (s)\n\nc2_rel\n\ntime of C2 release (s)\n\nvoicing_start\n\ntime of voicing onset (s)\n\nvoicing_end\n\ntime of voicing offset (s)\n\nvoicing_duration\n\nduration of voiced interval (ms)\n\nvoiced_points\n\nnumber of points out of 5 within the first half of C1 closure in which voicing is present\n\nGONS\n\nonset of C1 closing gesture (s)\n\nmax\n\ntime of maximum displacement of C1 closing gesture (s)\n\nNOFF\n\noffset of C1 gesture nucleus (s)\n\nNONS\n\nonset of C1 gesture nucleus (s)\n\npeak1\n\nfirst tongue velocity peak (s)\n\npeak2\n\nsecond tongue velocity peak (s)\n\nc1_duration\n\nduration of C1 (ms)\n\nc1_clos_duration\n\nduration of C1 closure (ms)\n\nc1_vot\n\nC1 Voice Onset Time (ms)\n\nc1_rvofft\n\nC1 release to V1 offset time (ms)\n\nv1_duration\n\nduration of V1 (ms)\n\nc1_duration\n\nduration of C1 (ms)\n\nc2_clos_duration\n\nduration of C2 closure (ms)\n\nv2_duration\n\nduration of V2 (ms)\n\nv_v\n\nV1 onset to V2 onset (Vowel-to-Vowel) duration (ms)\n\nword_duration\n\nduration of the word (ms)\n\nsentence_duration\n\nduration of sentence (s)\n\nlanguage\n\nspeaker’s native language (Italian, Polish)\n\ngender\n\nspeaker’s sex (f, m)\n\nglottocode\n\nlanguage Glottocode\n\nitem\n\nword ID number\n\nipa\n\nIPA transcription of the word\n\nc1\n\nfirst consonant (C1)\n\nc1_phonation\n\nvoicing of C1 (voiceless, voiced)\n\nvowel\n\nV1 and V2 (a, o, u)\n\nanteropost\n\nbackness of the vowel (back, central)\n\nheight\n\nheight of the vowel (high, mid, low)\n\nc2\n\nsecond consonant (C2)\n\nc2_phonation\n\nvoicing of C2 (voiceless or voiced)\n\nc2_place\n\nplace of C2 (coronal, velar)\n\nspeech_rate\n\nspeech rate as syllables per second\n\nspeech_rate_c\n\ncentred speech rate as syllables per second"
  },
  {
    "objectID": "data/coretta2018/token-measures.html#description",
    "href": "data/coretta2018/token-measures.html#description",
    "title": "Acoustics and articulatory durational measures of Italian and Polish",
    "section": "",
    "text": "index\n\nobservation number within speaker\n\nspeaker\n\nspeaker’s ID\n\nfile\n\naudio chunk file name\n\nrec_date\n\ndate and time of recording\n\nipu\n\nSPPAS IPU index\n\nprompt\n\nsentence stimulys\n\nword\n\nword stimulus\n\ntime\n\ntime of the sentence onset within the concatenated audio file (s)\n\nsentence_ons\n\nonset time of the sentence (s)\n\nsentence_off\n\noffset time of the sentence (s)\n\nword_ons\n\nonset time of the target word (s)\n\nword_off\n\noffset time of the target word (= C1 onset) (s)\n\nv1_ons\n\nonset time of V1 (= C1 offset) (s)\n\nc2_ons\n\nonset time of C2 (= V1 offset) (s)\n\nv2_ons\n\nonset time of V2 (= C2 offset) (s)\n\nc1_rel\n\ntime of C1 release (s)\n\nc2_rel\n\ntime of C2 release (s)\n\nvoicing_start\n\ntime of voicing onset (s)\n\nvoicing_end\n\ntime of voicing offset (s)\n\nvoicing_duration\n\nduration of voiced interval (ms)\n\nvoiced_points\n\nnumber of points out of 5 within the first half of C1 closure in which voicing is present\n\nGONS\n\nonset of C1 closing gesture (s)\n\nmax\n\ntime of maximum displacement of C1 closing gesture (s)\n\nNOFF\n\noffset of C1 gesture nucleus (s)\n\nNONS\n\nonset of C1 gesture nucleus (s)\n\npeak1\n\nfirst tongue velocity peak (s)\n\npeak2\n\nsecond tongue velocity peak (s)\n\nc1_duration\n\nduration of C1 (ms)\n\nc1_clos_duration\n\nduration of C1 closure (ms)\n\nc1_vot\n\nC1 Voice Onset Time (ms)\n\nc1_rvofft\n\nC1 release to V1 offset time (ms)\n\nv1_duration\n\nduration of V1 (ms)\n\nc1_duration\n\nduration of C1 (ms)\n\nc2_clos_duration\n\nduration of C2 closure (ms)\n\nv2_duration\n\nduration of V2 (ms)\n\nv_v\n\nV1 onset to V2 onset (Vowel-to-Vowel) duration (ms)\n\nword_duration\n\nduration of the word (ms)\n\nsentence_duration\n\nduration of sentence (s)\n\nlanguage\n\nspeaker’s native language (Italian, Polish)\n\ngender\n\nspeaker’s sex (f, m)\n\nglottocode\n\nlanguage Glottocode\n\nitem\n\nword ID number\n\nipa\n\nIPA transcription of the word\n\nc1\n\nfirst consonant (C1)\n\nc1_phonation\n\nvoicing of C1 (voiceless, voiced)\n\nvowel\n\nV1 and V2 (a, o, u)\n\nanteropost\n\nbackness of the vowel (back, central)\n\nheight\n\nheight of the vowel (high, mid, low)\n\nc2\n\nsecond consonant (C2)\n\nc2_phonation\n\nvoicing of C2 (voiceless or voiced)\n\nc2_place\n\nplace of C2 (coronal, velar)\n\nspeech_rate\n\nspeech rate as syllables per second\n\nspeech_rate_c\n\ncentred speech rate as syllables per second"
  },
  {
    "objectID": "data/coretta2022/glot_status.html",
    "href": "data/coretta2022/glot_status.html",
    "title": "Glottolog 4.6 data: Agglomerated Endangerment Status",
    "section": "",
    "text": "ID\n\nPrimary key.\n\nLanguage_ID\n\nReference to languages$ID.\n\nParameter_ID\n\nParameter ID.\n\nValue\n\nParameter value.\n\nCode_ID\n\nCode ID.\n\nComment\n\nComment.\n\nSource\n\nSource reference.\n\ncodeReference\n\n\n\nstatus\n\nAgglomerate Endangerment Status.\n\nName\n\nName of the language.\n\nMacroarea\n\nGeographic macro-area.\n\nLatitude\n\nLatitude.\n\nLongitude\n\nLongitude.\n\nGlottocode\n\nGlottocode.\n\nISO639P3code\n\nISO639-3 code.\n\nCountries\n\nCountries where the language is spoken.\n\nFamily_ID\n\nFamily ID.\n\nLanguage_ID.y\n\nLanguage ID."
  },
  {
    "objectID": "data/coretta2022/glot_status.html#description",
    "href": "data/coretta2022/glot_status.html#description",
    "title": "Glottolog 4.6 data: Agglomerated Endangerment Status",
    "section": "",
    "text": "ID\n\nPrimary key.\n\nLanguage_ID\n\nReference to languages$ID.\n\nParameter_ID\n\nParameter ID.\n\nValue\n\nParameter value.\n\nCode_ID\n\nCode ID.\n\nComment\n\nComment.\n\nSource\n\nSource reference.\n\ncodeReference\n\n\n\nstatus\n\nAgglomerate Endangerment Status.\n\nName\n\nName of the language.\n\nMacroarea\n\nGeographic macro-area.\n\nLatitude\n\nLatitude.\n\nLongitude\n\nLongitude.\n\nGlottocode\n\nGlottocode.\n\nISO639P3code\n\nISO639-3 code.\n\nCountries\n\nCountries where the language is spoken.\n\nFamily_ID\n\nFamily ID.\n\nLanguage_ID.y\n\nLanguage ID."
  },
  {
    "objectID": "data/song2020/shallow.html",
    "href": "data/song2020/shallow.html",
    "title": "Second language users exhibit shallow morphological processing",
    "section": "",
    "text": "Group\n\nParticipant group (L1, L2).\n\nID\n\nSubject unique identifier.\n\nList\n\nWord list.\n\nTarget\n\nTarget word.\n\nRT\n\nReaction time (ms).\n\nRT_log\n\nLogged reaction time.\n\nCritical_Filler\n\nWhether the trial is a critical or a filler trial.\n\nWord_Nonword\n\nWhether the word is a real word or a nonce word (only word is present in the data).\n\nRelation_type\n\nWhether the relation type is Unrelated, Constituent, or NonConstituent.\n\nBranching\n\nWhether the trial word is Left-branching or Right-branching."
  },
  {
    "objectID": "data/song2020/shallow.html#description",
    "href": "data/song2020/shallow.html#description",
    "title": "Second language users exhibit shallow morphological processing",
    "section": "",
    "text": "Group\n\nParticipant group (L1, L2).\n\nID\n\nSubject unique identifier.\n\nList\n\nWord list.\n\nTarget\n\nTarget word.\n\nRT\n\nReaction time (ms).\n\nRT_log\n\nLogged reaction time.\n\nCritical_Filler\n\nWhether the trial is a critical or a filler trial.\n\nWord_Nonword\n\nWhether the word is a real word or a nonce word (only word is present in the data).\n\nRelation_type\n\nWhether the relation type is Unrelated, Constituent, or NonConstituent.\n\nBranching\n\nWhether the trial word is Left-branching or Right-branching."
  },
  {
    "objectID": "data/winter2012/polite.html",
    "href": "data/winter2012/polite.html",
    "title": "The phonetic profile of Korean formal and informal speech registers",
    "section": "",
    "text": "subject\n\nSubject unique identifier (categorical).\n\ngender\n\nGender of subject (categorical).\n\nbirthplace\n\nBirth place of subject (categorical).\n\nmusicstudent\n\nDoes the subject have music training? (binary: yes, no)\n\nscenario\n\nUnique identifier of different items.\n\ntask\n\nTask type (categorical: not = mailbox task vs dct = discourse completion task). In the mailbox task people left a note on somebody’s mailbox, while in the discourse completion task they were prompted to role-play the start of a conversation.\n\nattitude\n\nAttitude (binary: pol polite vs inf informal).\n\ntotal_duration\n\nTotal duration of utterances in seconds (numeric).\n\narticulation_rate\n\nNumber of syllables per second (numeric).\n\nf0mn\n\nMean fundamental frequency (f0) (numeric).\n\nf0sd\n\nStadard deviation of fundamental frequency (numeric).\n\nf0range\n\nMinimum and maximum fundamental frequency (numeric).\n\ninmn\n\nMean intensity (numeric).\n\ninsd\n\nStandard deviation of intensity (numeric).\n\ninrange\n\nMinimum and maximum fundamental frequency (numeric).\n\nshimmer\n\nLocal shimmer (likewise normalized amplitude difference of consecutive periods) (numeric).\n\njitter\n\nLocal jitter (bsolute period-to-period difference divided by the average period) (numeric).\n\nHNRmn\n\nMean Harmonics-to-Noise Ratio (numeric).\n\nH1H2\n\nDifference between first and second harmonic (H1-H2) (numeric).\n\nbreath_count\n\nNumber of audible breath intakes (count).\n\nfiller_count\n\nNumber of oral fillers like “oh/ah” (count).\n\nhiss_count\n\nNumber of noisy breath intakes (count).\n\nnasal_count\n\nNumber of nasal fillers like “mh/nh” (count).\n\nsil_count\n\nNumber of silent pauses (count).\n\nya_count\n\nNumber of occurences of interjection “ya” (informal) (count).\n\nyey_count\n\nNumber of occurences of interjection “yey” (polite) (count)."
  },
  {
    "objectID": "data/winter2012/polite.html#description",
    "href": "data/winter2012/polite.html#description",
    "title": "The phonetic profile of Korean formal and informal speech registers",
    "section": "",
    "text": "subject\n\nSubject unique identifier (categorical).\n\ngender\n\nGender of subject (categorical).\n\nbirthplace\n\nBirth place of subject (categorical).\n\nmusicstudent\n\nDoes the subject have music training? (binary: yes, no)\n\nscenario\n\nUnique identifier of different items.\n\ntask\n\nTask type (categorical: not = mailbox task vs dct = discourse completion task). In the mailbox task people left a note on somebody’s mailbox, while in the discourse completion task they were prompted to role-play the start of a conversation.\n\nattitude\n\nAttitude (binary: pol polite vs inf informal).\n\ntotal_duration\n\nTotal duration of utterances in seconds (numeric).\n\narticulation_rate\n\nNumber of syllables per second (numeric).\n\nf0mn\n\nMean fundamental frequency (f0) (numeric).\n\nf0sd\n\nStadard deviation of fundamental frequency (numeric).\n\nf0range\n\nMinimum and maximum fundamental frequency (numeric).\n\ninmn\n\nMean intensity (numeric).\n\ninsd\n\nStandard deviation of intensity (numeric).\n\ninrange\n\nMinimum and maximum fundamental frequency (numeric).\n\nshimmer\n\nLocal shimmer (likewise normalized amplitude difference of consecutive periods) (numeric).\n\njitter\n\nLocal jitter (bsolute period-to-period difference divided by the average period) (numeric).\n\nHNRmn\n\nMean Harmonics-to-Noise Ratio (numeric).\n\nH1H2\n\nDifference between first and second harmonic (H1-H2) (numeric).\n\nbreath_count\n\nNumber of audible breath intakes (count).\n\nfiller_count\n\nNumber of oral fillers like “oh/ah” (count).\n\nhiss_count\n\nNumber of noisy breath intakes (count).\n\nnasal_count\n\nNumber of nasal fillers like “mh/nh” (count).\n\nsil_count\n\nNumber of silent pauses (count).\n\nya_count\n\nNumber of occurences of interjection “ya” (informal) (count).\n\nyey_count\n\nNumber of occurences of interjection “yey” (polite) (count)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for LEL",
    "section": "",
    "text": "Welcome to the main site of the course Data Analysis for Linguistics and English Language (Semester 2).\nThis website is your go-to place throughout the semester for any info related to the course.\n\n\n\n\n\n\nCourse description\n\n\n\nThis course is an introduction to quantitative data analysis (including data wrangling, visualisation and modelling) as commonly employed in linguistics, using the R software.\nWe will cover the following topics:\n\nThe basics of quantitative data analysis.\nData preparation.\nData summaries.\nPrinciples of data visualisation.\n\nAt completion of the course you will have gained the following skills:\n\nImport common data formats, tidy and transform data.\nChoosing and reporting appropriate summary measures.\nUsing compelling visualisations to communicate a specific message about patterns in the data.\n\nExamples from different branches of linguistics will be used to provide you with hands-on experience in quantitative data analysis and Open Research practices.\n\n\n\n\n\n\n\n\nCourse rationale\n\n\n\nThis course is designed to help you develop the necessary skills for conducting and interpreting analyses of data as commonly employed in linguistics.\nThe content and objectives of the course are in response to recent advances in our understanding of the theory behind research methods.\nRecent meta-scientific research has identified three important aspects of research: the reproducibility, replicability and generalisability.\n\nA result is reproducible when the same analysis steps performed on the same dataset consistently produces the same answer.\nA result is replicable when the same analysis performed on different datasets produces qualitatively similar answers.\nA result is generalisable when a different analysis workflow performed on different data sets produces qualitatively similar answers.\n\nSee Definitions for a more detailed explanation.\nHowever, based on surveys from different disciplines, we are currently facing the three research crises (reproducibility, replicability and generalisability crises) by which most results are neither reproducible, nor replicable, nor generalisable (Munafò et al. 2017, Simmons et al. 2011, Ioannidis 2005, Yarkoni 2022).\nThe Open Research movement (also known as Open Science or Open Scholarship, Crüwell et al. 2019) was developed with the aim of improving our understanding of these crises and with the objective of providing researchers with guidelines and tools to produce reproducible, replicable and generalisable research.\nThe statistical philosophy adopted in the course is that of the New Statistics (Cumming 2014, Kruschke and Liddell 2018. The main goal of the New Statistics is to shift the attention from statistical significance (p-values) to estimation of effect sizes and quantification of uncertainty. Although the course will not cover the New Statistics directly, you will learn a few basic concepts that pave the way to learning about the New Statistics.\n\n\n\n\n\n\n\n\nAsk for help\n\n\n\nIf at any point during the course you don’t feel comfortable with any aspect of the course, you are unsure about anything that has been covered in class or in your own time, you are struggling to keep up with the course workload, you are experiencing mental of physical distress due to a pre-existing or new illness, medical condition or disability, or you find yourself unable to access basic needs like food or housing, please do get in touch with me and/or the PPLS support (go to the PPLS UG or MSc Hub on SharePoint &gt; Support for students &gt; Health & Wellbeing).\nWe are humans first and the rapidly-changing new world we are living in now can put us under pressure. What is most important to me is that you are first and foremost healthy and able to participate to the course, and that you succeed and get the most out of the course.\nIt is OK not to be OK, and remember that you are not alone. Other people, teachers and students, might be struggling right now or have struggled before and might have gone through what you are going through now. Remember that support exists for you, so please do reach out. If you see somebody close to you struggling, please let them know about the available support network and encourage them to reach out.\n\n\n\n\n\n\n\n\nContacts\n\n\n\nYou can reach me (Stefano) at s.coretta@ed.ac.uk or on Teams.\nIf you want to book office hours with me or the tutors, you can do so here: https://bit.ly/33BH84L. Location depends on whom you are seeing and which day, so check the confirmation email for that info!"
  },
  {
    "objectID": "slides/lecture-w01.html#course-info",
    "href": "slides/lecture-w01.html#course-info",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Course info",
    "text": "Course info\n\n\nThe main course website https://uoelel.github.io/dal/.\nCourse announcements are sent via Learn.\nASK FOR HELP: It is OK not to be OK, and remember that you are not alone.\n\nGo to the PPLS UG or PG Hub (on SharePoint Online) &gt; Support for students &gt; Wellbeing and Health.\nCome to my office hours (booking link on the course website homepage).\n\nAssessment:\n\nFormative assessments: Two formative assessments.\nSummative assessments: Two summative assessments (50-50).\nInfo on the course website."
  },
  {
    "objectID": "slides/lecture-w01.html#course-rationale",
    "href": "slides/lecture-w01.html#course-rationale",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Course rationale",
    "text": "Course rationale\n\n\nDAL (Data Analysis for LEL) is a new course designed for undergraduate students.\nThe main learning objective of this course is to allow you to conduct basic data reports which can be applied to linguistic questions and data, for example as part of your dissertation.\nWe will focus on modern techniques of data handling and quantitative analysis.\n\n\n\n\n\nThis is not a programming course. (For that, see the Advanced R book).\nThere will be little maths.\nThe course covers the basics, but you will very likely have to learn something extra for your dissertation.\nWe will not cover inferential statistics (including significance testing)."
  },
  {
    "objectID": "slides/lecture-w01.html#research-methods",
    "href": "slides/lecture-w01.html#research-methods",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Research methods",
    "text": "Research methods"
  },
  {
    "objectID": "slides/lecture-w01.html#research-process",
    "href": "slides/lecture-w01.html#research-process",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Research process",
    "text": "Research process"
  },
  {
    "objectID": "slides/lecture-w01.html#research-process-1",
    "href": "slides/lecture-w01.html#research-process-1",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Research process",
    "text": "Research process"
  },
  {
    "objectID": "slides/lecture-w01.html#data-analysis",
    "href": "slides/lecture-w01.html#data-analysis",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Data analysis",
    "text": "Data analysis"
  },
  {
    "objectID": "slides/lecture-w01.html#what-is-the-purpose-of-data-analysis",
    "href": "slides/lecture-w01.html#what-is-the-purpose-of-data-analysis",
    "title": "Data Analysis for LEL - Week 1",
    "section": "What is the purpose of data analysis?",
    "text": "What is the purpose of data analysis?\n\nDiscuss in small groups.\nWrite your group answers on Wooclap."
  },
  {
    "objectID": "slides/lecture-w01.html#data-analysis-1",
    "href": "slides/lecture-w01.html#data-analysis-1",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Data analysis",
    "text": "Data analysis"
  },
  {
    "objectID": "slides/lecture-w01.html#data-analysis-2",
    "href": "slides/lecture-w01.html#data-analysis-2",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Data analysis",
    "text": "Data analysis\n\nThe numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.\n\n—Nate Silver, The Signal and the Noise"
  },
  {
    "objectID": "slides/lecture-w01.html#uncertainty-and-variability",
    "href": "slides/lecture-w01.html#uncertainty-and-variability",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Uncertainty and variability",
    "text": "Uncertainty and variability"
  },
  {
    "objectID": "slides/lecture-w01.html#can-you-guess-what-this-is",
    "href": "slides/lecture-w01.html#can-you-guess-what-this-is",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Can you guess what this is?",
    "text": "Can you guess what this is?"
  },
  {
    "objectID": "slides/lecture-w01.html#statistics-as-a-tool-to-deal-with-uncertainty-and-variability",
    "href": "slides/lecture-w01.html#statistics-as-a-tool-to-deal-with-uncertainty-and-variability",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Statistics as a tool to deal with uncertainty and variability",
    "text": "Statistics as a tool to deal with uncertainty and variability\n\nStatistics is the science concerned with developing and studying methods for collecting, analyzing, interpreting and presenting empirical data. (From UCI Department of Statistics)\n\n\n\nStatistics is the technology of extracting information, illumination and understanding from data, often in the face of uncertainty. (From the British Academy)\n\n\n\n\nStatistics is a mathematical and conceptual discipline that focuses on the relation between data and hypotheses. (From the Standford Encyclopedia of Philosophy)\n\n\n\n\nStatistics as the art of applying the science of scientific methods. (From ORI Results, Nature)"
  },
  {
    "objectID": "slides/lecture-w01.html#statistics-as-an-art-and-as-a-science",
    "href": "slides/lecture-w01.html#statistics-as-an-art-and-as-a-science",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Statistics as an art and as a science",
    "text": "Statistics as an art and as a science\n\nStatistic is both a science and an art\nIt is a science in that its methods are basically systematic and have general application and an art in that their successful application depends, to a considerable degree, on the skill and special experience of the statistician, and on his knowledge of the field of application.\n—L. H. C. Tippett"
  },
  {
    "objectID": "slides/lecture-w01.html#statistics-is-not-infallible",
    "href": "slides/lecture-w01.html#statistics-is-not-infallible",
    "title": "Data Analysis for LEL - Week 1",
    "section": "Statistics is not infallible",
    "text": "Statistics is not infallible\n\n\nBut…\nall that glisters is not gold"
  },
  {
    "objectID": "slides/lecture-w03.html#research-project-management",
    "href": "slides/lecture-w03.html#research-project-management",
    "title": "Data Analysis for LEL - Week 3",
    "section": "Research project management",
    "text": "Research project management"
  },
  {
    "objectID": "slides/lecture-w03.html#data-management-plan-dmp",
    "href": "slides/lecture-w03.html#data-management-plan-dmp",
    "title": "Data Analysis for LEL - Week 3",
    "section": "Data Management Plan (DMP)",
    "text": "Data Management Plan (DMP)\n\nA Data Management Plan (DMP) covers data types and volume, capture, storage, integrity, confidentiality, retention and destruction, sharing and deposit.\n\nUoE Research Data Management policy.\nUoE DMP online."
  },
  {
    "objectID": "slides/lecture-w03.html#research-compendium",
    "href": "slides/lecture-w03.html#research-compendium",
    "title": "Data Analysis for LEL - Week 3",
    "section": "Research Compendium",
    "text": "Research Compendium\n\nA research compendium accompanies, enhances, or is a scientific publication providing data, code, and documentation for reproducing a scientific workflow.\n\n—Research Compendium\n\n\nA research compendium is a collection of all digital parts of a research project including data, code, texts (protocols, reports, questionnaires, meta data). The collection is created in such a way that reproducing all results is straightforward.\n\n—The Turing Way: Research Compendia"
  },
  {
    "objectID": "slides/lecture-w03.html#organise-files",
    "href": "slides/lecture-w03.html#organise-files",
    "title": "Data Analysis for LEL - Week 3",
    "section": "Organise files",
    "text": "Organise files\n\n\nCreate one folder and make that the folder for your dissertation project.\nIn that folder, create folders for data/ and for scripts/ (and plots/, dissertation/, etc).\n\n\n\n\n\nIn data/ have a raw/ and derived/ folder:\n\nRaw data (data that, if lost, it is very unfortunate; for example, experiment data, data which was manually annotated, etc) should be saved in data/raw/.\nDerived data (data that is derived with scripts) should be saved in data/derived/."
  },
  {
    "objectID": "slides/lecture-w03.html#organise-files-example",
    "href": "slides/lecture-w03.html#organise-files-example",
    "title": "Data Analysis for LEL - Week 3",
    "section": "Organise files: example",
    "text": "Organise files: example"
  },
  {
    "objectID": "slides/lecture-w03.html#back-up",
    "href": "slides/lecture-w03.html#back-up",
    "title": "Data Analysis for LEL - Week 3",
    "section": "Back up",
    "text": "Back up\n\nMake sure you have a backup system in place.\n\n\n\n\nSave copies of the entire folder in an external hard drive.\nSaving copies of the entire folder in an online storage service (iCloud Drive, One Drive, DropBox, Google Drive, …).\n\nBut if you are working on that copy via syncing, make sure you have a second independent place you back up to, like a hard drive.\n\nUsing a versioning system like git."
  },
  {
    "objectID": "slides/lecture-w03.html#research-projects-are-dynamic",
    "href": "slides/lecture-w03.html#research-projects-are-dynamic",
    "title": "Data Analysis for LEL - Week 3",
    "section": "Research projects are dynamic",
    "text": "Research projects are dynamic\n\n\nBe prepared to change how files and folders are organised after you start.\nProjects evolve over time and sometimes you need to clean things up.\n\n\n\n\n\nUse a good system to mark versions in your files. Two simple systems:\n\nUse full DATE in the file name\n\ndissertation-2022-11-21.\ndissertation-2023-03-01.\n\nOr use version number\n\nInspired by Semantic versioning from programming but can be helpful with research files too!\ndissertation-v1.0.\ndissertation-v1.1.\ndissertation-v2.0."
  },
  {
    "objectID": "slides/lecture-w03.html#file-naming-donts",
    "href": "slides/lecture-w03.html#file-naming-donts",
    "title": "Data Analysis for LEL - Week 3",
    "section": "File naming don’ts",
    "text": "File naming don’ts"
  },
  {
    "objectID": "slides/lecture-w03.html#licensing",
    "href": "slides/lecture-w03.html#licensing",
    "title": "Data Analysis for LEL - Week 3",
    "section": "Licensing",
    "text": "Licensing\n\nA license gives someone official permission to reuse something while protecting the intellectual property of the original creator.\n\n\nUse open licenses to ensure the data/code can be used by other researchers.\nThe Creative Commons licenses are now common in research."
  },
  {
    "objectID": "slides/lecture-w03.html#activity",
    "href": "slides/lecture-w03.html#activity",
    "title": "Data Analysis for LEL - Week 3",
    "section": "Activity",
    "text": "Activity\n\nDiscuss in small groups.\n\nHow have you organised your files so far?\nSomething you would like to change?\nSomething you would like to keep?"
  },
  {
    "objectID": "slides/lecture-w05.html#sample-y",
    "href": "slides/lecture-w05.html#sample-y",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Sample \\(y\\)",
    "text": "Sample \\(y\\)\n\nWhen we ask a research question, we collect a sample \\(y\\) from a population."
  },
  {
    "objectID": "slides/lecture-w05.html#sample-y-1",
    "href": "slides/lecture-w05.html#sample-y-1",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Sample \\(y\\)",
    "text": "Sample \\(y\\)\n\n\\(y\\) is a sample of values (\\(y_1, y_2, y_3, ..., y_n\\)).\n\n\n\nSample of values can be e.g.:\n\nNumber of telic and atelic verbs in a historical corpus of Sanskrit.\nVoice Onset Time of stops from 50 speakers Mapudungun.\nFriendliness ratings of synthetic speech as indicated by 300 participants.\n…"
  },
  {
    "objectID": "slides/lecture-w05.html#sample-y-2",
    "href": "slides/lecture-w05.html#sample-y-2",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Sample \\(y\\)",
    "text": "Sample \\(y\\)\n\n\\(y\\) is a sample of values (\\(y_1, y_2, y_3, ..., y_n\\)).\n\n\nWe say that the values in the sample \\(y\\) were generated by a (random) variable \\(Y\\)."
  },
  {
    "objectID": "slides/lecture-w05.html#variable-y",
    "href": "slides/lecture-w05.html#variable-y",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Variable \\(Y\\)",
    "text": "Variable \\(Y\\)\n\n\\(Y\\) is a (random) variable that generates the values in the sample \\(y\\).\n\n\n\nA (statistical) variable is any characteristics, number, or quantity that can be measured or counted\n\nWhen you observe or measure something, you are taking note of the values generated by the variable.\nIt’s called variable because it varies (ha!).\nThe opposite of a variable is a constant."
  },
  {
    "objectID": "slides/lecture-w05.html#sample-y-3",
    "href": "slides/lecture-w05.html#sample-y-3",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Sample \\(y\\)",
    "text": "Sample \\(y\\)\n\n\\(Y\\) is a (random) variable that generates the values in the sample \\(y\\).\n\n\nVariables can be e.g.:\n\nToken number of telic verbs and atelic verbs in written Sanskrit.\nVoice Onset Time of stops in Mapudungun.\nFriendliness ratings of synthetic speech.\n…"
  },
  {
    "objectID": "slides/lecture-w05.html#types-of-variables",
    "href": "slides/lecture-w05.html#types-of-variables",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Types of variables",
    "text": "Types of variables"
  },
  {
    "objectID": "slides/lecture-w05.html#types-of-variables-1",
    "href": "slides/lecture-w05.html#types-of-variables-1",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Types of variables",
    "text": "Types of variables"
  },
  {
    "objectID": "slides/lecture-w05.html#types-of-variables-2",
    "href": "slides/lecture-w05.html#types-of-variables-2",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Types of variables",
    "text": "Types of variables\n\nNumeric continuous variable: between any two values there is an infinite number of values.\n\nThe variable can take on any positive and negative number, including 0.\nThe variable can take on any positive number only.\nProportions and percentages: The variable can take on any number between 0 and 1.\n\n\n\n\nNumeric discrete variable: between any two consecutive values there are no other values.\n\nCounts: The variable can take only on any positive integer number."
  },
  {
    "objectID": "slides/lecture-w05.html#types-of-variables-3",
    "href": "slides/lecture-w05.html#types-of-variables-3",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Types of variables",
    "text": "Types of variables\n\nCategorical (discrete) variable.\n\nBinary or dichotomous: The variable can take only one of two values.\nThe variable can take any of three of more values.\nOrdinal: The variable can take any of three of more values and the values have a natural order."
  },
  {
    "objectID": "slides/lecture-w05.html#operationalisation",
    "href": "slides/lecture-w05.html#operationalisation",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Operationalisation",
    "text": "Operationalisation\n\nWe can operationalise something as a numeric or a categorical variable.\n\n\n\nThink of ways to operationalise the following:\n\nVoice Onset Time.\nFriendliness of speech.\nLexical frequency.\n…"
  },
  {
    "objectID": "slides/lecture-w05.html#operationalisation-1",
    "href": "slides/lecture-w05.html#operationalisation-1",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Operationalisation",
    "text": "Operationalisation"
  },
  {
    "objectID": "slides/lecture-w05.html#summary",
    "href": "slides/lecture-w05.html#summary",
    "title": "Data Analysis for LEL - Week 5",
    "section": "Summary",
    "text": "Summary\n\n\nThe sample \\(y\\) is generated by a (random) variable \\(Y\\).\nA (statistical) variable is any characteristics, number, or quantity that can be measured or counted.\nVariables can be numeric or categorical.\n\nNumeric variables can be continuous or discrete.\nCategorical variables are only discrete.\n\nWe operationalise a measure/observation as a numeric or a categorical variable."
  },
  {
    "objectID": "slides/lecture-w07.html#types-of-data",
    "href": "slides/lecture-w07.html#types-of-data",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Types of data",
    "text": "Types of data\n\n\nTabular (or rectangular) data (like spreadsheets).\nAudio and/or video recordings.\nTexts or transcripts.\nAnnotation (ELAN, TextGrids, …).\nImages."
  },
  {
    "objectID": "slides/lecture-w07.html#tabular-data",
    "href": "slides/lecture-w07.html#tabular-data",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Tabular data",
    "text": "Tabular data\n\n\nTabular data is made of rows and columns.\n\n\n\n\nPrefer formats like Comma Separated Values (.csv) or Tab Separated Values (.tsv) over MS Excel files. For big data, use Parquet files.\nIf you use Excel files, keep one sheet per Excel file! (Don’t have data in multiple sheet within the same Excel file).\nInclude ONE TABLE per file. (You can transform and summarise data in R)."
  },
  {
    "objectID": "slides/lecture-w07.html#tabular-data-dont",
    "href": "slides/lecture-w07.html#tabular-data-dont",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Tabular data: DON’T",
    "text": "Tabular data: DON’T"
  },
  {
    "objectID": "slides/lecture-w07.html#coding-data",
    "href": "slides/lecture-w07.html#coding-data",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Coding data",
    "text": "Coding data\n\nUse explicit coding:\n\nDon’t use colours to code your data! (Software like R will discard colours and any formatting).\nEach variable to be coded should have its own column.\nUse clear labels:\n\nACCURACY: incorrect, correct.\n\nNot 0, 1.\n\nDYSLEXIC: dyslexic, non-dyslexic (or control).\n\nNot 0, 1 or yes, no.\n\nVOWEL: a, i, u.\nYEAR ABROAD: year_abroad, no_abroad\n\nNot yes, no."
  },
  {
    "objectID": "slides/lecture-w07.html#coding-data-dont",
    "href": "slides/lecture-w07.html#coding-data-dont",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Coding data: DON’T",
    "text": "Coding data: DON’T"
  },
  {
    "objectID": "slides/lecture-w07.html#coding-data-dont-1",
    "href": "slides/lecture-w07.html#coding-data-dont-1",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Coding data: DON’T",
    "text": "Coding data: DON’T"
  },
  {
    "objectID": "slides/lecture-w07.html#coding-data-do",
    "href": "slides/lecture-w07.html#coding-data-do",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Coding data: DO",
    "text": "Coding data: DO"
  },
  {
    "objectID": "slides/lecture-w07.html#tidy-data",
    "href": "slides/lecture-w07.html#tidy-data",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Tidy data",
    "text": "Tidy data\n\n\nhttps://github.com/allisonhorst/stats-illustrations"
  },
  {
    "objectID": "slides/lecture-w07.html#tidy-data-1",
    "href": "slides/lecture-w07.html#tidy-data-1",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Tidy data",
    "text": "Tidy data\n\n\nhttps://github.com/allisonhorst/stats-illustrations"
  },
  {
    "objectID": "slides/lecture-w07.html#tidy-data-dont",
    "href": "slides/lecture-w07.html#tidy-data-dont",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Tidy data: DON’T",
    "text": "Tidy data: DON’T"
  },
  {
    "objectID": "slides/lecture-w07.html#tidy-data-do",
    "href": "slides/lecture-w07.html#tidy-data-do",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Tidy data: DO",
    "text": "Tidy data: DO"
  },
  {
    "objectID": "slides/lecture-w07.html#tidy-data-do-1",
    "href": "slides/lecture-w07.html#tidy-data-do-1",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Tidy data: DO",
    "text": "Tidy data: DO"
  },
  {
    "objectID": "slides/lecture-w07.html#tidy-data-do-2",
    "href": "slides/lecture-w07.html#tidy-data-do-2",
    "title": "Data Analysis for LEL - Week 7",
    "section": "Tidy data: DO",
    "text": "Tidy data: DO"
  },
  {
    "objectID": "slides/lecture-w09.html#where-to-next",
    "href": "slides/lecture-w09.html#where-to-next",
    "title": "Data Analysis for LEL - Week 9",
    "section": "Where to next?",
    "text": "Where to next?\n\nIllustration by Yuliya Pauliukevich on www.vecteezy.com."
  },
  {
    "objectID": "slides/lecture-w09.html#statistical-modelling",
    "href": "slides/lecture-w09.html#statistical-modelling",
    "title": "Data Analysis for LEL - Week 9",
    "section": "Statistical modelling",
    "text": "Statistical modelling"
  },
  {
    "objectID": "slides/lecture-w09.html#section-2",
    "href": "slides/lecture-w09.html#section-2",
    "title": "Data Analysis for LEL - Week 9",
    "section": "",
    "text": "A statistical model is a (simplified) mathematical representation of a process.\n\n\n\nIllustration by Yuliya Pauliukevich on www.vecteezy.com."
  },
  {
    "objectID": "slides/lecture-w09.html#linear-models",
    "href": "slides/lecture-w09.html#linear-models",
    "title": "Data Analysis for LEL - Week 9",
    "section": "Linear models",
    "text": "Linear models\n\n\nLinear models are a type of statistical model.\nIn their simplest form, they are equivalent to the formula of a line.\n\n\\[\ny = a + b \\cdot x\n\\]"
  },
  {
    "objectID": "slides/lecture-w09.html#obscure-terminology",
    "href": "slides/lecture-w09.html#obscure-terminology",
    "title": "Data Analysis for LEL - Week 9",
    "section": "Obscure terminology",
    "text": "Obscure terminology\n\nSee One Thousand and One names.\n\nIllustration by Matt Cole on www.vecteezy.com."
  },
  {
    "objectID": "slides/lecture-w09.html#statistical-inference",
    "href": "slides/lecture-w09.html#statistical-inference",
    "title": "Data Analysis for LEL - Week 9",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nFrequentist inference\n\nNull Hypothesis Significance Testing (NHST).\nInference based on p-values (they don’t mean what most people think they mean).\nMUST READ: Mindless statistics by Gigerenzer.\n\n\n\n\nBayesian inference\n\nInference based on the probabilities of possible values.\nRe-use prior knowledge to make research a cumulative enterprise.\nMUST READ: How to become a Bayesian in eight easy steps by Etz et al."
  },
  {
    "objectID": "slides/lecture-w09.html#machine-learning",
    "href": "slides/lecture-w09.html#machine-learning",
    "title": "Data Analysis for LEL - Week 9",
    "section": "Machine learning",
    "text": "Machine learning\n\n\nBeware the golems! See The Golem of Prague by McElreath.\nMachine Learning is a mindset. See [Modeling Mindsets] by Molnar.\nNo interest in the how and why, just in predictive accuracy."
  },
  {
    "objectID": "slides/lecture-w09.html#not-the-end",
    "href": "slides/lecture-w09.html#not-the-end",
    "title": "Data Analysis for LEL - Week 9",
    "section": "(Not) the end",
    "text": "(Not) the end\n\n\nIllustration by Artyom Kozhemyakin on www.vecteezy.com."
  },
  {
    "objectID": "tutorials/tutorial-w02.html",
    "href": "tutorials/tutorial-w02.html",
    "title": "DAL tutorial - Week 2",
    "section": "",
    "text": "Last week you started your R journey with the R Console.\nWorking with the R Console can quickly become a bit inefficient: imagine having to run a lot of code, read a lot of different files, keeping track of a lot of variables and so on…\nThis is what Integrated Development Environment (IDE) software comes in: an IDE is just a graphical interface to programming languages that offer users with a lot of features to help them streamline their workflow. (But don’t get fooled! You still have to learn how to code.)\nR has a dedicated IDE called RStudio. This is what we will use from this week on. Note that RStudio even works with Python and many other languages!\n\n\nBeginners usually have trouble understanding the difference between R and RStudio.\nLet’s use a car analogy.\nWhat makes the car go is the engine and you can control the engine through the dashboard.\nYou can think of R as an engine and RStudio as the dashboard.\n\n\n\n\n\n\n\nR\n\n\n\n\nR is a programming language.\nWe use programming languages to interact with computers.\nYou run commands written in a console and the related task is executed.\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\n\nRStudio is an Integrated Development Environment or IDE.\nIt helps you using R more efficiently.\nIt has a graphical user interface or GUI.\n\n\n\nThe next section will give you a tour of RStudio."
  },
  {
    "objectID": "tutorials/tutorial-w02.html#rstudio",
    "href": "tutorials/tutorial-w02.html#rstudio",
    "title": "DAL tutorial - Week 2",
    "section": "",
    "text": "Last week you started your R journey with the R Console.\nWorking with the R Console can quickly become a bit inefficient: imagine having to run a lot of code, read a lot of different files, keeping track of a lot of variables and so on…\nThis is what Integrated Development Environment (IDE) software comes in: an IDE is just a graphical interface to programming languages that offer users with a lot of features to help them streamline their workflow. (But don’t get fooled! You still have to learn how to code.)\nR has a dedicated IDE called RStudio. This is what we will use from this week on. Note that RStudio even works with Python and many other languages!\n\n\nBeginners usually have trouble understanding the difference between R and RStudio.\nLet’s use a car analogy.\nWhat makes the car go is the engine and you can control the engine through the dashboard.\nYou can think of R as an engine and RStudio as the dashboard.\n\n\n\n\n\n\n\nR\n\n\n\n\nR is a programming language.\nWe use programming languages to interact with computers.\nYou run commands written in a console and the related task is executed.\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\n\nRStudio is an Integrated Development Environment or IDE.\nIt helps you using R more efficiently.\nIt has a graphical user interface or GUI.\n\n\n\nThe next section will give you a tour of RStudio."
  },
  {
    "objectID": "tutorials/tutorial-w02.html#rstudio-2",
    "href": "tutorials/tutorial-w02.html#rstudio-2",
    "title": "DAL tutorial - Week 2",
    "section": "2 RStudio",
    "text": "2 RStudio\nWhen you open RStudio, you can see the window is divided into 3 panels:\n\nBlue (left): the R Console. This is basically the same thing as the R Console you have been using last week.\nGreen (top-right): the Environment tab.\nPurple (bottom-right): the Files tab.\n\n\nThe Console is where R commands can be executed.\nThe Environment tab lists the objects created with R, while in the Files tab you can navigate folders on your computer to get to files and open them in the file Editor.\n\n2.1 RStudio and Quarto projects\nRStudio is an IDE (see above) which allows you to work efficiently with R, all in one place.\nNote that files and data live in folders on your computer, outside of RStudio: do not think of RStudio as an app where you can save files in.\nAll the files that you see in the Files tab are files on your computer and you can access them from the Finder or File Explorer as you would with any other file.\nIn principle, you can open RStudio and then navigate to any folder or file on your computer.\nHowever, there is a more efficient way of working with RStudio: RStudio Projects.\n\n\n\n\n\n\nRStudio Projects\n\n\n\nAn RStudio Project is a folder on your computer that has an .Rproj file.\n\n\nA special type of RStudio project are Quarto Projects. We will use these in this course.\n\n\n\n\n\n\nQuarto Projects\n\n\n\nA Quarto Project is an RStudio project which has a _quarto.yml file.\n\n\nYou will learn a bit more about the _quarto.yml file below.\nYou can create as many Quarto Projects as you wish, and I recommend to create one per project (your dissertation, a research project, a course, etc…). Also, I strongly recommend that you DO NOT save projects on One Drive, but do back up your files there (or at least somewhere else). This is known to cause issues, so it is best to save projects on your Documents folder, for example.\nWe will create a Quarto Project for this course (meaning, you will create a folder for the course which will be the Quarto Project). You will have to use this project/folder throughout the semester.\nTo create a new Quarto Project, click on the button that looks like a transparent light blue box with a plus, in the top-left corner of RStudio. A window like the one below will pop up.\n\nClick on New Directory then Quarto Project.\n\nNow, this will create a new folder (aka directory) on your computer and will make that an RStudio Project (meaning, it will add a file with the .Rproj extension to the folder; the name of the file will be the name of the project/folder).\nGive a name to your new project, something like the name of the course and year (e.g. dal-2024).\nThen you need to specify where to create this new folder/Project. Click on Browse… and navigate to the folder you want to create the new folder/Project in. DO NOT use One Drive, as mentioned above.\n**Make sure to untick the Use visual markdown editor option and that the engine is set to knitr.** The settings should be exactly as shown below.\n\nWhen done, click on Create Project. RStudio will automatically open your new project. A .qmd file will be opened automatically: you can safely close that for now.\nThe project folder will contain the following files:\n\nA _quarto.yml file, that tells RStudio this is a Quarto Project.\nA .qmd file, named after the name of project. You will learn about .qmd files next week. You can close this file if it is still open in the File panel.\nAn .rproj file, named after the name of the project. This file is there just to inform RStudio that this folder is a project and you are not supposed to edit it.\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen working through these tutorials, always make sure you are in the course’s RStudio Quarto Project you created.\nYou know you are in an RStudio Project because you can see the name of the Project in the top-right corner of RStudio, next to the light blue cube icon.\nIf you see Project (none) in the top-right corner, that means your are not in an RStudio Project.\nTo make sure you are in the RStudio project, to open the project go to the project folder in File Explorer or Finder and double click on the .Rproj file.\n\n\nThere are several ways of opening an RStudio Project:\n\nYou can go to the RStudio Project folder in Finder or File Explorer and double click on the .Rproj file.\nYou can click on File &gt; Open Project in the RStudio menu.\nYou can click on the project name in the top-right corner of RStudio, which will bring up a list of projects. Click on the desired project to open it.\n\n\n\n2.2 A few important settings\nBefore moving on, there are a few important settings that you need to change. See figure below for how they should look.\n\n\nOpen the RStudio preferences (Tools &gt; Global options..., might be different on Windows).\nUn-tick Restore .RData into workspace at startup.\n\nThis mean that every time you start RStudio you are working with a clean Environment. Not restoring the workspace ensures that the code you write is fully reproducible.\n\nSelect Never in Save workspace to .RData on exit.\n\nSince we are not restoring the workspace at start-up, we don’t need to save it. Remember that as long as you save the code, you will not lose any of your work! You will learn how to save code below.\n\nClick OK to confirm the changes.\n\n\n\n\n\n\n\nQuiz 1\n\n\n\nTrue or false?\n\nRStudio executes the code. TRUEFALSE\nR is a programming language. TRUEFALSE\nAn IDE is necessary to run R. TRUEFALSE\nRStudio projects are folders with an .Rproj file. TRUEFALSE\nThe project name is shown in the top-right corner of RStudio. TRUEFALSE"
  },
  {
    "objectID": "tutorials/tutorial-w02.html#running-code-in-the-console",
    "href": "tutorials/tutorial-w02.html#running-code-in-the-console",
    "title": "DAL tutorial - Week 2",
    "section": "3 Running code in the Console",
    "text": "3 Running code in the Console\nNow you can run R code in the Console from within RStudio.\nTry the following:\n\napples &lt;- 10\noranges &lt;- 6\ndurians &lt;- 2\n\nfruit_n &lt;- sum(apples, oranges, durians)\ncat(\"We have in total\", fruit_n, \"fruits.\")\n\nWe have in total 18 fruits.\n\n\nThe sentence We have in total 18 fruits. will be printed on the Console.\nMoreover, you will see that the variables we created (apples, oranges, durians, fruit_n) are listed in the Environment tab in the top-right panel of RStudio.\nThis is much better than having to use ls() to remember which variables you have created.\nNow create three more variables:\n\nThey should all be vectors of at least three elements.\nYou should create one numeric, one character and one logical vector.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo create vectors, you should use the c() function.\n\n\n\nNow check the Environment tab: your new variables will be there. The values of each variable should be prefixed with:\n\nThe vector type: num for numeric, chr for character and logi for logical.\nAnd the length of the vector in the form [1:N] where N is the number of element/values in the vector.\n\nThat’s neat! You can obtain the length of a vector (i.e. the number of element/values) with the length() function.\n\n\n\n\n\n\nLength of a vector\n\n\n\nThe length of a vector is the number of values contained in the vector.\nYou can obtain the vector length with length().\n\n\nFor example:\n\nwords &lt;- c(\"gold\", \"nice\", \"up\", \"of\")\nlength(words)\n\n[1] 4\n\n\nRemember you can always check the type of vector with the class() function."
  },
  {
    "objectID": "tutorials/tutorial-w02.html#r-scripts",
    "href": "tutorials/tutorial-w02.html#r-scripts",
    "title": "DAL tutorial - Week 2",
    "section": "4 R scripts",
    "text": "4 R scripts\nSo far, you’ve been asked to write code in the Console and run it there.\nBut this is not very efficient. Every time, you need to write the code and execute it in the right order and it quickly becomes very difficult to keep track of everything when things start getting more involved.\nA solution is to use R scripts.\n\n\n\n\n\n\nR script\n\n\n\nAn R script is a file with the .R extension that contains R code.\n\n\nFor the rest of this tutorial, you will write all code in an R script.\n\n4.1 Create an R script\nFirst, create a folder called code in your project folder. You can do so from within RStudio, in the Files tab or you can just create the folder in the File Explorer/Finder. This will be the folder where you will save all of your R scripts and other code files.\nNow, to create a new R script, look at the top-left corner of RStudio: the first button to the left looks like a white sheet with a green plus sign. This is the New file button. Click on that and you will see a few options to create a new file.\nClick on R Script. A new empty R script will be created and will open in the File Editor window of RStudio.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that creating an R script does not automatically saves it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\n\n\n\n\n\n\n\nSave the file inside the code/ folder with the following name: tutorial-w02.R.\n\n\n\n\n\n\nWarning\n\n\n\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer.\nSo you can always access them from the Finder or File Explorer! However, do not open a file by double clicking on it from the Finder/File Explorer.\nRather, open the RStudio project by double clicking on the .Rproj file and then open files from RStudio to ensure you are working within the RStudio project and the working directory is set correctly.\n\n\nNow your script is ready to be filled with code. Copy the following lines of code and paste them at the top of your R script (this is the same code as above).\n\napples &lt;- 10\noranges &lt;- 6\ndurians &lt;- 2\n\nfruit_n &lt;- sum(apples, oranges, durians)\ncat(\"We have in total\", fruit_n, \"fruits.\")\n\nwords &lt;- c(\"gold\", \"nice\", \"up\", \"of\")\nlength(words)\n\n\n\n4.2 Run code\nNow, there are several ways to run code.\nOne is to click on the Run button. You can find this in the top-right corner of the script window.\n\nWhen you click Run, R runs the line of code that currently has the text cursor (|) and then moves the cursor to the next line (you can click Run again to run the line and so on.) You can also select multiple lines on the script and click Run, and all the selected lines will be run.\nAn alternative way is to place the text cursor on the line of code you want to run and then press CMD+ENTER/CTRL+ENTER. This will run the line of code and move the text cursor to the next line of code, as if you had clicked Run.\nYou can even select multiple lines of code (as you would select text) and press CMD+ENTER/CTRL+ENTER to run multiple lines of code!\nNow that you know how to use R scripts and run code in them, I will assume that you will keep writing new code from this tutorial in your script and run it from there!\nIn the next section, you will learn how to extend R capabilities with packages."
  },
  {
    "objectID": "tutorials/tutorial-w02.html#r-packages",
    "href": "tutorials/tutorial-w02.html#r-packages",
    "title": "DAL tutorial - Week 2",
    "section": "5 R packages",
    "text": "5 R packages\nWhen you install R, a library of packages is also installed. Packages provide R with extra functionalities, usually by making extra functions available for use. You can think of packages as “plug-ins” that you install once and then you can “activate” them when you need them. The library installed with R contains a set of packages that are collectively known as the base R packages, but you can install more any time!\nNote that the R library is a folder on your computer. Packages are not installed inside RStudio. Remember that RStudio is just an interface.\nYou can check all of the currently installed packages in the bottom-right panel of RStudio, in the Packages tab. There you can also install new packages.\n\n\n\n\n\n\nR library and packages\n\n\n\n\nThe R library contains the base R packages and all the user-installed packages.\nR packages provide R with extra functionalities and are installed into the R library.\n\n\n\n\n\n\n\n\n\nExtra: Where is my R library?\n\n\n\nIf you want to find the path of the R library on your computer, type .libPaths() in the Console. The function returns (i.e. outputs) the path or paths where your R library is.\n\n\n\n5.0.1 Install packages\nYou can install extra packages in the R library in two ways:\n\nYou can use the install.packages() function. This function takes the name of the package you want to install as a string, for example install.packages(\"cowsay\").\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you install a package with the function install.packages(), do so in the Console! Do not include this function in your scripts (this is because you install packages only once, see below)).\n\n\n\nOr you can go the Packages tab in the bottom-right panel of RStudio and click on Install. A small window will pop up. See the screenshot below.\n\n\n\n\n\n\nGo ahead and try to install a package using the second method. Install the cowsay and the fortunes packages (see picture above for how to write the packages). After installing you will see that the package fortunes is listed in the Packages tab.\n\n\n\n\n\n\nInstall packages\n\n\n\nTo install packages, go to the Packages tab of the bottom-right panel of RStudio and click on Install.\nIn the “Install packages” window, list the package names and then click Install.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou need to install a package ONLY ONCE! Once installed, it’s there for ever, saved in the R library. You will be able to use all of your installed packages in any RStudio project you create.\n\n\n\n\n5.0.2 Attach packages\nNow, to use a package you need to attach the package to the current R session with the library() function. Attaching a package makes the functions that come with the package available to us.\n\n\n\n\n\n\nWarning\n\n\n\nYou need to attach the packages you want to use once per R session.\nNote that every time you open RStudio, a new R session is started.\n\n\nLet’s attach the cowsay and fortunes packages. Write the following code at the top of your R script, before all the other code you wrote.\n\nlibrary(cowsay)\nlibrary(fortunes)\n\nNote that library(cowsay) takes the name of the package without quotes, although if you put the name in quotes it also works. You need one library() function per package (there are other ways, but we will stick with this one).\n\n\n\n\n\n\nAttaching packages\n\n\n\nPackages are attached with the library(pkg.name) function, where pkg.name is the name of the package.\nIt is customary to put all the packages used in the script at the top of the script.\n\n\nNow you can use the functions provided by the attached packages. Try out the say() function from the cowsay package.\nWrite the following in your R script and run it!\n\nsay(\"hot diggity\", \"frog\")\n\n(I know, the usefulness of the package might be questionable, but it is fun!)\n\n\n\n\n\n\nWarning\n\n\n\nRemember, you need to install a package only once but you need to attach it with library() every time you start R.\nThink of install.packages() as mounting a light bulb (installing the package) and library() as the light switch (attaching the package).\n\n\n\n\n\n5.1 Package documentation\nTo learn what a function does, you can check its documentation by typing in the Console the function name preceded by a ? question mark. Type ?say in the Console and hit ENTER to see the function documentation. You should see something like this:\n\n\n\n\n\nThe Description section is usually a brief explanation of what the function does.\nIn the Usage section, the usage of the function is shown by showing which arguments the function has and which default values (if any) each argument has. When the argument does not have a default value, NULL is listed as the value.\nThe Arguments section gives a thorough explanation of each function argument. (Ignore … for now).\nHow many arguments does say() have? How many arguments have a default value?\nDefault argument values allow you to use the function without specifying those arguments. Just write say() in your script on a new line and run it. Does the output make sense based on the Usage section of the documentation?\nThe rest of the function documentation usually has further details, which are followed by Examples. It is always a good idea to look at the example and test them in the Console when learning new functions.\n\n\n\n\n\n\nQuiz 2\n\n\n\nWhich of the following statements is wrong?\n\n You attach libraries with library(). install.packages() does not load packages. The R library is a folder.\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThis was a question about terminology. In R, you attach packages from the library using (confusingly) the library() function."
  },
  {
    "objectID": "tutorials/tutorial-w02.html#including-comments",
    "href": "tutorials/tutorial-w02.html#including-comments",
    "title": "DAL tutorial - Week 2",
    "section": "6 Including comments",
    "text": "6 Including comments\nSometimes we might want to add a few lines of text in our script, for example to take notes.\nYou can add so-called comments in R scripts, simply by starting a line with #. If you add # at the end of a line, anything after that will be considered a comment. Comments are simply skipped when R runs code.\n\n\n\n\n\n\nComments\n\n\n\nYou can add text comments in R scripts by starting a new line with # or by writing text preceded by # at the end of any line of code.\n\n\nFor example:\n\n# This is a comment. Let's add 6 + 3.\n6 + 3\n\n[1] 9\n\n3 + 6 # is the same as 6 + 3\n\n[1] 9\n\n# You can write long comments like this, for example if you want to explain what\n# the code does or if you want remind yourself of something. It is usual practice\n# to start new lines when comments are very long, each line preceded by #. We\n# call these \"comment blocks\"\n\n\n\n\n\n\n\nQuiz 4\n\n\n\nIs the following a valid and complete line of R code? TRUEFALSE\nsum(3, 2 #)  4"
  },
  {
    "objectID": "tutorials/tutorial-w02.html#summary",
    "href": "tutorials/tutorial-w02.html#summary",
    "title": "DAL tutorial - Week 2",
    "section": "7 Summary",
    "text": "7 Summary\nYou made it! You completed this week’s tutorial.\nHere’s a summary of what you learnt.\n\n\n\n\n\n\n\nR is a programming language while RStudio is an IDE.\nQuarto projects are folders with an .Rproj file (you can see the name of the project you are currently in in the top-right corner of RStudio).\nR scripts contain R code and help you keep track of the code you run.\nR packages provide R with extra functionalities. The R library is a folder with all the installed packages.\n.libPaths() returns the path(s) to the R library.\nlibrary() attaches R packages (i.e. makes the package’s functions available for use).\nYou can inspect the documentation of any function by running ?function in the Console (where function is the function’s name, e.g. ?paste).\nYou can write text comments in R scripts by starting a line with #."
  },
  {
    "objectID": "tutorials/tutorial-w04.html",
    "href": "tutorials/tutorial-w04.html",
    "title": "DAL tutorial - Week 4",
    "section": "",
    "text": "Last week, you learnt how to use R scripts to save your code.\nKeeping track of the code you use for data analysis is a very important aspect of research project managing: not only the code is there if you need to rerun it later, but it allows your data analysis to be reproducible (i.e., it can be reproduced by you or other people in such a way that starting with the same data and code you get to the same results).\n\n\n\n\n\n\nReproducible research\n\n\n\nResearch is reproducible when the same data and same code return the same results.\nSee the Definitions page of The Turing Way for definitions of reproducible, replicable, robust and generalisable research.\n\n\nR scripts are great for writing code, and you can even document the code (add explanations or notes) with comments (i.e. lines that start with #).\nBut for longer text or complex data analysis reports, R scripts can be a bit cumbersome.\nA solution to this is using Quarto files (they have the .qmd extension).\n\n\n\n\n\n\nQuiz 1\n\n\n\nWhen is research not reproducible?\n\n a. When the results do not match the researcher's expectations. b. When the the same data and code as in the original study do not produce the published results. c. When research conducted by a different research team with new data does not produce the results as published in the original study. \n\n\n\n\n\n\n\nHint\n\n\n\n\n\nResearch is reproducible when you can produce the same results using the original data and code/methods.\nResearch is replicable when you can produce the same results using new data and the original code/methods.\nSee https://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html#table-of-definitions-for-reproducibility.\n\n\n\n\n\n\n\nQuarto is a file format that allows you to mix code and formatted text in the same file.\nThis means that you can write dynamic reports using Quarto files: dynamic reports are just like analysis reports (i.e. they include formatted text, plots, tables, code output, code, etc…) but they are dynamic in the sense that if, for example, data or code changes, you can just rerun the report file and all code output (plots, tables, etc…) is updated accordingly!\n\n\n\n\n\n\nDynamic reports in Quarto\n\n\n\nQuarto is a file type with extension .qmd in which you can write formatted text and code together.\nQuarto can be used to generate dynamic reports: these are files that are generated automatically from the file source, ensuring data and results in the report are always up to date.\n\n\n\n\n\nR comments in R scripts cannot be formatted (for example, you can’t make text bold or italic).\nText in Quarto files can be fully formatted using a simple but powerful mark-up language called markdown.\nYou don’t have to learn markdown all in one go, so I encourage you to just learn it bit by bit, at your pace. You can look at the the Markdown Guide for an in-depth intro and/or dive in the Markdown Tutorial for a hands-on approach.\nA few quick pointers (you can test them in the Markdown Live Preview):\n\nText can be made italics by enclosing it between single stars: *this text is in italics*.\nYou can make text bold with two stars: **this text is bold!**.\nHeadings are created with #:\n\n# This is a level-1 heading\n\n## This is a level-2 heading\n\n\n\n\n\n\nMark-up, Markdown\n\n\n\nA mark-up language is a text-formatting system consisting of symbols or keywords that control the structure, formatting or relationships of textual elements. The most common mark-up languages are HTML, XML and TeX.\nMarkdown is a simple yet powerful mark-up language.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen working through these tutorials, always make sure you are in the course Quarto Project you created before.\nYou know you are in a Quarto Project because you can see the name of the Project in the top-right corner of RStudio, next to the light-blue cube icon.\nIf you see Project (none) in the top-right corner, that means you are not in the Quarto Project.\nTo make sure you are in the Quarto project, you can open the project by going to the project folder in File Explorer (Windows) or Finder (macOS) and double click on the .Rproj file.\n\n\nTo create a new .qmd file, just click on the New file button (the white square with the green plus symbol), then Quarto Document.... (If you are asked to install/update packages, do so.)\n\n\n\n\n\nA window will open. Add a title of your choice and your name. Make sure the Use visual markdown editor is NOT ticked, then click Create (you will be free to use the visual editor later, but it is important that you first see what a Quarto document looks like under the hood first).\n\n\n\n\n\nA new .qmd file will be created and will open in the File Editor panel in RStudio.\nNote that creating a Quarto file does not automatically save it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\n\n\n\n\n\nSave the file inside the code/ folder with the following name: tutorial-w04.qmd.\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer.\n\n\n\nA Quarto file usually has three main parts:\n\nThe YAML header (green in the screenshot below).\nCode chunks (red).\nText (blue).\n\n\n\n\n\n\nEach Quarto file has to start with a YAML header, but you can include as many code chunks and as much text as you wish, in any order.\n\n\n\n\n\n\nQuarto: YAML header\n\n\n\nThe header of a .qmd file contains a list of key: value pairs, used to specify settings or document info like the title and author.\nYAML headers start and end with three dashes ---.\n\n\n\n\n\n\n\n\nQuarto: Code chunks\n\n\n\nCode chunks start and end with three back-ticks ``` and they contain code.\n{r} indicates that the code is R code. Settings can be specified inside the chunk with the #| prefix: for example #| label: setup sets the name of the chunk (the label) to setup.\n\n\n\n\n\nWhen using Quarto projects, the working directory (the directory all relative paths are relative to) is the project folder.\nHowever, when running code from a Quarto file, the code is run as if the working directory were the folder in which the file is saved.\nThis isn’t an issue if the Quarto file is directly in the project folder, but in our case our Quarto files live in the code/ folder within the project folder (and it is good practice to do so!).\nWe can instruct R to always run code from the project folder (i.e. the working directory is the project folder). This is when the _quarto.yml file comes into play.\nOpen the _quarto.yml file in RStudio (you can simply click on the file in the Files tab and that will open the file in the RStudio editor). Add the line execute-dir: project under the title. Note that indentation should be respected, so the line you write should align with title:, not with project:.\nproject:\n  title: \"dal\"\n  execute-dir: project\nNow, all code in Quarto files, no matter where they are saved, will be run with the project folder as the working directory.\n\n\n\nYou will use the Quarto document you created to write text and code for this tutorial.\nDelete everything in the Quarto document below the YAML header. It’s just example text—we’re not attached to it!\nThis is what the Quarto document should look like now (if your YAML header also contains “format:html, that’s completely fine):\n\n\n\n\n\nNow add an empty line and in the following line write a second-level heading ## Attach packages, followed by two empty lines. Like so:\n\n\n\n\n\nNow we can insert a code chunk to add the code to attach the tidyverse. To insert a new code chunk, you can click on the Insert a new code chunk button (the little green square icon with a C and a plus) , or you can press OPT+CMD+I/ALT+CTRL+I.\n\n\n\n\n\nA new R code chunk will be inserted at the text cursor position.\nNow go ahead and add the following lines of code inside the R code chunk.\n#| label: setup\n\nlibrary(tidyverse)\n\n\n\n\n\n\nRunning code in Quarto documents\n\n\n\nTo run the code, you have two options:\n\nYou click the small green triangle in the top-right corner of the chunk. This runs all the code in the code chunk.\nEnsure the text cursor is inside the code chunk and press SHIFT+CMD+ENTER/SHIFT+CTRL+ENTER. This too runs all the code in the code chunk.\n\nIf you want to run line by line in the code chunk, you can place the text cursor on the line you want to run and press CMD+ENTER/CTRL+ENTER. The current line is run and the text cursor is moved to the next line. Just like in the .R scripts that we’ve been using in past weeks.\n\n\nRun the setup chunk now.\n\n\n\n\n\nYou will see messages printed below the code chunk, in your Quarto file (don’t worry about the Conflicts, they just tell you that some functions from the tidyverse packages have replaced the base R functions, which is OK).\n\n\n\n\n\n\nPractice 1\n\n\n\nTry this yourself:\n\nCreate a new second-level heading (with ##) called Read data.\nCreate a new R code chunk.\nSet the label of the chunk to read-data.\nAdd code to read the following files (hint: think about where these files are located relative to the working directory, that is, the project folder). Assign the datasets to the variable names polite and glot_status respectively.\n\nwinter2012/polite.csv\ncoretta2022/glot_status.rds\n\nRun the code.\n\n\n\n\n\n\nYou can render a .qmd file into a nicely formatted HTML file.\nTo render a Quarto file, just click on the Render button and an HTML file will be created and saved in the same location of the Quarto file.\n\n\n\n\n\nIt may be shown in the Viewer pane (like in the picture below) or in a new browser window. There are a few ways you can set this option to whichever version you prefer. Follow the instructions that work for you—they all do the same thing.\n\nTools &gt; Global Options &gt; R Markdown &gt; Show output preview in…\nPreferences &gt; R Markdown &gt; Basics &gt; Show output preview in….\nRight beside the Render button, you will see a little white gear. Click on that gear, and a drop-down menu will open. Click on Preview in Window or Preview in Viewer Pane, whichever you prefer.\n\n\n\n\n\n\nRendering Quarto files is not restricted to HTML, but also PDFs and even Word documents!\nThis is very handy when you are writing an analysis report you need to share with others.\n\n\n\n\n\n\nQuarto: Rendering\n\n\n\nQuarto files can be rendered into other formats, like HTML, PDF and Word documents.\n\n\nThe assessments of this course will require you to write text and code in a Quarto file and render it to HTML.\nYou could even write your dissertation in Quarto!\nThe following sections will introduce you to the basics of plotting data. You will keep learning how to create plots throughout the course."
  },
  {
    "objectID": "tutorials/tutorial-w04.html#quarto",
    "href": "tutorials/tutorial-w04.html#quarto",
    "title": "DAL tutorial - Week 4",
    "section": "",
    "text": "Last week, you learnt how to use R scripts to save your code.\nKeeping track of the code you use for data analysis is a very important aspect of research project managing: not only the code is there if you need to rerun it later, but it allows your data analysis to be reproducible (i.e., it can be reproduced by you or other people in such a way that starting with the same data and code you get to the same results).\n\n\n\n\n\n\nReproducible research\n\n\n\nResearch is reproducible when the same data and same code return the same results.\nSee the Definitions page of The Turing Way for definitions of reproducible, replicable, robust and generalisable research.\n\n\nR scripts are great for writing code, and you can even document the code (add explanations or notes) with comments (i.e. lines that start with #).\nBut for longer text or complex data analysis reports, R scripts can be a bit cumbersome.\nA solution to this is using Quarto files (they have the .qmd extension).\n\n\n\n\n\n\nQuiz 1\n\n\n\nWhen is research not reproducible?\n\n a. When the results do not match the researcher's expectations. b. When the the same data and code as in the original study do not produce the published results. c. When research conducted by a different research team with new data does not produce the results as published in the original study. \n\n\n\n\n\n\n\nHint\n\n\n\n\n\nResearch is reproducible when you can produce the same results using the original data and code/methods.\nResearch is replicable when you can produce the same results using new data and the original code/methods.\nSee https://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html#table-of-definitions-for-reproducibility.\n\n\n\n\n\n\n\nQuarto is a file format that allows you to mix code and formatted text in the same file.\nThis means that you can write dynamic reports using Quarto files: dynamic reports are just like analysis reports (i.e. they include formatted text, plots, tables, code output, code, etc…) but they are dynamic in the sense that if, for example, data or code changes, you can just rerun the report file and all code output (plots, tables, etc…) is updated accordingly!\n\n\n\n\n\n\nDynamic reports in Quarto\n\n\n\nQuarto is a file type with extension .qmd in which you can write formatted text and code together.\nQuarto can be used to generate dynamic reports: these are files that are generated automatically from the file source, ensuring data and results in the report are always up to date.\n\n\n\n\n\nR comments in R scripts cannot be formatted (for example, you can’t make text bold or italic).\nText in Quarto files can be fully formatted using a simple but powerful mark-up language called markdown.\nYou don’t have to learn markdown all in one go, so I encourage you to just learn it bit by bit, at your pace. You can look at the the Markdown Guide for an in-depth intro and/or dive in the Markdown Tutorial for a hands-on approach.\nA few quick pointers (you can test them in the Markdown Live Preview):\n\nText can be made italics by enclosing it between single stars: *this text is in italics*.\nYou can make text bold with two stars: **this text is bold!**.\nHeadings are created with #:\n\n# This is a level-1 heading\n\n## This is a level-2 heading\n\n\n\n\n\n\nMark-up, Markdown\n\n\n\nA mark-up language is a text-formatting system consisting of symbols or keywords that control the structure, formatting or relationships of textual elements. The most common mark-up languages are HTML, XML and TeX.\nMarkdown is a simple yet powerful mark-up language.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen working through these tutorials, always make sure you are in the course Quarto Project you created before.\nYou know you are in a Quarto Project because you can see the name of the Project in the top-right corner of RStudio, next to the light-blue cube icon.\nIf you see Project (none) in the top-right corner, that means you are not in the Quarto Project.\nTo make sure you are in the Quarto project, you can open the project by going to the project folder in File Explorer (Windows) or Finder (macOS) and double click on the .Rproj file.\n\n\nTo create a new .qmd file, just click on the New file button (the white square with the green plus symbol), then Quarto Document.... (If you are asked to install/update packages, do so.)\n\n\n\n\n\nA window will open. Add a title of your choice and your name. Make sure the Use visual markdown editor is NOT ticked, then click Create (you will be free to use the visual editor later, but it is important that you first see what a Quarto document looks like under the hood first).\n\n\n\n\n\nA new .qmd file will be created and will open in the File Editor panel in RStudio.\nNote that creating a Quarto file does not automatically save it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\n\n\n\n\n\nSave the file inside the code/ folder with the following name: tutorial-w04.qmd.\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer.\n\n\n\nA Quarto file usually has three main parts:\n\nThe YAML header (green in the screenshot below).\nCode chunks (red).\nText (blue).\n\n\n\n\n\n\nEach Quarto file has to start with a YAML header, but you can include as many code chunks and as much text as you wish, in any order.\n\n\n\n\n\n\nQuarto: YAML header\n\n\n\nThe header of a .qmd file contains a list of key: value pairs, used to specify settings or document info like the title and author.\nYAML headers start and end with three dashes ---.\n\n\n\n\n\n\n\n\nQuarto: Code chunks\n\n\n\nCode chunks start and end with three back-ticks ``` and they contain code.\n{r} indicates that the code is R code. Settings can be specified inside the chunk with the #| prefix: for example #| label: setup sets the name of the chunk (the label) to setup.\n\n\n\n\n\nWhen using Quarto projects, the working directory (the directory all relative paths are relative to) is the project folder.\nHowever, when running code from a Quarto file, the code is run as if the working directory were the folder in which the file is saved.\nThis isn’t an issue if the Quarto file is directly in the project folder, but in our case our Quarto files live in the code/ folder within the project folder (and it is good practice to do so!).\nWe can instruct R to always run code from the project folder (i.e. the working directory is the project folder). This is when the _quarto.yml file comes into play.\nOpen the _quarto.yml file in RStudio (you can simply click on the file in the Files tab and that will open the file in the RStudio editor). Add the line execute-dir: project under the title. Note that indentation should be respected, so the line you write should align with title:, not with project:.\nproject:\n  title: \"dal\"\n  execute-dir: project\nNow, all code in Quarto files, no matter where they are saved, will be run with the project folder as the working directory.\n\n\n\nYou will use the Quarto document you created to write text and code for this tutorial.\nDelete everything in the Quarto document below the YAML header. It’s just example text—we’re not attached to it!\nThis is what the Quarto document should look like now (if your YAML header also contains “format:html, that’s completely fine):\n\n\n\n\n\nNow add an empty line and in the following line write a second-level heading ## Attach packages, followed by two empty lines. Like so:\n\n\n\n\n\nNow we can insert a code chunk to add the code to attach the tidyverse. To insert a new code chunk, you can click on the Insert a new code chunk button (the little green square icon with a C and a plus) , or you can press OPT+CMD+I/ALT+CTRL+I.\n\n\n\n\n\nA new R code chunk will be inserted at the text cursor position.\nNow go ahead and add the following lines of code inside the R code chunk.\n#| label: setup\n\nlibrary(tidyverse)\n\n\n\n\n\n\nRunning code in Quarto documents\n\n\n\nTo run the code, you have two options:\n\nYou click the small green triangle in the top-right corner of the chunk. This runs all the code in the code chunk.\nEnsure the text cursor is inside the code chunk and press SHIFT+CMD+ENTER/SHIFT+CTRL+ENTER. This too runs all the code in the code chunk.\n\nIf you want to run line by line in the code chunk, you can place the text cursor on the line you want to run and press CMD+ENTER/CTRL+ENTER. The current line is run and the text cursor is moved to the next line. Just like in the .R scripts that we’ve been using in past weeks.\n\n\nRun the setup chunk now.\n\n\n\n\n\nYou will see messages printed below the code chunk, in your Quarto file (don’t worry about the Conflicts, they just tell you that some functions from the tidyverse packages have replaced the base R functions, which is OK).\n\n\n\n\n\n\nPractice 1\n\n\n\nTry this yourself:\n\nCreate a new second-level heading (with ##) called Read data.\nCreate a new R code chunk.\nSet the label of the chunk to read-data.\nAdd code to read the following files (hint: think about where these files are located relative to the working directory, that is, the project folder). Assign the datasets to the variable names polite and glot_status respectively.\n\nwinter2012/polite.csv\ncoretta2022/glot_status.rds\n\nRun the code.\n\n\n\n\n\n\nYou can render a .qmd file into a nicely formatted HTML file.\nTo render a Quarto file, just click on the Render button and an HTML file will be created and saved in the same location of the Quarto file.\n\n\n\n\n\nIt may be shown in the Viewer pane (like in the picture below) or in a new browser window. There are a few ways you can set this option to whichever version you prefer. Follow the instructions that work for you—they all do the same thing.\n\nTools &gt; Global Options &gt; R Markdown &gt; Show output preview in…\nPreferences &gt; R Markdown &gt; Basics &gt; Show output preview in….\nRight beside the Render button, you will see a little white gear. Click on that gear, and a drop-down menu will open. Click on Preview in Window or Preview in Viewer Pane, whichever you prefer.\n\n\n\n\n\n\nRendering Quarto files is not restricted to HTML, but also PDFs and even Word documents!\nThis is very handy when you are writing an analysis report you need to share with others.\n\n\n\n\n\n\nQuarto: Rendering\n\n\n\nQuarto files can be rendered into other formats, like HTML, PDF and Word documents.\n\n\nThe assessments of this course will require you to write text and code in a Quarto file and render it to HTML.\nYou could even write your dissertation in Quarto!\nThe following sections will introduce you to the basics of plotting data. You will keep learning how to create plots throughout the course."
  },
  {
    "objectID": "tutorials/tutorial-w04.html#plotting-basics",
    "href": "tutorials/tutorial-w04.html#plotting-basics",
    "title": "DAL tutorial - Week 4",
    "section": "2 Plotting basics",
    "text": "2 Plotting basics\nPlotting data in R is easy once you understand the basics.\nCreate a new second-level header in your Quarto document called ## Making plots. Everything else in this tutorial will go into this section.\n\n2.1 Graphic systems\nIn R, you can create plots using different systems.\n\nBase R.\nlattice.\nggplot2.\nmore…\n\nIn this course you will learn how to use the ggplot2 system, but before we dive in, let’s have a look at the base R plotting system too.\n\n\n2.2 Base R plotting function\nLet’s create two vectors, x and y and plot them. For now, run the following code in the Console (not in the Quarto document).\n\nx &lt;- 1:10\ny &lt;- x^3\n\nplot(x, y)\n\n\n\n\n\n\n\n\nEasy!\nNow let’s add a few more things.\n\nplot(x, y, type = \"l\", col = \"purple\", lwd = 3, lty = \"dashed\")\n\n\n\n\n\n\n\n\nWith plots as simple as this one, the base R plotting system is sufficient, but to create more complex plots (which is virtually always the case), base R gets incredibly complicated.\nInstead we can use the tidyverse package ggplot2. ggplot2 works well with the other tidyverse packages and it follows the same principles, so it is convenient to use it for data visualisation instead of base R!"
  },
  {
    "objectID": "tutorials/tutorial-w04.html#your-first-ggplot2-plot",
    "href": "tutorials/tutorial-w04.html#your-first-ggplot2-plot",
    "title": "DAL tutorial - Week 4",
    "section": "3 Your first ggplot2 plot",
    "text": "3 Your first ggplot2 plot\nThe tidyverse package ggplot2 provides users with a consistent set of functions to create captivating graphics.\n\n\n\n\n\n\nWarning\n\n\n\nTo be able to use the functions in a package, you first need to attach the package. We have already attached the library(tidyverse) packages, among which there is ggplot2, so you don’t need to do anything else.\n\n\nWe will first use the polite data to learn the basics of plotting using ggplot (remember you read this data in before in Practice 1?).\nIn this tutorial we will use the following columns:\n\nf0mn: mean f0 (fundamental frequency).\nH1H2: difference between H2 and H1 (second and first harmonic). A higher H1-H2 difference indicates greater breathiness.\n\n\n3.1 A basic plot\nThese are the minimum constituents of a ggplot2 plot.\n\n\n\n\n\n\nggplot basics\n\n\n\n\nThe data: you have to specify the data frame with the data you want to plot.\nThe mapping: the mapping tells ggplot how to map data columns to parts of the plot like the axes or groupings within the data. (For example, which variable is shown on the x axis, and which one is on the y axis? If data comes from two different groups, should each group get its own colour?) These different parts of the plot are called aesthetics, or aes for short.\n\n\n\nYou can specify the data and mapping with the data and mapping arguments of the ggplot() function.\nNote that the mapping argument is always specified with aes(): mapping = aes(…).\nIn the following bare plot, we are just mapping f0mn to the x-axis and H1H2 to the y-axis, from the polite data frame.\nCreate a new code chunk, copy the following code and run it. From this point on I will assume you’ll create a new code chunk and run the code yourself, without explicit instructions.\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n)\n\n\n\n\n\n\n\n\nNot much to see here: just two axes! So where’s the data? Don’t worry, we didn’t do anything wrong. Showing the data itself requires a further step, which we’ll turn to next.\n\n\n\n\n\n\nQuiz 2\n\n\n\nIs the following code correct? Justify your answer. TRUEFALSE\nggplot(\n  data = polite,\n  mapping = c(x = total_duration, y = articulation_rate)\n)\n\n\n\n\n3.2 Let’s add geometries\nOur code so far makes nice axes, but we are missing the most important part: showing the data!\nData is represented with geometries, or geoms for short. geoms are added to the base ggplot with functions whose names all start with geom_.\n\n\n\n\n\n\nGeometries\n\n\n\nGeometries are plot elements that show the data through geometric shapes.\nDifferent geometries are added to a ggplot using one of the geom_*() functions.\n\n\nFor this plot, you want to use geom_point(). This geom simply adds point to the plot based on the data in the polite data frame.\nTo add geoms to a plot, you write a + at the end of the ggplot() command and include the geom on the next line. For example:\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 1: Scatter plot of mean f0 and H1-H2 difference.\n\n\n\n\n\nThis type of plot, with two continuous axes and data represented by points, is called a scatter plot.\n\n\n\n\n\n\nScatter plot\n\n\n\nA scatter plot is a plot with two numeric axes and points indicating the data. It is used when you want to show the relationship between two numeric variables.\nTo create a scatter plot, use the geom_point() geometry.\n\n\nWhen writing your results section, you could describe the plot this way:\n\nFigure 1 shows a scatter plot of mean f0 on the x-axis and H1-H2 difference on the y-axis. The plot suggest an overall negative relationship between mean f0 and H1-H2 difference. In other words, increasing mean f0 corresponds to decreasing breathiness.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that using the + is a quirk of ggplot(). The idea behind it is that you start from a bare plot and you add (+) layers of data on top of it. This is because of the philosophy behind the package, called the Layered Grammar of Graphics. In fact, Grammar of Graphics is where you get the GG in ggplot!\n\n\n\n\n3.3 Function arguments\nNote that the data and mapping arguments don’t have to be named explicitly (with data = and mapping =) in the ggplot() function, since they are obligatory and they are specified in that order.\n\nggplot(\n  polite,\n  aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nIn fact, you can also leave out x = and y =.\n\nggplot(\n  polite,\n  aes(f0mn, H1H2)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nTry running ?ggplot in the Console to see the arguments of the function and the order they appear in.\n\n\n\n\n\n\nQuiz 3\n\n\n\nWhich of the following will produce the same plot as the one above? Reason through it first without running the code, then run all of these to check whether they look the way you expected.\n\n ggplot(polite, aes(H1H2, f0mn)) + geom_point() ggplot(polite, aes(y = H1H2, x = f0mn)) + geom_point() ggplot(polite, aes(y = f0mn, x = H1H2)) + geom_point()\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen specifying arguments, the order matters when not using the argument names.\nSo aes(a, b) is different from aes(b, a).\nBut aes(y = b, x = a) is the same as aes(a, b).\n\n\n\n\n\n\n\n3.4 What the pipe?!\nThe code of the latest plot can also be written this way.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2)) +\n    geom_point()\n\nWait, what is that thing, |&gt;?\nIt’s called a pipe. Think of a pipe as a teleporter.\nThe pipe |&gt; teleports the data polite into the following function as the first argument. So polite |&gt; ggplot() is equivalent to ggplot(polite).\nFor now it might not make much sense using the pipe, but you will learn next week how to chain many functions one after the other using the pipe, at which point its usefulness will be more obvious.\nAs a sneak peek, you will be able to filter the data before plotting it, like so:\n\npolite |&gt;\n  # include only rows where f0mn &lt; 300\n  filter(f0mn &lt; 300) |&gt;\n  ggplot(aes(f0mn, H1H2)) +\n    geom_point()"
  },
  {
    "objectID": "tutorials/tutorial-w04.html#changing-aesthetics",
    "href": "tutorials/tutorial-w04.html#changing-aesthetics",
    "title": "DAL tutorial - Week 4",
    "section": "4 Changing aesthetics",
    "text": "4 Changing aesthetics\n\n4.1 colour aesthetic\nYou might notice that there seems to be two subgroups within the data: one below about 200 Hz and one above about it.\nIn fact, these subgroups are related to gender. Let’s colour the points by gender then.\nYou can use the colour aesthetic to colour the points by gender, like so:\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 2: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\n\n\n\n\n\n\ncolour or color?\n\n\n\nTo make ggplot easy for users of different Englishes, it’s possible to write the colour aesthetic either as the British-style colour or the American-style color! Both will get the job done.\n\n\nNotice how colour = gender must be inside the aes() function, because we are trying to map colour to the values of the column gender. Colours are automatically assigned to each level in gender.\nThe default colour palette is used, but you can customise it. You will learn later in the course how to create custom palettes, but you can quickly change palette using one of the scale_colour_*() functions.\nA useful function is the scale_colour_brewer() function. This function creates palettes based on ColorBrewer 2.0. There are three types of palettes (see the linked website for examples):\n\nSequential (seq): a gradient sequence of hues from lighter to darker.\nDiverging (div): useful when you need a neutral middle colour and sequential colours on either side of the neutral colour.\nQualitative (qual): useful for categorical variables.\n\nLet’s use the default qualitative palette.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  scale_color_brewer(type = \"qual\")\n\n\n\n\n\n\n\nFigure 3: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\nNow try changing the palette argument of the scale_colour_brewer() function to different palettes. (Check the function documentation for a list).\nAnother set of palettes is provided by scale_colour_viridis_d() (the d stands for “discrete” palette, to be used for categorical variables). Here’s an example.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  scale_color_viridis_d(option = \"B\")\n\n\n\n\n\n\n\nFigure 4: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\n\n\n\n\n\n\nExtra: The default colour palette\n\n\n\n\n\nIf you want to know more about the default colour palette, check this blog post out.\n\n\n\n\n\n4.2 alpha aesthetic\nAnother useful ggplot2 aesthetic is alpha. This aesthetic sets the transparency of the geometry: 0 means completely transparent and 1 means completely opaque.\nChange alpha in the code below to 0.5.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point(alpha = ...) +\n  scale_color_brewer(type = \"qual\")\n\nWhen you are setting a value yourself that should apply to all instances of some geometry, rather than mapping an aesthetic to values in a specific column (like we did above with colour), you should add the aesthetic outside of aes() and usually in the geom function you want to modify.\nSetting a lower alpha is useful when there are a lot of points or other geometries that overlap with each other and it just looks like a blob of colour (you can’t really see the individual geometries; you will see an example next week). It is not the case here, and in fact reducing the alpha makes the plot quite illegible!"
  },
  {
    "objectID": "tutorials/tutorial-w04.html#labels",
    "href": "tutorials/tutorial-w04.html#labels",
    "title": "DAL tutorial - Week 4",
    "section": "5 Labels",
    "text": "5 Labels\nIf you want to change the labels of the axes and the legend, you can use the labs() function, like this.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  labs(\n    x = \"Mean f0 (Hz)\",\n    y = \"H1-H2 difference (dB)\",\n    colour = \"Gender\"\n  )\n\n\n\n\n\n\n\nFigure 5: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\nAlso add a title and a subtitle (use these two arguments within the labs() function).\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor example, labs(title = \"...\", ...)."
  },
  {
    "objectID": "tutorials/tutorial-w04.html#render-your-quarto-file",
    "href": "tutorials/tutorial-w04.html#render-your-quarto-file",
    "title": "DAL tutorial - Week 4",
    "section": "6 Render your Quarto file!",
    "text": "6 Render your Quarto file!\nNow that you have done all of this hard work, why don’t you try and render the Quarto file you’ve been working on to an HTML file?\nGo ahead, click on the “Render” button and if everything works fine you should see a rendered HTML file in a second!\nNote that you will be asked to render your Quarto files for the assessments, so I recommend you try this out now."
  },
  {
    "objectID": "tutorials/tutorial-w04.html#summary",
    "href": "tutorials/tutorial-w04.html#summary",
    "title": "DAL tutorial - Week 4",
    "section": "7 Summary",
    "text": "7 Summary\nThat’s all for this week!\n\n\n\n\n\n\nQuarto\n\nQuarto files can be used to create dynamic and reproducible reports.\nMark-up languages are text-formatting systems that specify text formatting and structure using symbols or keywords. Markdown is the mark-up language that is used in Quarto documents.\nThe main parts of a .qmd file are the YAML header, text and code chunks.\n\nPlotting\n\nggplot2 is a plotting package from the tidyverse.\n\nTo create a basic plot, you use the ggplot() function and specify data and mapping.\nThe aes() function allows you to specify aesthetics (like axes, colours, …) in the mapping argument.\nGeometries map data values onto shapes in the plot. All geometry functions are of the type geom_*().\n\nScatter plots are created with geom_point() and can be used with two numeric variables."
  },
  {
    "objectID": "tutorials/tutorial-w06.html",
    "href": "tutorials/tutorial-w06.html",
    "title": "DAL tutorial - Week 6",
    "section": "",
    "text": "During the lecture, we have learnt two types of measures.\n\n\n\n\n\n\nSummary measures\n\n\n\nMeasures of central tendency (mean, median, mode) indicate the typical or central value of a sample.\nMeasures of dispersion (min-max, range, standard deviation) indicate the dispersion of the sample values around the central tendency value.\n\n\nWhen you work with data, you always want to get summary measures for most of the variables in the data.\nData reports usually include summary measures. It is also important to understand which summary measure is appropriate for which type of variable.\nWe have covered this in the lecture, so we won’t go over it again here. Instead, you will learn how to obtain summary measures using the summarise() function from the dplyr tidyverse package.\nsummarise() takes at least two arguments:\n\nThe data frame to summarise.\nOne or more summary functions.\n\nFor example, let’s get the mean the reaction time column RT. Easy! (First attach the tidyverse and read the song2020/shallow.csv file into a variable called shallow.)\n\nsummarise(shallow, RT_mean = mean(RT))\n\n\n  \n\n\n\nGreat! The mean reaction times of the entire sample is 867.3592 ms.\nYou can round numbers with the round() function. For example:\n\nnum &lt;- 867.3592\nround(num)\n\n[1] 867\n\nround(num, 1)\n\n[1] 867.4\n\nround(num, 2)\n\n[1] 867.36\n\n\nThe second argument sets the number of decimals to round to (by default, it is 0, so the number is rounded to the nearest integer, that is, to the nearest whole number with no decimal values).\nLet’s recalculate the mean by rounding it this time.\n\nsummarise(shallow, RT_mean = round(mean(RT)))\n\n\n  \n\n\n\nWhat if we want also the standard deviation? Easy: we use the sd() function. (Round the mean and SD with the round() function in your code).\n\n# round the mean and SD\nsummarise(shallow, RT_mean = mean(RT), RT_sd = sd(RT))\n\n\n  \n\n\n\nNow we know that reaction times are on average 867 ms long and have a standard deviation of about 293 ms (rounded to the nearest integer).\nLet’s go all the way and also get the minimum and maximum RT values with the min() and max() functions (round all the summary measures).\n\nsummarise(\n  shallow,\n  RT_mean = mean(RT), RT_sd = sd(RT),\n  RT_min = ..., RT_max = ...\n)\n\nFab! When writing a data report, you could write something like this.\n\nReaction times are on average 867 ms long (SD = 293 ms), with values ranging from 0 to 1994 ms.\n\nWe won’t go into the details of what standard deviations are, but you can just think of them as a relative measure of how dispersed the data are around the mean: the higher the SD, the greater the dispersion around the mean, i.e. the greater the variability in the data.\nWhen required, you can use the median() function to calculate the median, instead of the mean(). Go ahead and calculate the median reaction times in the data. Is it similar to the mean?\n\n\nMost base R functions behave unexpectedly if the vector they are used on contain NA values.\nNA is a special object in R, that indicates that a value is Not Available, meaning that that observation does not have a value.\nFor example, in the following numeric vector, there are 5 objects:\n\na &lt;- c(3, 5, 3, NA, 4)\n\nFour are numbers and one is NA.\nIf you calculate the mean of a with mean() something strange happens.\n\nmean(a)\n\n[1] NA\n\n\nThe functions returns NA.\nThis is because by default when just one value in the vector is NA then operations on the vector will return NA.\n\nmean(a)\n\n[1] NA\n\nsum(a)\n\n[1] NA\n\nsd(a)\n\n[1] NA\n\n\nIf you want to discard the NA values when operating on a vector that contains them, you have to set the na.rm (for “NA remove”) argument to TRUE.\n\nmean(a, na.rm = TRUE)\n\n[1] 3.75\n\nsum(a, na.rm = TRUE)\n\n[1] 15\n\nsd(a, na.rm = TRUE)\n\n[1] 0.9574271\n\n\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nWhat does the na.rm argument of mean() do?\n\n It changes NAs to FALSE. It converts NAs to 0s. It removes NAs before taking the mean.\n\nWhich is the mean of c(4, 23, NA, 5) when na.rm has the default value?\n\n NA. 0. 10.66.\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCheck the documentation of ?mean.\n\n\n\n\n\nNote that R has a mode() function, but alas this is not the statistical mode. To get the mode of a categorical variable you can just count the occurrences of the values of that variable and the value that occurs the most is the mode!\nYou will learn how to count occurrences below. But first, let’s see what density plots are!"
  },
  {
    "objectID": "tutorials/tutorial-w06.html#summary-measures",
    "href": "tutorials/tutorial-w06.html#summary-measures",
    "title": "DAL tutorial - Week 6",
    "section": "",
    "text": "During the lecture, we have learnt two types of measures.\n\n\n\n\n\n\nSummary measures\n\n\n\nMeasures of central tendency (mean, median, mode) indicate the typical or central value of a sample.\nMeasures of dispersion (min-max, range, standard deviation) indicate the dispersion of the sample values around the central tendency value.\n\n\nWhen you work with data, you always want to get summary measures for most of the variables in the data.\nData reports usually include summary measures. It is also important to understand which summary measure is appropriate for which type of variable.\nWe have covered this in the lecture, so we won’t go over it again here. Instead, you will learn how to obtain summary measures using the summarise() function from the dplyr tidyverse package.\nsummarise() takes at least two arguments:\n\nThe data frame to summarise.\nOne or more summary functions.\n\nFor example, let’s get the mean the reaction time column RT. Easy! (First attach the tidyverse and read the song2020/shallow.csv file into a variable called shallow.)\n\nsummarise(shallow, RT_mean = mean(RT))\n\n\n  \n\n\n\nGreat! The mean reaction times of the entire sample is 867.3592 ms.\nYou can round numbers with the round() function. For example:\n\nnum &lt;- 867.3592\nround(num)\n\n[1] 867\n\nround(num, 1)\n\n[1] 867.4\n\nround(num, 2)\n\n[1] 867.36\n\n\nThe second argument sets the number of decimals to round to (by default, it is 0, so the number is rounded to the nearest integer, that is, to the nearest whole number with no decimal values).\nLet’s recalculate the mean by rounding it this time.\n\nsummarise(shallow, RT_mean = round(mean(RT)))\n\n\n  \n\n\n\nWhat if we want also the standard deviation? Easy: we use the sd() function. (Round the mean and SD with the round() function in your code).\n\n# round the mean and SD\nsummarise(shallow, RT_mean = mean(RT), RT_sd = sd(RT))\n\n\n  \n\n\n\nNow we know that reaction times are on average 867 ms long and have a standard deviation of about 293 ms (rounded to the nearest integer).\nLet’s go all the way and also get the minimum and maximum RT values with the min() and max() functions (round all the summary measures).\n\nsummarise(\n  shallow,\n  RT_mean = mean(RT), RT_sd = sd(RT),\n  RT_min = ..., RT_max = ...\n)\n\nFab! When writing a data report, you could write something like this.\n\nReaction times are on average 867 ms long (SD = 293 ms), with values ranging from 0 to 1994 ms.\n\nWe won’t go into the details of what standard deviations are, but you can just think of them as a relative measure of how dispersed the data are around the mean: the higher the SD, the greater the dispersion around the mean, i.e. the greater the variability in the data.\nWhen required, you can use the median() function to calculate the median, instead of the mean(). Go ahead and calculate the median reaction times in the data. Is it similar to the mean?\n\n\nMost base R functions behave unexpectedly if the vector they are used on contain NA values.\nNA is a special object in R, that indicates that a value is Not Available, meaning that that observation does not have a value.\nFor example, in the following numeric vector, there are 5 objects:\n\na &lt;- c(3, 5, 3, NA, 4)\n\nFour are numbers and one is NA.\nIf you calculate the mean of a with mean() something strange happens.\n\nmean(a)\n\n[1] NA\n\n\nThe functions returns NA.\nThis is because by default when just one value in the vector is NA then operations on the vector will return NA.\n\nmean(a)\n\n[1] NA\n\nsum(a)\n\n[1] NA\n\nsd(a)\n\n[1] NA\n\n\nIf you want to discard the NA values when operating on a vector that contains them, you have to set the na.rm (for “NA remove”) argument to TRUE.\n\nmean(a, na.rm = TRUE)\n\n[1] 3.75\n\nsum(a, na.rm = TRUE)\n\n[1] 15\n\nsd(a, na.rm = TRUE)\n\n[1] 0.9574271\n\n\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nWhat does the na.rm argument of mean() do?\n\n It changes NAs to FALSE. It converts NAs to 0s. It removes NAs before taking the mean.\n\nWhich is the mean of c(4, 23, NA, 5) when na.rm has the default value?\n\n NA. 0. 10.66.\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCheck the documentation of ?mean.\n\n\n\n\n\nNote that R has a mode() function, but alas this is not the statistical mode. To get the mode of a categorical variable you can just count the occurrences of the values of that variable and the value that occurs the most is the mode!\nYou will learn how to count occurrences below. But first, let’s see what density plots are!"
  },
  {
    "objectID": "tutorials/tutorial-w06.html#density-plots",
    "href": "tutorials/tutorial-w06.html#density-plots",
    "title": "DAL tutorial - Week 6",
    "section": "2 Density plots",
    "text": "2 Density plots\n\n\n\n\n\n\nDensity plots\n\n\n\nDensity plots show the distribution (i.e. the “probability density”) of the values of a continuous variable.\nThey are created with geom_density().\n\n\nReaction times is a numeric continuous variable so density plots are appropriate.\nTo plot the probability density of a continuous variable, you can use the density geometry.\n\nshallow |&gt;\n  ggplot(aes(x = RT)) +\n  geom_density()\n\n\n\n\n\n\n\n\nThe black solid curve in the plot indicates the density of the data (the y-axis) along the values of RT (the x-axis).\nThe higher the point of the curve is on the y-axis (i.e. the higher the density), the more data there is at the corresponding x-axis value.\nFor example, the highest point in the curve is at around 750 ms (the white vertical line between 500 and 1000 is 750).\nThis means that around 750 ms there are many observations.\nOn the other hand, if you look at the curve to the left of 500 ms RT and above 1500 ms RT, the height of the points forming the curve are much lower and in some cases they even go to 0 density (y-axis).\nNote that to create a density plot, you only need to specify the x-axis. The y-axis is the probability density, which is automatically calculated (a bit like counts in bar charts, remember?).\n\n2.1 Make things cosy with a rug\nThe density line shows you a smoothed representation of the data distribution over RT values, but you might also want to see the raw data represented on the xsxis.\nYou can do so by adding the rug geometry. Go ahead and add a rug…\n\nshallow |&gt;\n  ggplot(aes(RT)) +\n  geom_density() +\n  ...\n\nYou should get the following:\n\n\n\n\n\n\n\n\n\nNice huh? You can also change the opacity of the ticks of the rug to have a better sense of how many ticks there are at certain values on the x-axis.\nOpacity of geometries can be adjusted with the alpha argument: 0 means completely transparent and 1 means completely opaque.\nLet’s set the alpha of the rug geometry to 0.1.\n\n\n\n\n\n\n\n\n\nCan you see how the blackest parts of the rug correspond to the higher parts of the density curve?\n\n\n\n\n\n\nRug\n\n\n\nRaw data can be shown with a rug, i.e. ticks on the axes that mark where the data is.\nYou can add a rug with geom_rug().\n\n\n\n\n\n\n\n\nQuiz 2\n\n\n\nWhat can you notice about the distribution of RT values?\n\n\n\n\n\n\nHint\n\n\n\n\n\nIs the distribution symmetric around the highest density point?\n\n\n\n\n\nKeep reading to learn how to count occurrences."
  },
  {
    "objectID": "tutorials/tutorial-w06.html#count-occurrences",
    "href": "tutorials/tutorial-w06.html#count-occurrences",
    "title": "DAL tutorial - Week 6",
    "section": "3 Count occurrences",
    "text": "3 Count occurrences\nOften, you need to count occurrences in the data frame based on the values of specific columns.\nFor example, let’s count the number of correct and incorrect trials in the shallow data frame.\nThe column ACC tells us whether a trial is incorrect 0 or correct 1. (We will see how this way of coding binary variables, with 0s and 1s is not an ideal, although very common way, of coding binary variables. For now let’s keep it as is.)\nWe can use the count() function from the dplyr tidyverse package to count the number of occurrences for each value of a specific column.\nThe function count() takes the name of a tibble and the name of the column you want to count values in.\n\ncount(shallow, ACC)\n\n\n  \n\n\n# You can also write that as\n# shallow |&gt; count(ACC)\n\nHow many correct trials are there in the shallow tibble? And how many incorrect trials?\nNote that you can add multiple column names, separated by commas, to get counts for the combinations of values of each column.\nTry to get counts of the combination of ACC and Group (L1 vs L2 participants). Replace ... with the right code. Mess around and find out what works—you can’t break anything!—or check the documentation of count().\n\ncount(shallow, ...)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn count(), include the names of the two columns you want to get counts of, separated by commas.\n\n\n\nThis is the output:\n\n\n\n  \n\n\n\nAre there differences in accuracy between the L1 and L2 group? It’s difficult to say just by looking at those numbers, because the total number of trials for L1 and L2 participants is different.\n\n\n\n  \n\n\n\nThere are more L2 trials than L1 trials in the data set.\nWhen the total number of observations is not the same in the groups we are trying to compare, we can calculate the proportion instead of the raw count.\nThis should ring a bell. Do you remember position = \"fill\" in bar charts? This is based on the same reasoning.\nWe can calculate the proportion of correct and incorrect trials using a chain of functions.\n\nshallow |&gt;\n  add_count(Group, name = \"tot\") |&gt;\n  count(Group, ACC, tot) |&gt;\n  mutate(\n    prop = round(n / tot, 2)\n  )\n\n\n  \n\n\n\nTo learn what each line does, you can split the chain in multiple steps and inspect each step.\n\nshallow_tot &lt;- shallow |&gt;\n  add_count(Group, name = \"tot\")\n\nshallow_count &lt;- shallow_tot |&gt;\n  count(Group, ACC, tot) \n\nshallow_prop &lt;- shallow_count |&gt;\n  mutate(\n    # round proportion to 2 decimals.\n    prop = round(n / tot, 2)\n  )\n\nNow check shallow_tot, shallow_count and shallow_prop.\nBased on the proportion of correct trials in the L1 and L2 group, are there substantial differences in how the two groups performed? Or are they similar? If not, who was better?"
  },
  {
    "objectID": "tutorials/tutorial-w06.html#grouping-data",
    "href": "tutorials/tutorial-w06.html#grouping-data",
    "title": "DAL tutorial - Week 6",
    "section": "4 Grouping data",
    "text": "4 Grouping data\nSometimes you might want to get summary measures for one column depending on different values of another column.\nYou can use the group_by() function from the dplyr tidyverse package, together with summarise() to achieve that. Let’s see how it works.\n\ngroup_by(shallow, Group) |&gt;\n  summarise(\n    RT_mean = round(mean(RT)),\n    RT_sd = round(sd(RT))\n  )\n\n\n  \n\n\n\nThe group_by() function takes at least two arguments:\n\nThe name of the tibble you want to group.\nThe name of the columns you want to group the tibble by, separated by commas.\n\nHere we are grouping shallow by Group.\nIn fact, you can even use a pipe for the tibble of group_by() as we have done for other functions, like so:\n\nshallow |&gt;\n  group_by(Group) |&gt;\n    summarise(\n      RT_mean = round(mean(RT)),\n      RT_sd = round(sd(RT))\n    )\n\n\n  \n\n\n\n\n\n\n\n\n\nQuiz 3\n\n\n\nWhich of the following returns the number of words in shallow?\n\n count(shallow, Target). shallow |&gt; distinct(Target) |&gt; count(). shallow |&gt; count(Target)."
  },
  {
    "objectID": "tutorials/tutorial-w06.html#log-transformation",
    "href": "tutorials/tutorial-w06.html#log-transformation",
    "title": "DAL tutorial - Week 6",
    "section": "5 Log-transformation",
    "text": "5 Log-transformation\nLook again at the density plot of reaction times. Can you see the long tail of the density line on the right side of it?\nReaction times are numeric and continuous, but can only be positive! Because of this, usually the distribution of reaction times looks like the one in plot: a big lump on the left side and a long tail to the right.\n(Remember above, how the median of RT was smaller than the mean of RT? The long tail results in a higher mean, whereas the median is a better reflection of where the data is densest.)\nIt is common practice to transform variables that can only take positive values to reduce the asymmetry in the distribution.\nA common transformation is to calculate the logarithm of the values. You can calculate the logarithm (or log) of a number in R using the log() function. Calculating the logs of a variable is known as a log-transformation.\n\n\n\n\n\n\nLog-transformation\n\n\n\nYou log-transform values by taking the logarithm (or log) of the values with the log() function.\n\n\nLet’s log-transform the reaction times and plot them.\n\nshallow |&gt;\n  ggplot(aes(x = log(RT))) +\n  geom_density(fill = \"#800000\", alpha = 0.7) +\n  geom_rug(alpha = 0.1)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\nProbability distributions\n\n\n\n\n\nA probability distribution defines the distribution of probabilities along a range of values. Probability distributions can be visualised using density lines. There are different families of probability distributions, each with its own characteristic shape. Probability distributions can be summarised with a set of parameters (each distribution family has its own parameters).\nAn important probability distribution family is the Gaussian distribution (aka normal distribution). In practice, data generated by a Gaussian distribution are very rare in the world, and most linguistic data follows other distributions.\nA Gaussian distribution can be defined by specifying the value of a mean and a standard deviation. Check out this web app to learn more about probability and probability distributions.\nLog-transformation is normally applied to variables that follow the log-normal distribution. Logging a log-normal variable changes its properties so that it matches more the properties of a Gaussian distribution (that’s why it’s called log-normal: it becomes more Gaussian (aka normal) if you log it).\nProbability distributions are a very important concept for statistical modelling. Linear models are a very flexible tool to model all sorts of data with all sorts of distributions. If you want to learn more about linear models, check out Bodo Winter’s book, Statistics for Linguistics with R."
  },
  {
    "objectID": "tutorials/tutorial-w06.html#practice",
    "href": "tutorials/tutorial-w06.html#practice",
    "title": "DAL tutorial - Week 6",
    "section": "6 Practice",
    "text": "6 Practice\n\n\n\n\n\n\nPractice 1\n\n\n\n\nRead the cameron2020/gestures.csv file in R.\nCalculate the following:\n\nAppropriate measures of central tendency and dispersion for the count column (it contains the number of gestures performed by each child in different tasks).\nAppropriate measures of central tendency and dispersion for the count column grouped by months (the child’s age).\nTotal number of gestures from each child (dyad).\nNumber of children by background.\n\nWrite a short paragraph where you report the measures.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nTo calculate the total number of gestures by children, you need the sum() function.\nTo calculate the number of children by background, you need the distinct() function.\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHave you tried doing the exercise and couldn’t work it out?\nThe you can check the code solution here…\n\n\n\n\n\n\nCode\n\n\n\n\n\ngestures |&gt;\n  summarise(\n    count_med = median(count, na.rm = TRUE),\n    count_min = min(count, na.rm = TRUE),\n    count_max = max(count, na.rm = TRUE),\n    count_range = count_max - count_min\n  )\n\ngestures |&gt;\n  group_by(months) |&gt;\n  summarise(\n    count_med = median(count, na.rm = TRUE),\n    count_min = min(count, na.rm = TRUE),\n    count_max = max(count, na.rm = TRUE),\n    count_range = count_max - count_min\n  )\n\ngestures |&gt;\n  group_by(dyad) |&gt;\n  summarise(\n    count_tot = sum(count)\n  )\n\ngestures |&gt;\n  distinct(background, dyad) |&gt;\n  count(background)"
  },
  {
    "objectID": "tutorials/tutorial-w06.html#summary",
    "href": "tutorials/tutorial-w06.html#summary",
    "title": "DAL tutorial - Week 6",
    "section": "7 Summary",
    "text": "7 Summary\n\n\n\n\n\n\nData summaries\n\nsummarise() allows you to calculate measures of central tendency and dispersion (with mean(), median(), min() and max(), sd(), …).\ncount() lets you count the number of occurrences of levels in a categorical variable.\ngroup_by() allows you to group a tibble according to one or more variables.\n\nPlotting\n\ngeom_density() creates density plots of continuous variables.\ngeom_rug() adds raw data as ticks on the x-axis of density plots."
  },
  {
    "objectID": "tutorials/tutorial-w08.html",
    "href": "tutorials/tutorial-w08.html",
    "title": "DAL tutorial - Week 8",
    "section": "",
    "text": "In this tutorial, you can go through the different sections in any order you like and you can pick and choose which sections you want to work on. Note that you should complete Section 4 because it covers stat_summary() which you need for Exercise 1 of the first summative.\nThe tutors will be able to help you choose sections that might be useful based on your interests or your dissertation projects (whether you are working on it now or you will next year!).\nIn this tutorial, there will be less explicit instructions or explanations. Before starting, make sure you attach the necessary packages (like tidyverse).\nYou can find information on the data in the QM data website or by clicking the “Description” button in each section."
  },
  {
    "objectID": "tutorials/tutorial-w08.html#overview",
    "href": "tutorials/tutorial-w08.html#overview",
    "title": "DAL tutorial - Week 8",
    "section": "",
    "text": "In this tutorial, you can go through the different sections in any order you like and you can pick and choose which sections you want to work on. Note that you should complete Section 4 because it covers stat_summary() which you need for Exercise 1 of the first summative.\nThe tutors will be able to help you choose sections that might be useful based on your interests or your dissertation projects (whether you are working on it now or you will next year!).\nIn this tutorial, there will be less explicit instructions or explanations. Before starting, make sure you attach the necessary packages (like tidyverse).\nYou can find information on the data in the QM data website or by clicking the “Description” button in each section."
  },
  {
    "objectID": "tutorials/tutorial-w08.html#formant-data",
    "href": "tutorials/tutorial-w08.html#formant-data",
    "title": "DAL tutorial - Week 8",
    "section": "2 Formant data",
    "text": "2 Formant data\nDescription\n\n\nIn this section we will plot formant data from Italian.\n\n\n\n\n\n\nWhat to do if you don’t have coretta2018a/ in your data/ folder\n\n\n\n\n\nYou’ll need to download formants.rda from GitHub, create a folder called coretta2018a/, and move formants.rda into it.\n\nVisit this link.\nTo the right of the “Raw” button on the right-hand side of the page, click the button with the down arrow to download the “raw” file. Now formants.rda should be downloaded onto your computer.\nIn your data/ folder, create a new folder called coretta2018a/.\nMove formants.rda into data/coretta2018a/.\n\n\n\n\n\n# .rda are R data files and they can be read with `load()`\n# `load()` does not need to be assigned to avariable for the data to be added to the enviornment\nload(\"data/coretta2018a/formants.rda\")\n\nThe data contains F1 and F2 measurements at 3 points within each vowel. We want to plot the values at the mid-point of the vowel.\n\nformants |&gt; \n  ggplot(aes(f12, f22, colour = vowel)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nThis doesn’t look right though… Shouldn’t /a/ be at the bottom, /i/ on the top-left and /u/ on the top-right?\nLet’s fix this! First we need to use f22 as the x-axis and f12 as the y-axis.\n\nformants |&gt; \n  ggplot(aes(f22, f12, colour = vowel)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nBut now the plot is upside-down!\nTo fix this we need to reverse both axes with scale_*_reverse().\n\nformants |&gt; \n  ggplot(aes(f22, f12, colour = vowel)) +\n  geom_point(alpha = 0.5) +\n  scale_x_reverse() + scale_y_reverse()\n\n\n\n\n\n\n\n\nWe can do better! Let’s split the data for each participant and let’s plot ellipses (circles) for each vowel.\nWe can plot speakers separately with facet_wrap(): this works like facet_grid() but instead of specifying variables for rows and columns, you just specify one variable. The data of each value in that variable will be plotted in a separate panel. This is useful when you have many different values for a single variable, like speaker here.\nWe’re also adding in ellipses around each cluster using stat_ellipse(). If you see test that says Warning: Probable convergence failure, that’s because some ellipses might not fit the data super well. Don’t worry about it for now.\n(Note: the plot your code generates will look more squashed than to the one below. If you want to learn how to make your plot taller, like the one below, check out the Extra box below.)\n\nformants |&gt; \n  ggplot(aes(f22, f12, colour = vowel)) +\n  geom_point(alpha = 0.5) +\n  stat_ellipse() +\n  scale_x_reverse() + scale_y_reverse() +\n  facet_wrap(vars(speaker), ncol = 3)\n\n\n\n\n\n\n\n\nIn the plot below, the data is normalised within speakers: note how the numbers along the axis labels are different and the clusters are a bit less dispersed. If you want to learn how to normalise the data within speakers so that you can recreate the following plot, check Section 5 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtra: Setting the figure aspect ratio\n\n\n\n\n\nYou can set the aspect ratio of a figure in a Quarto document with the fig-asp Quarto option.\nThe result won’t look different when you just run the code chunk, but when you render the document to HTML, the figure will have the aspect ratio you specify.\n{r}\n#| label: tall-figure\n#| fig-asp: 1.5\n\n(your figure code)"
  },
  {
    "objectID": "tutorials/tutorial-w08.html#reaction-times",
    "href": "tutorials/tutorial-w08.html#reaction-times",
    "title": "DAL tutorial - Week 8",
    "section": "3 Reaction times",
    "text": "3 Reaction times\nDescription\n\n\nYou have learnt how to use density plots, but an alternative way of plotting continuous data by categorical groups is to use strip charts and violin plots, alone or in combination!\nLet’s plot reaction times from the shallow data.\n\nshallow &lt;- read_csv(\"data/song2020/shallow.csv\")\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou can plot violin plots using geom_violin(). A violin is basically just a density plot, mirrored vertically. The wider parts of the violin indicate a greater density of data around the related y-axis values.\n\nshallow |&gt; \n  # we filter to include only the critical trials\n  filter(Critical_Filler == \"Critical\") |&gt; \n  ggplot(aes(Relation_type, RT)) +\n  geom_violin() +\n  facet_grid(cols = vars(Group))\n\n\n\n\n\n\n\n\nWe can overlay a strip chart, which is basically just the raw data, plotted as dots that are jittered horizontally.\n\nshallow |&gt; \n  # we filter to include only the critical trials\n  filter(Critical_Filler == \"Critical\") |&gt; \n  ggplot(aes(Relation_type, RT)) +\n  geom_violin() +\n  # the width argument in geom_jitter() specifies how wide the jitter should be, as\n  # a value between 0 and 1.\n  # the lower the number, the narrower the jitter.\n  geom_jitter(width = 0.1, alpha = 0.25) +\n  facet_grid(cols = vars(Group))\n\n\n\n\n\n\n\n\nCan you tell if there are differences in RTs between the different groups and relation types?"
  },
  {
    "objectID": "tutorials/tutorial-w08.html#sec-accuracy",
    "href": "tutorials/tutorial-w08.html#sec-accuracy",
    "title": "DAL tutorial - Week 8",
    "section": "4 Accuracy and computing proportions",
    "text": "4 Accuracy and computing proportions\nDescription\n\n\nLet’s look at data from Koppensteiner et al, 2016. The study looked at the relationship between sound iconicity and body motion.\n\ntakete_maluma &lt;- read_delim(\"data/koppensteiner2016/takete_maluma.txt\")\n\nRows: 460 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): Tak_Mal_Stim, Answer, Rater\ndbl (2): Corr_1_Wrong_0, Female_0\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe study specifically analysed the accuracy of the responses (Answer) but in this tutorial we will look instead at the response itself (whether they selected a shaking motion or a flowing motion).\nAlas, this piece of information is not coded in a column in the data, but we can create a new column based on the available info.\n\nWhen the stimulus is Takete and the answer is CORRECT then the participant’s response was shaking.\nWhen the stimulus is Takete and the answer is WRONG then the participant’s response was flowing.\nWhen the stimulus is Maluma and the answer is CORRECT then the participant’s response was shaking.\nWhen the stimulus is Maluma and the answer is WRONG then the participant’s response was flowing.\n\nNow, go ahead and create a new column called Response using the mutate() and the case_when() function.\nWe have not encountered this function yet, so here’s a challenge for you: check out its documentation to learn how it works. You will also need to use the AND operator &: this allows you to put two statements together, like Tak_Mal_Stim == \"Takete\" & Answer == \"CORRECT\" for “if stimulus is Takete AND answer is CORRECT”.\n(If you are following the documentation but case_when() is still giving you mysterious errors, make sure that your version of dplyr is the most current one. To do this, run packageVersion(\"dplyr\") in the console. You want the output to be 1.1.0. If it’s not, you’ll need to update tidyverse by re-installing it.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntakete_maluma &lt;- takete_maluma |&gt;\n  mutate(\n    Response = case_when(\n      Tak_Mal_Stim == \"Takete\" & Answer == \"CORRECT\" ~ \"shaking\",\n      Tak_Mal_Stim == \"Takete\" & Answer == \"WRONG\" ~ \"flowing\",\n      Tak_Mal_Stim == \"Maluma\" & Answer == \"CORRECT\" ~ \"flowing\",\n      Tak_Mal_Stim == \"Maluma\" & Answer == \"WRONG\" ~ \"shaking\"\n    )\n  )\n\n\n\n\nYou should now have something that looks like this:\n\ntakete_maluma\n\n\n  \n\n\n\nA common (but incorrect) way of plotting proportion/percentage data (like accuracy) is to calculate the mean accuracy of each participant and then produce a bar chart with error bars that indicate the mean accuracy and the dispersion around the mean accuracy.\nYou might have seen something like this in many papers.\n\n\n\n\n\n\n\n\n\nTHE HORROR. Alas, this is a very bad way of processing proportion data. Check this blog post if you want to learn why.\nThe alternative (robust) way to plot proportion data is to show the proportion for individual participants (Rater in the takete_maluma data frame). For example, what percentage of the time did a participant give a “shaking” response?\nTo do so we can use a combination of summarise(), geom_jitter() and stat_summary().\nFirst, we need to calculate the proportion of “shaking” responses by participant (Rater) with the simple formula: number of “shaking” responses / total number of responses. (Note that you could have well picked “flowing” instead of “shaking”, and that would give you an equivalent but opposite result: if somebody gave 70% “shaking” responses, they must have given 30% “flowing” responses. The most important thing is to be consistent throughout the entire computation.)\nTo calculate this proportion, we first need to create a new column with a numeric version of the Response column. Since we want to calculate the proportion of “shaking” responses, we use 1 for “shaking and 0 for”flowing”.\nWe can do that using the ifelse() function.\n\ntakete_maluma &lt;- takete_maluma |&gt;\n  mutate(\n    Response_num = ifelse(Response == \"shaking\", 1, 0)\n  )\ntakete_maluma\n\n\n  \n\n\n\nWe can use a special tidyverse function, n(), which returns the number of rows in the data frame, or if the data is grouped, the number of rows in each group. Since we will group the data frame by Rater and Tak_Mal_Stim, n() returns the number of rows for each participant and each stimulus!\n\nshaking_prop &lt;- takete_maluma |&gt;\n  group_by(Rater, Tak_Mal_Stim) |&gt;\n  summarise(\n    shaking_prop = sum(Response_num) / n(),\n    # The following drops the grouping created by group_by() which we don't\n    # need anymore.\n    .groups = \"drop\"\n  )\nshaking_prop\n\n\n  \n\n\n\nWhy does sum(Response_num) / n() give us the proportion of “shaking” responses? We coded every “shaking” response as 1, so if somebody gave 5 “shaking” responses and 10 “flowing” responses, we would calculate \\(5 / (5 + 10) = 5/15 = 33.3\\%\\).\nNow we can plot the proportions and add mean and confidence intervals using geom_jitter() and stat_summary().\nBefore proceeding, you need to install the Hmisc package. There is no need to attach it (it is used by stat_summary() under the hood). Remember not to include the code for installation in your document; you need to install the package only once.\n\nggplot() +\n  # Proportion of each participant\n  geom_jitter(\n    data = shaking_prop,\n    aes(x = Tak_Mal_Stim, y = shaking_prop),\n    width = 0.1, alpha = 0.5\n  ) +\n  # Mean proportion by stimulus with confidence interval\n  stat_summary(\n    data = takete_maluma,\n    aes(x = Tak_Mal_Stim, y = Response_num, colour = Tak_Mal_Stim),\n    fun.data = \"mean_cl_boot\", size = 1\n  ) +\n  labs(\n    title = \"Proportion of takete responses by participant and stimulus\",\n    caption = \"Mean proportion is represented by coloured points with 95% bootstrapped Confidence Intervals.\",\n    x = \"Stimulus\",\n    y = \"Proportion\"\n  ) +\n  # ylim(0, 1) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nDo you see how much more informative and interesting this plot is, compared to the bar-and-whisker plot above?\nYou will notice something new in the code: we have specified the data inside geom_jitter() and stat_summary() instead of inside ggplot(). This is because the two functions need different data: geom_jitter() needs the data with the proportion we calculated for each participant and stimulus; stat_summary() needs to calculate the mean and CIs from the overall data, rather than from the proportion data we created.\nWe are also specifying the aesthetics within each geom/stat function, because while x is the same, the y differs!\nIn stat_summary(), the fun.data argument lets you specify the function you want to use for the summary statistics to be added. Here we are using the mean_cl_boot function, which returns the mean proportion of Response_num and the 95% Confidence Intervals (CIs, there are frequentist confidence intervals) of that mean. The CIs are calculated using a bootstrapping procedure (if you are interested in learning what that is, check the documentation of smean.sd from the Hmisc package)."
  },
  {
    "objectID": "tutorials/tutorial-w08.html#sec-f0",
    "href": "tutorials/tutorial-w08.html#sec-f0",
    "title": "DAL tutorial - Week 8",
    "section": "5 Fundamental frequency (f0) and normalisation",
    "text": "5 Fundamental frequency (f0) and normalisation\nDescription\n\n\n\n# .rda are R data files and they can be read with `load()`\nload(\"data/coretta2018a/formants.rda\")\n\n\n\n\n\n\n\nWhat to do if you don’t have coretta2018a/ in your data/ folder\n\n\n\n\n\nYou’ll need to download formants.rda from GitHub, create a folder called coretta2018a/, and move formants.rda into it.\n\nVisit this link.\nTo the right of the “Raw” button on the right-hand side of the page, click the button with the down arrow to download the “raw” file. Now formants.rda should be downloaded onto your computer.\nIn your data/ folder, create a new folder called coretta2018a/.\nMove formants.rda into data/coretta2018a/.\n\n\n\n\nLet’s plot f0 for different vowels: We can use a violin plot with a strip chart (see previous section!).\n\nformants |&gt; \n  ggplot(aes(label, f0)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.1)\n\n\n\n\n\n\n\n\nYou might notice something weird going on… The violins are quite bumpy! This is because different people have different mean f0.\nWe want to normalise the data across speakers, so that individual differences in mean f0 are removed. This process is also called standardisation or z-scoring.\nZ-scores are a standardised unit that allows you to compare things that are on different scales. Our f0 values are on different scales because some speakers have higher mean f0, some have lower mean f0.\nZ-scores are calculated by removing the mean from each value and dividing it by the standard deviation. Since we are normalising within subject, we need to calculate the mean and standard deviation for each participant and apply those to each participant’s data separately. We achieve this by grouping the data by speaker, and then creating a new column that contains each speaker’s z-scores.\n\nformants &lt;- formants |&gt; \n  # group data by speaker so that normalisation is applied for each speaker\n  # separately\n  group_by(speaker) |&gt; \n  # calculate z-scores\n  mutate(\n    f0_z = (f0 - mean(f0)) / sd(f0)\n  )\n\nhead(formants$f0_z)\n\n[1]  3.4126740 -0.9289615  1.1254401 -0.5180812 -1.0111376  0.2488954\n\n\nNow we can recreate the plot above but using f0_z.\n\nformants |&gt; \n  ggplot(aes(label, f0_z)) +\n  geom_violin() +\n  geom_jitter(aes(colour = vowel), width = 0.1, alpha = 0.1)\n\n\n\n\n\n\n\n\nNow the distributions are a lot less lumpy!\nWhy is colour = vowel in geom_jitter(aes(...))? Because we want to colour only the jittered points. If we put colour = vowel in the main ggplot(aes(...)) , then the violin borders would also be coloured.\nIf you want to colour the areas of the violins instead, you can use the fill aesthetic. This will work both in the main ggplot(aes(...)) or in geom_violin(aes(...)) because the points of geom_jitter() can’t be given a colour fill, but only a colour."
  },
  {
    "objectID": "tutorials/tutorial-w08.html#rating-likert-scales",
    "href": "tutorials/tutorial-w08.html#rating-likert-scales",
    "title": "DAL tutorial - Week 8",
    "section": "6 Rating (Likert) scales",
    "text": "6 Rating (Likert) scales\nDescription\n\n\nLikert scales are a kind of rating scales (see here for more on the relationship between them). Such scales produce very strange variables. They are categorical and ordinal.\nDespite being very common, the practice of treating Likert scales as numeric is quite inappropriate. For an explanation, check the first two pages of Verissimo (2021).\nA very nice type of plot that is very useful for Likert scale data is a so-called divergent stacked bar chart.\nThe HH package has a few handy functions to make creating divergent stacked bar charts very straightforward. You will have to install this package (remember not to include the code for installation in your document; you need to install the package only once).\nWe will create a plot of language attitudes towards Esperanto and Emilian (Gallo-Italian).\nParticipants were asked the following: “when you hear somebody speaking Emilian/Esperanto, you would think they are…” and they were presented with 9 adjectives, which they had to rate from strongly disagree (meaning they strongly disagreed that somebody speaking Emilian/Esperanto had that quality) to strongly agree (meaning they strongly agreed that the person had that quality). Esperanto speakers rated Esperanto while Emilian speakers rated Emilian.\nLet’s read the data (it’s an .rds file!).\n\nemilianto_attitude &lt;- readRDS(\"data/hampton2023/emilianto_attitude.rds\")\n\n\n\n\n\n\n\nWhat to do if you don’t have hampton2023/ in your data/ folder\n\n\n\n\n\nYou’ll need to download emilianto_attitude.rds from GitHub, create a folder called hampton2023/, and move emilianto_attitude.rds into it.\n\nVisit this link.\nTo the right of the “Raw” button on the right-hand side of the page, click the button with the down arrow to download the “raw” file. Now emilianto_attitude.rds should be downloaded onto your computer.\nIn your data/ folder, create a new folder called hampton2023/.\nMove emilianto_attitude.rds into data/hampton2023/.\n\n\n\n\nInstall the HH package before proceeding (but you don’t need to attach it).\nTo be able to plot with the likert() function from the HH package, we first need to wrangle the data.\nBasically, we need a new tibble with counts of each value of the scale for each of the adjectives that participants had to rate. In other words, how many times did people say “strong disagree”, “disagree”, and so on, for each adjective.\nMake sure you understand each line of the following code.\n\nemilianto_lik &lt;- emilianto_attitude |&gt;\n  \n  # let's select only the columns we need\n  select(language, educated:familiar) |&gt;\n  \n  # let's pivot the data so that we get a longer tibble, with the columns\n  # language, adjective and score\n  pivot_longer(educated:familiar, names_to = \"adjective\", values_to = \"score\") |&gt;\n  \n  # and we count the numbers of each adjective and score combo\n  count(language, adjective, score) |&gt;\n  \n  # then we pivot again so that now the data is wider, with the columns,\n  # language, adjective, and one column for each of the five scores\n  pivot_wider(names_from = \"score\", values_from = n) %&gt;%\n  \n  # we rename the scores to the actual label that the participants saw\n  rename(\"strong disagree\" = `1`, \"disagree\" = `2`, \"neither\" = `3`, \"agree\" = `4`, \"strong agree\" = `5`)\nemilianto_lik\n\n\n  \n\n\n\nNow we can finally plot this using the likert() function. The function takes a formula that describes how to plot the data: here, we want a divergent stacked bar for each adjective and we want all the scores (adjective ~ .), but we also want to facet by language (| language).\n\nHH::likert(\n  # formula\n  adjective ~ . | language,\n  # data\n  emilianto_lik,\n  as.percent = TRUE,\n  # plot title\n  main = \"Language attitudes towards Emilian and Esperanto\"\n)\n\n\n\n\n\n\n\n\nWhat is that HH::likert()? That’s the R way of calling a function from a package directly, without the need to attach the full package with library(HH). This is useful, for example, for packages that override functions from other packages when attached or for packages that attach a lot of package dependencies.\nIn most cases you won’t have to worry, but if you ever think that function overwriting is causing issues, you know you can use the package::function() syntax.\nWhat can you tell about the attitude towards Emilian and Esperanto from the plot?"
  }
]